\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb,lscape, natbib}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{setspace}
\usepackage[final]{pdfpages}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm, bottom=2.5cm]{geometry}
\usepackage{natbib} 
\usepackage{bibentry} 
\newcommand{\bibverse}[1]{\begin{verse} \bibentry{#1} \end{verse}}
\newcommand{\vs}{\vspace{.3in}}
\renewcommand{\ni}{\noindent}
\usepackage{xr-hyper}
\usepackage[]{hyperref}
\usepackage[capposition=top]{floatrow}
\usepackage{amssymb}
%\usepackage[capposition=top]{floatrow}


\def \myFigPath {../../figures/} 
% BE CAREFUL WITH FIGNAMES, IN LATEX THEY'RE NOT CASE SENSITIVE!!
\def \myTablePath {../../tables/} 
\def \myBibPath {../../literature/} 


\definecolor{citec}{rgb}{0,0,.5}
\definecolor{linkc}{rgb}{0,0,.6}
\definecolor{bcolor}{rgb}{1,1,1}
\hypersetup{
%hidelinks = true
  colorlinks = true,
  urlcolor=linkc,
  linkcolor=linkc,
  citecolor = citec,
  filecolor = linkc,
  pdfauthor={Laura G\'ati},
}


\geometry{left=.83in,right=.89in,top=1in,
bottom=1in}
\renewcommand{\[}{\begin{equation}}
\renewcommand{\]}{\end{equation}}

% New Options
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{result}{Result}

%\newtheorem{theorem}{Theorem}[section] % the third argument specifies that their number will be adopted to the section
%\newtheorem{corollary}{Corollary}[theorem]
%\newtheorem{lemma}[theorem]{Lemma}
%\declaretheorem{proposition}
%\linespread{1.3}
%\raggedbottom
%\font\reali=msbm10 at 12pt

% New Commands
\newcommand{\real}{\hbox{\reali R}}
\newcommand{\realp}{\hbox{\reali R}_{\scriptscriptstyle +}}
\newcommand{\realpp}{\hbox{\reali R}_{\scriptscriptstyle ++}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\argmin}{arg\,min}
\newcommand\w{3.0in}
\newcommand\wnum{3.0}
\def\myFigWidth{5.3in}
\def\mySmallerFigWidth{2.1in}
\def\myEvenBiggerFigScale{0.8}
\def\myPointSixFigScale{0.6}
\def\myBiggerFigScale{0.4}
\def\myFigScale{0.3}
\def\mySmallFigScale{0.22}
\def\mySmallerFigScale{0.18}
\def\myTinyFigScale{0.16}
\def\myPointFourteenFigScale{0.14}
\def\myTinierFigScale{0.12}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % this defines a command to make align only number this line
\newcommand{\code}[1]{\texttt{#1}} %code %

\renewcommand*\contentsname{Overview}
\setcounter{tocdepth}{2}
\linespread{1.2}

\begin{document}



\title{Monetary Policy \& Anchored Expectations \\
An Endogenous Gain Learning Model \\
\vspace{0.8cm}
\small{Preliminary and Incomplete}}
\author{Laura G\'ati} 
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%             DOCUMENT           %%%%%%%%%%%%%%%%%% 

\begin{abstract}
This paper analyzes optimal monetary policy in a behavioral model where expectation formation is characterized by potential anchoring of expectations. Expectations are anchored when in an adaptive learning setting, the private sector endogenously chooses a decreasing learning gain. Within the context of an otherwise standard macro model with nominal rigidities and natural-rate and cost-push shocks, I find that the anchoring mechanism introduces two new intertemporal tradeoffs: a stabilization and a volatility tradeoff. Optimal policy is thus time-varying in two ways. First, the anchoring expectation formation allows the central bank to postpone current intratemporal tradeoffs to the future. Second, while concerns for stabilization lead the central bank to seek to anchor expectations in the long run, getting them anchored is costlier the more current expectations are unanchored. For this reason, the bank prefers to suppress any unanchoring before it materializes. In particular, it raises the interest rate by 0.4547\%-points for an average upward unanchoring of expectations. 
\end{abstract}



%\tableofcontents

%\listoffigures

 %%%%%%%%%%%%%%%%%%           INTRO            %%%%%%%%%%%%%%%%%% 
\newpage
\section{Introduction}\label{introduction}

\begin{quote}
I must - and I do - take seriously the risk that inflation shortfalls that persist even in a robust economy could precipitate a difficult-to-arrest downward drift in inflation expectations. At the heart of [our] review is the evaluation of potential changes to our strategy designed to strengthen the credibility of our [...] 2 percent inflation objective [such that] inflation expectations [remain] well anchored.\\
Jerome Powell, Chairman of the Federal Reserve \footnote{Federal Reserve ``Conference on Monetary Policy Strategy, Tools, and Communications Practices,''  June 4, 2019, Opening Remarks.}
\end{quote}
%\begin{quote}
%Policymakers came out of the Great Inflation era with a clear understanding that it was essential to anchor inflation expectations at some low level. \\
%Jerome Powell, Chairman of the Federal Reserve \footnote{Federal Reserve ``Challenges for Monetary Policy,''  August 23, 2019, Opening Remarks.}
%\end{quote}

In his opening remarks to the Federal Reserve's recent review of its policy tools, Fed Chairman Jerome Powell echoes the common sentiment among central bankers that anchoring inflation expectations is ``at the heart'' of monetary policy. Yet the leading macroeconomic models of monetary policy are based on the rational expectations (RE) paradigm and therefore do not involve an expectation formation that would capture anchored or unanchored expectations. These models are thus unable to guide the conduct of monetary policy in the very environment central bankers emphasize as important: the environment of potentially unanchored expectations.

This paper addresses the need for a theory of optimal monetary policy in an environment where expectations may become unanchored. In my behavioral model of expectation formation, the notion of anchoring is the stability of the private sector's expectations of mean inflation over the long run. Whether expectations are anchored or not depends on the size of the learning gain, the weight that the public places on current forecast errors when updating its expectation of mean inflation. Since the size of the gain depends on forecast errors, the central bank's interest-rate setting interacts with expectation formation, leading to anchoring or unanchoring of expectations.

The main argument of the paper is that monetary policy optimally monitors and bases its actions on the time series of expected mean inflation.\footnote{By ``expected mean inflation'' I mean the private sector's expectation of where inflation will be on average over a longer period in the future. Therefore I also refer to this as ``long-run inflation expectations.''} Fig. \ref{epi} portrays this series for the last decade in the US. As the figure shows, long-run inflation expectations of the public, averaging a little above the 2\% target prior to 2015, display a marked downward drift since 2015. Not only can rational expectations models not capture drifting long-run expectations, restricting them to lie at the deterministic steady state, but this feature also renders them unable to answer the question how monetary policy should respond to long-run expectations drifting away from the target, as on Fig. \ref{epi}.

\begin{figure}[h!]
\subfigure[Market-based inflation expectations, 10 year, average, \%]{\includegraphics[scale = \mySmallFigScale]{\myFigPath epi10_2020_02_09}}
\caption{}
\label{epi}
\end{figure}

Macroeconomic theory of monetary policy and anchored expectations thus needs to account for expectation formation that a) departs from rational expectations, b) features a margin along which the public updates its forecasting behavior. I propose a behavioral model in the adaptive learning tradition where the public sector's choice of learning gain is endogenous, as in \cite{carvalho2019anchored}. This captures the idea that in normal times, when firms and households observe economic data that confirms their previous predictions, agents choose a decreasing gain and thus do not change their forecasting rules by much. By contrast, when incoming data suggests that the current forecasting rule is incorrect, agents select an increasing gain, updating their forecasting rule strongly. I refer to the former case as \emph{anchored} and to the latter as \emph{unanchored expectations}.

The contribution of this paper is to investigate how the anchoring expectation formation affects the optimal conduct of monetary policy. The main results can be summarized as follows. Optimal monetary policy under anchoring is time-varying both in response to shocks and in terms of the optimal time path of the interest rate. The time-variance of optimal policy comes from the novel intertemporal tradeoffs monetary policy faces because of the anchoring expectation formation. 

First, the analytical results show that the optimal Ramsey policy follows a targeting rule that spells out how the intratemporal tradeoff between inflation and unemployment is complimented by novel intertemporal tradeoffs due to the anchoring expectation formation. The extent of these tradeoffs is determined by the current and expected future stance of anchoring. 

My second analytical result is that, unlike time-zero optimal commitment under rational expectations, optimal policy will be time consistent in the sense of \cite{kydland1977rules}. But not only that. I also find that optimal policy in the anchoring model is not history-dependent: the commitment and discretion solutions of the Ramsey problem are indistinguishable. Mathematically, the solution resembles discretion under rational expectations since lagged multipliers are absent from the solution. Intuitively, expectation formation based on past data cannot incorporate promises on the part of the policymaker regarding the course of future policy. Qualitatively, however, this implies responses to shocks more akin to commitment under rational expectations because the central bank spreads out the effects of shocks over time.

Interestingly, this implies that the novel intertemporal tradeoff alleviates the intratemporal tradeoff between inflation and the output gap. From this standpoint, it is therefore not unambiguously desirable to anchor expectations because as long as the private sector is learning, the presence of the novel intertemporal tradeoff allows the central bank to postpone the current intratemporal tradeoff to the future. On the other hand, unanchored expectations however imply faster learning and thus faster convergence, thus eliminating the intertemporal margin. 

A third set of results characterizes the interest rate sequence that implements the optimal Ramsey policy. Due to the nonlinearity introduced by the endogenous gain, I use a parameterized expectations approach to solve for the optimal interest rate sequence numerically. Moreover, I approximate the optimal policy function using a cubic spline interpolant. The analysis of the optimal policy function reveals that not only do unanchored expectations induce undesired volatility to the economy, but they also require the central bank to respond aggressively to get expectations anchored again. % add numbers here once you have em!
 Anchoring expectations is therefore desirable from a long-run volatility standpoint, yet it comes at a short-run cost of heightened volatility. Thus the monetary authority faces yet another intertemporal tradeoff: the volatility tradeoff between the short-run costs and the long-run benefits of anchoring expectations.

A last result pertains to the choice of optimal coefficients when monetary policy is constrained to follow a Taylor rule.  It turns out that if the Taylor rule coefficients are time-invariant, monetary policy is optimally less aggressive on inflation than under rational expectations. This of course relates to the intertemporal tradeoff between the volatility costs and benefits of anchoring. By being aggressive on inflation, the central bank introduces negative feedback into a system with positive feedback, and can thus anchor expectations. However, when the interest rate responds strongly to realized inflation, unanchored expectations induce high fluctuations in forecasts and observables, making it costly to choose large coefficients. 

Overall, my work uncovers that in the presence of expectations that can become unanchored, the monetary authority needs to monitor the evolution of long-run expectations and adjust its policy instrument in accordance with their stance. In particular, the central bank needs to act aggressively if it perceives signs of potential unanchoring to foreclose long-run expectations drifting away from the bank's target. This way, it can maintain the low-volatility benefits of having anchored expectations.  
 %%%%%%%%%%%%%%%%%%           RELATED LITERATURE            %%%%%%%%%%%%%%%%%% 
%\subsection{Related literature}

The model I use to study the interaction between monetary policy and anchoring is a behavioral version of standard New Keynesian (NK) model of the type widely used for monetary policy analysis. Monetary policy in the rational expectations version of this model has been studied extensively, for example in \cite{clarida1999science} or \cite{woodford2011interest}, whose exposition I follow. The formulation of a target criterion to implement optimal policy is in the tradition of \cite{svensson1999inflation}.

The behavioral part of the model is the departure from a rational expectation formation on the part of the public sector. Instead, I allow the public sector to form expectations via an adaptive learning scheme, where the learning gain - the parameter governing the extent to which forecasting rules are updated in the direction of the most recent forecast error - is endogenous. The learning framework belongs to the statistical learning literature advocated in the book by \cite{evans_honkapohja2001}. This literature replaces the rational expectations assumption by postulating an ad-hoc forecasting rule, the perceived law of motion (PLM), as the expectation-formation process. Agents use the PLM to form expectations and update it in every period using recursive estimation techniques. 

Adaptive learning is an attractive alternative to rational expectations for several reasons. The first reason is that it is intuitive to think that individuals in the economy do not know the true underlying model. Economists do not know the true model of the economy, so why should firms and households? And just like an econometrician estimates statistical models to form forecasts of relevant variables, it is reasonable to suppose that the private sector does too. In fact, the anchoring expectation formation in this paper allows the private sector not only to estimate a simple statistical model, but also to change the model specification if its forecasting rule is performing poorly. 

Secondly, many studies document the ability of adaptive learning models to match properties of expectations data and of macro aggregates. First and foremost, \cite{milani2007expectations} estimates a constant gain learning model and matches the persistence of inflation without recourse to backward-looking elements in the Phillips curve. \cite{eusepi2011expectations} show how a calibrated adaptive learning version of the real business cycle (RBC) model outperforms the rational expectations version. In particular, the learning model leads to persistent and hump-shaped responses to iid shocks, resolving the long-standing critique of RBC models of \cite{cogley1993impulse}. Having an endogenous gain improves the empirical properties of adaptive learning models further. \cite{milani2014learning} documents that endogenous gain models can generate endogenous time-varying volatility  in models without any exogenous time-variance. Lastly, most related to my work is the paper by \cite{carvalho2019anchored} that estimates the evolution of the endogenous gain for the last fifty years in the US. Not only does the model display excellent out-of-sample forecasting performance in terms of matching long-run expectations, but the gain time series invites a reinterpretation of the Great Inflation as a period of unanchored expectations. \cite{carvalho2019anchored} thus provide empirical backing to the ideas summarized in \cite{sargent1999} that American inflation was conquered once the Fed succeeded in anchoring inflation expectations.

The paper is structured as follows. Section \ref{NK} introduces the model. Section \ref{learning} describes the learning framework and spells out the anchoring mechanism. Section \ref{analytical} presents the results in three parts. First, Section \ref{ramsey} discusses an analytical characterization of the Ramsey policy. Second, Section \ref{implement} solves for the interest rate sequence that implements the optimal Ramsey allocation using global methods. Third, Section \ref{opt_TR} investigates the optimal choice of response coefficients if monetary policy is restricted to follow a Taylor rule.  Section \ref{conclusion} concludes.

 %%%%%%%%%%%%%%%%%%           NK MODEL            %%%%%%%%%%%%%%%%%% 
%\newpage
\section{The model}\label{NK}
Apart from expectation formation, the model is a standard New Keynesian model with nominal frictions \`a la \cite{calvo1983staggered}. The advantage of having a standard NK backbone to the model is that one can neatly isolate the way the anchoring mechanism alters the behavior of the model. Since the mechanics of the rational expectations version of this model are well understood, I only lay out the model briefly and pinpoint the places where the assumption of nonrational expectations matters.\footnote{For the specifics of the NK model the reader is referred to \cite{woodford2011interest}.}
\subsection{Households}
The representative household is infinitely-lived and maximizes expected discounted lifetime utility from consumption net of the disutility of supplying labor hours:
\begin{equation}
\hat{\E}_t\sum^{\infty}_{T=t}\beta^{T-t} \bigg[ U(C^i_T) - \int_0^1 v(h^i_T(j)) dj \bigg]
\label{lifetime_U}
\end{equation}
$U(\cdot)$ and $v(\cdot)$ denote the utility of consumption and disutility of labor respectively and $\beta$ is the discount factor of the household. $h^i_t(j)$ denotes the supply of labor hours of household $i$ at time $t$ to the production of good $j$ and the household participates in the production of all goods $j$. Similarly, household $i$'s consumption bundle at time $t$,  $C_t^i$, is a Dixit-Stiglitz composite of all goods in the economy:
\begin{equation}
C^i_t =  \bigg[  \int_0^1 c^i_t(j)^{\frac{\theta-1}{\theta}} dj \bigg]^{\frac{\theta}{\theta-1}}\label{dixit}
\end{equation}
$\theta>1$ is the elasticity of substitution between the varieties of consumption goods. Denoting by $p_t(j)$ the time-$t$ price of good $j$, the aggregate price level in the economy can then be written as
\begin{equation}
P_t =  \bigg[  \int_0^1 p_t(j)^{1-\theta} dj \bigg]^{\frac{1}{\theta-1}}
\label{agg_price}
\end{equation}
The budget constraint of household $i$ is given by
\begin{equation}
 B^i_t \leq (1+i_{t-1})B^i_{t-1} + \int_0^1 w_t(j)h^i_t(j) + \Pi_t^i(j)  dj-T_t -P_tC^i_t
 \label{BC}
\end{equation}
where $\Pi_t^i(j)$ denotes profits from firm $j$ remitted to household $i$, $T_t$ taxes, and $B^i_t$ the riskless bond purchases at time $t$.\footnote{For ease of exposition I have suppressed potential money assets here. This has no bearing on the model implications since it represents the cashless limit of an economy with explicit money balances.}

The only difference to the standard New Keynesian model thus far is the expectation operator, $\hat{\E}$. This is the subjective expectation operator that differs from its rational expectations counterpart, $\E$, in that it does not encompass knowledge of the model. In particular, households have no knowledge of the fact that they are identical and by extension they also do not internalize that they hold identical beliefs about the evolution of the economy. As we will see in Section \ref{FOCs}, this has implications for their forecasting behavior and will result in decision rules that differ from those of the rational expectations version of the model.

\subsection{Firms}

Firms are monopolistically competitive producers of the differentiated varieties $y_t(j)$. The production technology of firm $j$ is $y_t(j)=A_tf(h_t(j))$, whose inverse, $f^{-1}(\cdot)$, signifies the amount of labor input. Noting that $A_t$ is the level of technology and that $w_t(j)$ is the wage per labor hour, firm $j$ profits at time $t$ can be written as
\begin{equation}
\Pi_t^j = p_t(j)y_t(j) -w_t(j)f^{-1}(y_t(j)/A_t)
\end{equation}
Firm $j$'s problem then is to set the price of the variety it produces, $p_t(j)$, to maximize the present discounted value of profit streams
\begin{equation}
\hat{\E}_t\sum^{\infty}_{T=t}\alpha^{T-t} Q_{t,T} \bigg[ \Pi^j_t(p_t(j))\bigg]
\label{lifetime_profits}
\end{equation}
subject to the downward-sloping demand curve
\begin{equation}
y_t(j) = Y_t \bigg(\frac{p_t(j)}{P_t}\bigg)^{-\theta}
\end{equation}
where 
\begin{equation}
Q_{t,T} = \beta^{T-t} \frac{P_t U_c(C_T)}{P_T U_c(C_t)}
\end{equation}
is the stochastic discount factor from households. Nominal frictions enter the model through the parameter $\alpha$ in Equation (\ref{lifetime_profits}). This is the Calvo probability that firm $j$ is not able to adjust its price in a given period. 

Analogously to households, the setup of the production side of the economy is standard up to the expectation operator. Also here the rational expectations operator $\E$ has been replaced by the subjective expectations operator $\hat{\E}$. This implies that firms, like households, do not know the model equations and fail to internalize that they are identical. Thus their decision rules, just like those of the households, will be distinct from their rational expectations counterparts. 

\subsection{Aggregate laws of motion}\label{FOCs}
The model solution procedure entails deriving first order conditions, taking a loglinear approximation around the nonstochastic steady state and imposing market clearing conditions to reduce the system to two equations, the New Keynesian Phillips curve (NKPC) and IS curve (NKIS). The presence of subjective expectations, however, implies that firms and households are not aware of the fact that they are identical. Thus, as \cite{preston2005} points out, imposing market clearing conditions in the expectations of agents is inconsistent with the assumed information structure.\footnote{The target of \cite{preston2005}'s critique is the Euler-equation approach as exemplified for example by \cite{bullard2002learning}. This approach involves writing down the loglinearized first order conditions of the model, and simply replacing the rational expectations operators with subjective ones. In a separate paper, I demonstrate that the Euler-equation approach is not only inconsistent on conceptual grounds as \cite{preston2005} shows, but also delivers substantially different quantitative dynamics in a simulated New Keynesian model. Thus relying on the Euler-equation approach when investigating the role of learning is not only incorrect in terms of microfoundations, but also leads to mistaken quantitative inferences. In the context of this model, the problem becomes more acute when expectations are unanchored.} 

Instead, I prevent firms and households from internalizing market clearing conditions. As \cite{preston2005} demonstrates, this leads to long-horizon forecasts showing up in firms' and households' first order conditions. As a consequence, instead of the familiar expressions, the NKIS and NKPC take the following form:
 \begin{align}
x_t &=  -\sigma i_t +\hat{\E}_t \sum_{T=t}^{\infty} \beta^{T-t }\big( (1-\beta)x_{T+1} - \sigma(\beta i_{T+1} - \pi_{T+1}) +\sigma r_T^n \big)  \label{NKIS}  \\
\pi_t &= \kappa x_t +\hat{\E}_t \sum_{T=t}^{\infty} (\alpha\beta)^{T-t }\big( \kappa \alpha \beta x_{T+1} + (1-\alpha)\beta \pi_{T+1} + u_T\big) \label{NKPC} 
\end{align}
Here $x_t$, $\pi_t$ and $i_t$ are the log-deviations of the output gap, inflation and the nominal interest rate from their steady state values, and $\sigma$ is the intertemporal elasticity of substitution. The variables $r_t^n$ and $u_t$ are exogenous disturbances representing a natural rate shock and a cost-push shock respectively. 

The laws of motion (\ref{NKIS}) and (\ref{NKPC}) are obtained by deriving individual firms' and households' decision rules, which involve long-horizon expectations, and aggregating across the cross-section. Importantly, agents in the economy have no knowledge of these relations since they don't know that they are identical and thus are not able to impose market clearing conditions required to arrive at (\ref{NKIS}) and (\ref{NKPC}). Thus, although the evolution of the observables $(\pi,x)$ is governed by the exogenous state variables $(r^n, u)$ and long-horizon expectations via these two equations, agents in the economy are unaware of this. As I will spell out more formally in Section \ref{learning}, it is indeed the equilibrium mapping between states and jump variables the agents are attempting to learn.\footnote{The learning of (\ref{NKIS}) and (\ref{NKPC}) is complicated by the fact that the current stance of expectations figures into the equations, resulting in the well-known positive feedback effects of learning.} 

To simplify notation, I gather the exogenous state variables in the vector $s_t$ and observables in the vector $z_t$ as
\begin{equation}
s_t =  \begin{bmatrix}r_t^n \\ \bar{i}_t \\ u_t \end{bmatrix} \quad \quad \quad \quad  z_t = \begin{bmatrix}\pi_t \\ x_t \\ i_t \end{bmatrix}
\end{equation}
which allows me to denote long-horizon expectations by 
 \begin{align}
f_{a,t}  \equiv  \hat{\E}_t\sum_{T=t}^{\infty} (\alpha\beta)^{T-t } z_{T+1} \quad \quad \quad \quad f_{b,t}  \equiv \hat{\E}_t\sum_{T=t}^{\infty} (\beta)^{T-t } z_{T+1} \label{fafb}
\end{align}
As detailed in App. \ref{app_compact}, one can use this notation to reformulate the laws of motion of jump variables (Equations (\ref{NKIS}), (\ref{NKPC}) and (\ref{TR})) compactly as
\begin{equation}
z_t  = A_af_{a,t} + A_b f_{b,t} + A_s s_t \label{LOM_LR} \\
\end{equation}
where the matrices $A_i, \; i=\{a,b,s\}$ gather coefficients and are given in App. \ref{app_compact}. Assuming that exogenous variables evolve according to independent AR(1) processes, I write the state transition matrix equation as
 \begin{equation}
 s_t  = h s_{t-1} + \epsilon_t  \quad \quad \epsilon_t \sim \mathcal{N}(\mathbf{0}, \Sigma) \label{LOM_s}
 \end{equation}
where $h$ gathers the autoregressive coefficients $\rho_j$, $\epsilon_t$ the Gaussian innovations $\varepsilon_t^j$, and $\eta$ the standard deviations $\sigma_t^j$, for $j=\{r,i,u\}$. $\Sigma = \eta \eta'$  is the variance-covariance matrix of disturbances.\footnote{For the sake of conciseness, I have suppressed the expressions for these in the main text. See App. \ref{app_compact}.}


 %%%%%%%%%%%%%%%%%%           LEARNING            %%%%%%%%%%%%%%%%%% 
%\newpage
\section{Learning with an anchoring mechanism}\label{learning}
The informational assumption of the model is that agents have no knowledge of the equilibrium mapping between states and jumps in the model. Therefore they are not able to form rational expectations forecasts.\footnote{To see this, observe that an agent with rational expectations would internalize the rational expectations state-space system (see (\ref{state}) - (\ref{obs_RE}) in App. \ref{app_compact}) and would therefore forecast future jumps as $\E_t z_{t+h} = g^{RE}\E_ts_{t+h} = g^{RE}h^{h}s_t$, where $g^{RE}$ is the mapping between states and jumps under rational expectations. Agents in the learning model however don't know $g^{RE}$ and are thus indeed unable to form the rational expectations forecast.} Instead, agents postulate an ad-hoc forecasting relationship between states and jumps and seek to refine it in light of incoming data. 
\subsection{Perceived law of motion}
I assume agents consider a forecasting model for jumps of the form
\begin{equation}
\hat{\E}_{t}z_{t+1} = a_{t-1} + b_{t-1} s_{t} \label{PLM}  
\end{equation}
where $a$ and $b$ are estimated coefficients of dimensions $3\times1$ and $3\times3$ respectively. This perceived law of motion (PLM) reflects the assumption that agents forecast jumps using a linear function of current states and a constant, with last period's estimated coefficients. I also assume that 
\begin{equation}
\hat{\E}_{t}{\phi_{t+h}} = \phi_{t} \quad \forall \; h\geq0 
\end{equation}
This assumption, known in the learning literature as anticipated utility, means that agents fail to internalize that they will update the forecasting rule in the future.\footnote{This is a conventional assumption in the learning literature and serves to simplify the algebra. As \cite{sargent1999} shows, similar results obtain upon relaxing anticipated utility.} This is the most behavioral element in the expectation-formation process since it postulates that agents think differently about their own behavior than how they actually act. Clearly, this poses a higher level of irrationality than not knowing the model and using statistical techniques to attempt to learn it.

Assuming that agents know the evolution of states, that is they have knowledge of Equation (\ref{state})\footnote{This is another common simplifying assumption in studies of adaptive learning. In an extension, I relax this assumption and find that it has similar implications as having agents learn the Taylor rule: initial responses to shocks lack intertemporal expectation effects, but these reemerge as the evolution of state variables is learned.}, the PLM together with anticipated utility implies that $h$-period ahead forecasts are constructed as
\begin{equation}
\hat{\E}_t z_{t+h} = a_{t-1} + b_{t-1}h^{h-1}s_t  \quad \forall h\geq 1 \label{PLM_fcst_general}
\end{equation}
Summarizing the estimated coefficients as $\phi_{t-1} \equiv \begin{bmatrix}a_{t-1} & b_{t-1}\end{bmatrix}$, here $3\times 4$, I can rewrite Equation (\ref{PLM}) as 
\begin{equation} 
\hat{\E}_t z_{t+1} = \phi_{t-1}\begin{bmatrix} 1 \\ s_{t} \end{bmatrix} \label{PLMcompact}
\end{equation}
The timing assumptions of the model are as follows. In the beginning of period $t$, the current state $s_t$ is realized. Agents then form expectations according to (\ref{PLM}) using last period's estimate $\phi_{t-1}$ and the current state $s_t$. Given exogenous states and expectations, today's jump vector $z_t$ is realized. This allows agents to evaluate the most recent forecast error $fe_{t|t-1} \equiv z_t - \phi_{t-1}\begin{bmatrix} 1\\ s_{t-1}\end{bmatrix}$ to update their forecasting rule. The estimate is updated according to the following recursive least-squares algorithm:
\begin{align}
\phi_t  & = \bigg( \phi_{t-1}' + k_t^{-1} R_t^{-1}\begin{bmatrix} 1 \\ s_{t-1} \end{bmatrix}\bigg(z_{t} - \phi_{t-1} \begin{bmatrix} 1 \\ s_{t-1} \end{bmatrix} \bigg)' \bigg)' \\
R_t &= R_{t-1} +  k_t^{-1} \bigg( \begin{bmatrix} 1 \\ s_{t-1} \end{bmatrix} \begin{bmatrix} 1 & s_{t-1} \end{bmatrix}  - R_{t-1} \bigg)
\end{align}
where $R_t$ is the $4\times 4$ variance-covariance matrix of the regressors and $k_t^{-1}$ is the learning gain, specifying to what extent the updated estimate loads on the forecast error. Clearly, a high gain implies high loadings and thus strong changes in the estimated coefficients $\phi$. A low gain, by contrast, means that the current forecast error only has a small effect on $\phi_t$.

\subsection{Endogenous gain as anchoring mechanism}

The vast majority of the learning literature specifies the gain either as a constant, $\bar{g}$, or decreasing with time so that $k_t^{-1} = (k_{t-1}+1)^{-1}$. Instead, in the spirit of \cite{carvalho2019anchored}, I allow firms and households in the model to choose whether to use a constant or a decreasing gain. I use the following endogenous gain specification: let $fe_{t|t-1}$ denote the forecast error of time $t$ variables given information at $t-1$. Then the gain evolves as
\begin{equation}
k_t^{-1}  = k_{t-1}^{-1} + \mathbf{g}(fe_{t|t-1}) 
\label{anchoring}
\end{equation}
where $\mathbf{g(\cdot)}$ is a smooth, increasing function in the forecast error that I refer to as the anchoring function. App. \ref{alternative_criteria} compares alternative specifications for $\mathbf{g(\cdot)}$.

Having an endogenous gain has the interpretation of agents being able to adapt their forecasting behavior to the volatility of their environment. If they observe small forecast errors, $\mathbf{g(\cdot)} < 0$.  Firms and households thus use decreasing gains, reflecting their belief that the underlying data-generating process (DGP) has not changed compared to their earlier held beliefs. I refer to this case as \emph{anchored expectations} because it captures the notion that the private sector's estimate of $g$, $g^l$, is not updated strongly. In other words, the previously maintained PLM is seen as close enough to the true DGP. 

By contrast, observing large forecast errors leads to increasing gains, which corresponds to assigning a higher weight to more recent observations than old ones. Such a forecasting scheme outperforms the decreasing gain scheme when the environment is volatile, reflecting a possible regime switch. If the previous DGP has been replaced by a new one, having high gains allows agents to discount old observations generated by the previous DGP that is no longer in effect, and rely more on the newest observations that come from the current DGP. In this way, agents can learn the new DGP faster, correcting their previously held PLM. This is the case of \emph{unanchored expectations}. 

It is intuitive why the central bank might care whether expectations are anchored or not. When expectations are unanchored at time $t$, the private sector believes that the true DGP involves a different mapping $g$ between states and jumps than $g^l_{t-1}$, so that$g^l_t$ is updated strongly. Private sector forecasts will thus drift in the direction of the update in $g^l$, implying that the observables will also shift in the same direction owing to the law of motion (\ref{LOM_LR}). From the perspective of the central bank, stabilization of the observables therefore implies stabilization of expectations. However, it is not obvious that the central bank prefers to anchor expectations at all points in time because regime shifts in model parameters might warrant letting the private sector learn the new DGP fast. Indeed, the contribution of this paper is to characterize formally the nature of the monetary policy problem when expectation formation is characterized by the  anchoring mechanism.

\subsection{Actual law of motion}
To complete the model, I now use the specifics of the anchoring expectation formation to characterize the evolution of the jump variables under learning. Using the PLM from Equation (\ref{PLM}), I write the long-horizon expectations in (\ref{fafb}) as
\begin{equation}
f_a(t) = \frac{1}{1-\alpha\beta}a_{t-1}  + b_{t-1}(I_3 - \alpha\beta h)^{-1}s_t \quad \quad \quad f_b(t) = \frac{1}{1-\beta}a_{t-1}  + b_{t-1}(I_3 - \beta h)^{-1}s_t  \label{fafb_anal}
\end{equation}
Substituting these into the law of motion of observables (Equation (\ref{LOM_LR})) yields the actual law of motion (ALM):
\begin{equation}
z_t = g_{t-1}^l \begin{bmatrix} 1 \\ s_t
\end{bmatrix}
\label{ALM}
\end{equation}
where $g^l$ is a $3\times4$ matrix given in App. \ref{app_FG}. Thus, instead of the state-space solution of the RE version of the model (Equations (\ref{LOM_s}) and (\ref{RE_obs})), the state-space solution for the learning model is characterized by the pair of equations (\ref{LOM_s}) and (\ref{ALM}). 




 %%%%%%%%%%%%%%%%%%            RESULTS            %%%%%%%%%%%%%%%%%% 
%\newpage
\section{Monetary policy and anchoring}\label{analytical}

This section is the heart of the paper. It sets up and solves the optimal monetary policy problem in the model with the anchoring expectation formation. In Section \ref{ramsey}, I begin by analyzing the Ramsey problem of determining optimal paths for the endogenous variables that policy seeks to bring about.  While the anchoring mechanism introduces substantial nonlinearity into the model, it is possible to derive analytically an optimal targeting criterion for the policymaker to follow. As we shall see, the optimal rule prescribes for monetary policy to act conditionally on the stance of expectations. In particular, whether expectations are anchored or not matters for the extent to which there is a tradeoff between inflation and output gap stabilization, alleviating this tradeoff when expectations are anchored. 

I then turn to the question of how to implement optimal policy. Section \ref{implement} solves for the interest rate sequence that implements the target criterion using global methods. The optimal sequence is contrasted with a Taylor rule with standard parameters in the case of a particular evolution of exogenous state variables. I then go on to discuss the properties that the optimal interest rate policy should have. 

Since history-dependence is not a feature of the optimal solution, purely forward-looking Taylor rules are no longer excluded from the class of rules that can implement the Ramsey solution. In Section \ref{opt_TR}, I therefore restrict attention to Taylor-type feedback rules for the interest rate. I solve for the optimal Taylor-rule coefficients numerically and investigate how the choice of Taylor-rule coefficients affects the anchoring mechanism. In contrast to conventional wisdom in learning models, optimal monetary policy is \emph{less} aggressive on inflation (and on the output gap) in the anchoring model than under rational expectations. As I explain in detail, the reason is that while having anchored expectations is beneficial in order to lower economic volatility in the long run, in the short run it is costly to get expectations anchored.

  %%%%%%%%%%%%%%%%%%           MON.POL. PROBLEM            %%%%%%%%%%%%%%%%%% 
%\newpage
\subsection{The Ramsey policy under anchoring}\label{ramsey}
I assume the monetary authority seeks to maximize welfare of the representative household under commitment. As shown in \cite{woodford2011interest}, a second-oder Taylor approximation of household utility delivers a central bank loss function of the form
\begin{equation}
L^{CB} =\E_t \sum_{T=t}^{\infty}\{\pi_T^2 +\lambda_x(x_T - x^*)^2 +\lambda_i(i_T - i^*)\} \label{CBloss}
\end{equation}
where $\lambda_j \; j=\{x,i\}$ is the weight the central bank assigns to stabilizing variable $j$ and $j^*$ is its target value. The central bank's problem, then, is to determine paths for inflation, the output gap and the interest rate that minimize the loss in Equation (\ref{CBloss}), subject to the model equations (\ref{NKIS}) and (\ref{NKPC}), as well as the evolution of long-horizon expectations, spelled out in Section \ref{learning}. A second question is how to implement the optimal allocation; that is, to find a response function for the policy instrument $i_t$ that implements the optimal sequences of the observables.

While for most of the paper I consider a general specification for monetary policy, in Section \ref{opt_TR}, I will restrict attention to a standard Taylor rule:
\begin{equation}
i_t = \psi_{\pi}(\pi_t -\bar{\pi}) + \psi_{x} (x_t -\bar{x}) + \bar{i}_t  \label{TR}
\end{equation}
where $\psi_{\pi}$ and $\psi_{x}$ represent the responsiveness of monetary policy to inflation and the output gap respectively, $\bar{\pi}$ and $\bar{x}$ are the central bank's targets. Lastly, $\bar{i}_t$ is a monetary policy shock. I also assume that when the Taylor rule is in effect, the central bank publicly announces this. Thus Equation (\ref{TR}) is common knowledge and is therefore not the object of learning.\footnote{In an extension I consider the case where the Taylor rule is not known (or not believed) by the public and therefore is learned together with the relations (\ref{NKIS}) and (\ref{NKPC}). This dampens intertemporal expectation effects as long as the Taylor rule is not learned; afterwards, the model dynamics are identical to those of the baseline.} 
 

% %%%%%%%%%%%%%%%%%%           TARGET CRITERION            %%%%%%%%%%%%%%%%%% 

\subsubsection{Optimal Ramsey policy as a target criterion}

Appendix \ref{app_midsimple_problem} lays out the policy problem for a simplified version of the baseline model. In particular, I make three simplifying assumptions compared to the baseline model. First, I assume that only the  inflation process is learned; expectations about the output gap and the interest rate are rational evaluations of the infinite sum of future expectations. Second, I assume that only the constant of the inflation process is learned. Third, I consider the following simplification of the anchoring mechanism:\footnote{These assumptions are made for algebraic convenience only and do not alter the qualitative implications of the model. For a more general version, see App. \ref{app_generalTC}.}

\begin{equation}
k_t = \mathbf{g}(fe_{t|t-1}) \label{anchoring_simple}
\end{equation}
The solution of the Ramsey problem under these assumptions is stated in the following result.

\begin{result} Target criterion in the anchoring model \\
The targeting rule in the simplified learning model with anchoring is given by
\begin{align*}
\pi_t  = -\frac{\lambda_x}{\kappa}\bigg\{x_t - \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+((\pi_t - \bar{\pi}_{t-1}-b_1 s_{t-1}))\mathbf{g}_{\pi,t} \bigg) \\
\bigg(\E_t\sum_{i=1}^{\infty}x_{t+i}\prod_{j=0}^{i-1}(1-k_{t+1+j}^{-1} - (\pi_{t+1+j} - \bar{\pi}_{t+j}-b_1 s_{t+j})\mathbf{g_{\bar{\pi},t+j}}) \bigg)
\bigg\} \numberthis \label{target}
\end{align*}

For the derivation, see Appendix \ref{app_midsimple_problem}. For a general target criterion without assumption (\ref{anchoring_simple}), see Appendix \ref{app_generalTC}. 
\label{result_target_anchoring}
\end{result}
The interpretation of Equation (\ref{target}) is that the \emph{intra}temporal tradeoff between inflation and output gaps due to cost-push shocks is complemented by two \emph{intertemporal} tradeoffs due to the anchoring mechanism. The first is the current stance of anchoring, captured by the gain $k_t^{-1}$ and the derivative of the anchoring function, $\mathbf{g}_{\pi,t}$, in the second term on the right-hand side. 

One part of this effect, namely the current level of the gain, is related to the result of \cite{molnar2014optimal}. In the context of a decreasing or constant gain Euler-equation learning model, they show that the presence of learning introduces a novel intertemporal tradeoff between inflation and output gap stabilization. That effect is thus present here too since a nonzero gain indicates that the learning process has not yet converged. The presence of anchoring amplifies this intertemporal tradeoff because now the degree and direction in which the gain changes matters too. 

However, there is a second intertemporal tradeoff due to the anchoring mechanism. It is manifest in the multiplication of $(1-k_{t+j}^{-1}(\pi_{t+1+j} - \bar{\pi}_{t+j}-b_1 s_{t+j})$ in the second bracket on the right-hand side. This says that additionally to the intertemporal tradeoff coming from a not-yet converged learning mechanism, there is another novel intertemporal tradeoff coming from all future expected levels of the gain given the expected future changes of the gain. In other words, the central bank needs to consider whether its chosen interest rate sequence contributes to anchoring expectations in future periods, or whether it actually serves to unanchor them.

Let me investigate these channels in isolation. To see exactly what the role of anchoring is in the target criterion, consider first the special case of exogenous gain adaptive learning, for simplicity with a constant gain specification.\footnote{The intuition is identical if the public were using a decreasing gain specification but the math would not convey that same intuition as cleanly.} In this case the anchoring function and the forecast error are irrelevant (since $\mathbf{g_z}=0, z=\pi,\bar{\pi}$) and (\ref{target}) boils down to
\begin{align}
\pi_t  = -\frac{\lambda_x}{\kappa}\bigg\{x_t - \frac{(1-\alpha)\beta}{1-\alpha\beta} k^{-1}
\bigg(\sum_{i=1}^{\infty}x_{t+i}(1-k^{-1})^i \bigg)
\bigg\} \label{target_molnar} % I think this is still correct, despite the corrections of March 30, 2020.
\end{align}
which now is the analogue of \cite{gaspar2010inflation}'s Equation (24).\footnote{In their Handbook chapter, \cite{gaspar2010inflation} provide a parsimonious treatment of \cite{molnar2014optimal}. I am referring to their expression for the target criterion because \cite{molnar2014optimal} do not provide one explicitly.} This result, found also by \cite{molnar2014optimal}, suggests that already the presence of learning introduces an intertemporal tradeoff between inflation and output gap stabilization. However, the fact that the central bank now has future output gaps as a margin of adjustment means that it does not have to face the full tradeoff in the current period. Learning allows the central bank to improve the current output gap without sacrificing inflation stability today; however, this results in a worsened tradeoff in the future. In other words, adaptive learning by itself allows the central bank to postpone the current tradeoff to later periods. 

Intuitively, this happens because adaptive expectations are slow in converging to rational expectations. In the transition, the private sector's expectations do not adjust to fully internalize the intratemporal tradeoff. This gives the monetary authority room to transfer the tradeoff to the future.

Contrasting Equations (\ref{target_molnar}) and (\ref{target}) highlights the role of anchoring. With anchoring, the extent to which policy can transfer the intratemporal tradeoff to future periods depends not only on the stance of the learning process, as in (\ref{target_molnar}), but also on whether expectations are anchored or not and in which direction they are moving. In fact, not only the current stance and change of anchoring matters, but also all future levels and changes. 

Anchoring, however, complicates the possibility of transfering today's tradeoff to the future. If the central bank is able to achieve zero forecast errors in all periods, then we're back to the decreasing gain specification. This is not likely, however, given that the noise in the evolution of exogenous states gives rise to forecast errors at all periods, both positive and negative. And while only the size of squared forecast errors matters for the sign of the derivative of the anchoring function, the sign of the forecast errors proper matters for the sign of the two large parentheses on the right-hand side. 

Let me therefore rewrite Equation (\ref{gaspar22}) from App. \ref{app_midsimple_problem}, the first of the three-equation system obtained by solving the Ramsey problem:
\begin{equation}
2\pi_t = - 2\frac{\lambda}{\kappa}x_t +\varphi_{5,t} k_t^{-1} + \varphi_{6,t} \mathbf{g}_{\pi,t} 
\end{equation}
The Lagrange multipliers $\varphi_5 \geq 0$ and $\varphi_6 \geq 0$ are the multipliers of the RLS updating and the anchoring equation respectively. This equation, upon substitution of the solutions for the two multipliers, yields the target criterion and is thus ideal for interpretation. First, since $\varphi_{5,t}k_t^{-1} > 0$, one immediately obtains the above-discussed conclusion that as long as the adaptive learning equation is a constraint to the policymaker ($\varphi_{5,t} > 0$), the central bank has more room to transfer the contemporaneous tradeoff between inflation and the output gap to the future.\footnote{Strictly speaking, $\varphi_5$ and $\varphi_6$ are never zero in this model. The reason is that the anchoring model is a convex combination of decreasing and constant gain learning. The former has the RE solution as a limit, while the latter fluctuates around the RE solution with bounded variance. In the anchoring setting, if expectations are anchored and there are no inflation surprises, the gain converges to zero and the decreasing gain limit of discretionary RE obtains. However, exogenous disturbances induce unforecastable variation, producing forecast errors that will unanchor expectations, restarting the learning process. This is in stark contrast with \cite{carvalho2019anchored}, where the anchoring function is the map between the PLM and the expected ALM and thus only depends on the endogenous component of the forecast error. Therefore, in their model absent regime switches, expectations can never become unanchored once learning has converged.}

However, whether the anchoring equation alleviates or exacerbates the inflation-output gap tradeoff depends on the sign of $\mathbf{g_{\pi,t}}$. If the derivative is positive, the effect is the same as above, and the central bank has more leeway to postpone the tradeoff to the future. By contrast, if the derivative is negative, that is expectations are becoming anchored, the intratemporal tradeoff is worsened.

Why do unanchored expectations give the central bank the possibility to postpone its current inflation-output gap tradeoff? The reason is that when expectations become unanchored, the learning process is restarted. A not-yet converged learning process implies, as I discussed above, that postponing the tradeoff is possible. Restarting the convergence process thus unlocks this possibility. 

This does not imply that the central bank should prefer to have unanchored expectations. In fact, the answer to whether expectations should be anchored from the viewpoint of the central bank depends on the current stance of learning.   Clearly, the central bank prefers to face a learning process that on the one hand has not yet converged, on the other is converging only slowly. A high gain under unanchored expectations implies both a sizable distance from convergence as well as faster learning and thus faster convergence. Therefore, ideally the central bank would have expectations anchored but the gain far from zero. Once the gain approaches zero, only unanchored expectations can raise it again to restart the learning process.

Unanchoring expectations, however, is costly. As I demonstrate in Section \ref{opt_TR}, unanchored expectations lead to more volatility in the observables due to more volatile forecast errors in the wake of a higher gain. Therefore heightened inflation- and output gap volatility may be too high a price to pay for an ameliorated inflation-output gap tradeoff.

\subsubsection{Time consistence of optimal plans under adaptive learning}
Now simplify the target criterion further, assuming that learning has converged, $k_t^{-1} = \mathbf{g_\pi} = 0$. We're left with 
\begin{equation}
\pi_t  = -\frac{\lambda_x}{\kappa}x_t \label{cgg_discretion}
\end{equation}
which corresponds to the optimal discretionary solution for rational expectations in \cite{clarida1999science}. This is formalized in the following result.

\begin{result} Coincidence of commitment and discretion under adaptive learning \\
In an adaptive learning model with exogenous or endogenous gain, the optimal Ramsey policies under commitment and discretion coincide. The optimal Ramsey plan is more akin to discretion than to commitment as it does not involve making promises about future policy actions. Optimal policy is thus not subject to the time-inconsistency problem.
\label{result_no_commitment}
\end{result}

To illustrate this result in a parsimonious manner, consider a simplified version of the model. The planner chooses $\{\pi_t, x_t, f_t, k_t^{-1}\}_{t=t_0}^{\infty}$ to minimize
 \begin{align*}
\mathcal{L} &= \E_{t_0}\sum_{t=t_0}^{\infty} \beta^{t-t_0}\bigg\{ \pi_t^2  + \lambda x_t^2 + \varphi_{1,t} (\pi_t -\kappa x_t- \beta f_t +u_t) \\ &+ \varphi_{2,t}(f_t - f_{t-1} -k_t^{-1}(\pi_t - f_{t-1})) + \varphi_{3,t}(k_t^{-1} - \mathbf{g}(\pi_t - f_{t-1})) \bigg\}
 \end{align*}
 where the IS-curve, $x_t = \E_t x_{t+1}+\sigma f_t -\sigma i_t +\sigma r_t^n$, is a non-binding constraint, therefore excluded from the problem, and $\E_t x_{t+1}$ is rational. In this simplified setting, $f_t$ is a stand-in variable capturing inflation expectations and evolves according to a recursive least squares algorithm. The anchoring function $\mathbf{g}$ specifies how the gain $k_t^{-1}$ changes as a function of the current forecast error according to assumption (\ref{anchoring_simple}).\footnote{Here I maintain assumption (\ref{anchoring_simple}) for presentational purposes. It has no bearing on the results.} Note that the problem involves commitment because the monetary authority internalizes the effects of its actions both on the evolution of expectations and on that of the gain. 
 
 After some manipulation, first order conditions reduce to:
 \begin{align}
  2\pi_t +2\frac{\lambda}{\kappa}x_t -\varphi_{2,t}(k_t^{-1} + \mathbf{g_{\pi}}(\pi_t -f_{t-1}))& = 0 \label{simpleFOC1} \\
  -2\beta\frac{\lambda}{\kappa}x_t + \varphi_{2,t} -\varphi_{2,t+1}(1-k_{t+1}^{-1} -\mathbf{g_{f}}(\pi_{t+1} -f_{t})) & = 0 \label{simpleFOC2} 
 \end{align}
Inspection of this system reveals that unlike the rational expectations case ($f_t = \E_t{\pi_{t+1}}$), the optimal solution does not involve lagged multipliers.\footnote{This echoes the findings of \cite{molnar2014optimal} and \cite{gaspar2010inflation}.} This implies that the monetary authority cannot condition the optimal time path of inflation and the output gap on the past; optimal policy is not history-dependent. 

The intuition for this result is easy to see if one compares rational expectations and learning in an infinitely repeated game setting. Under rational expectations, the lagged multiplier appears in the solution because expected inflation is a jump variable. This reflects that expectations fulfill a form of optimality, rendering the private sector a strategic player. The adjustment of expectations under rational expectations enables the central bank to make promises about future policy that are incorporated into expectations. 

Not so for adaptive expectations. Learning agents look exclusively to past data to form expectations. Their expectation formation thus cannot incorporate the policymaker's promises about the future course of policy. In fact, a private sector with adaptive expectations has a pre-specified, non-strategic expectation formation.\footnote{The literature on central bank reputation seeks to address this issue by reintroducing some form of optimality to expectation formation. See \cite{cho1995induction} and \cite{ireland2000expectations}. Another possible remedy is to retain a sufficient degree of forward-looking expectations as in the finite-horizon planning approach advocated by \cite{woodford2019monetary}.}  Therefore, households and firms act as an automaton, leaving the central bank unable to make promises that have any effect on expectations.\footnote{\cite{mele2019perils} report a similar finding in a decreasing gain learning model.Their terminology of contrasting ``inflation-targeting'' with ``price-level targeting" policy rules renders the connection to commitment less immediate, but it is helpful to recall that in the rational expectations NK model, optimal discretionary monetary policy involves inflation stabilization, while optimal commitment entails full price-level stabilization.} 

%%%%%%%%%%%%%%%%%%           IMPLEMENTING THE TC           %%%%%%%%%%%%%%%%%% 

\subsection{Implementing the Ramsey policy: the optimal interest rate sequence}\label{implement}
Having a characterization of optimal policy in the anchoring model as a first-order condition, the next relevant question is how the central bank should set its interest rate tool in order to implement the target criterion in (\ref{target}). As emphasized in Section \ref{ramsey}, the nonlinearity of the model does not admit an analytical answer to this question. I therefore solve for the optimal interest rate policy numerically using global methods. 

For the numerical approach I rely on the calibration of model parameters outlined in Table \ref{calibration1}. Where possible, I adopt standard parameters from the literature. In particular, I rely on the parameterization in \cite{woodford2011interest}. The other motivation for the choice of some parameters is ease of interpretation. Therefore I restrict the shocks to be iid, and I set $\lambda_x = 0.01$ so that a concern for output gap stabilization should not be the main driver of the central bank's actions, yet the right hand side of the target criterion in (\ref{target}) should also not be trivial. Reflecting this, I set a small, but nonzero coefficient on the output gap when I compare the optimal solution for the interest rate with an interest rate sequence brought about by a Taylor rule.
\begin{center}
\begin{table}
\begin{tabular}{ c | c  | l }
 $\beta$ & 0.99 & stochastic discount factor \\  \hline
 $\sigma$ & 1  & intertemporal elasticity of substitution \\  \hline
 $\alpha$ & 0.5 &  Calvo probability of not adjusting prices \\\hline
 $\psi_{\pi} $& 1.5  & coefficient of inflation in Taylor rule \\\hline
 $\psi_x$ & 0.01   & coefficient of the output gap in Taylor rule  \\\hline %% <---- Diffs. wrt calibration2
 $\bar{g}$ & $0.145$  & initial value of the gain \\\hline %% <----
%& & \\ [-1em] % this adds an extra empty row, and decreases its size, so it looks as if thetbar's row was higher
% $\tilde{\theta}$ &  2.5  & threshold value for criterion of endogenous gain choice \\ \hline %% <----
%  $\tilde{\kappa}$ &  0.2  & scaling parameter of gain for forecast error variance estimation \\ \hline %% <----
    $\rho_r$ & 0 &   persistence of natural rate shock \\ \hline %% <---
    $\rho_i$ & 0 &  persistence of monetary policy shock  \\ \hline
    $\rho_u$ & 0  &  persistence of cost-push shock  \\ \hline
    $\sigma_r$ & 1 & standard deviation of natural rate shock  \\ \hline
%    $\sigma_i$ &  1  &standard deviation of monetary policy shock  \\ \hline %% <----
    $\sigma_u$ & 1 & standard deviation of cost-push shock   \\ \hline  
    $\lambda_x$ & 0.01 & weight on the output gap in central bank loss   \\ \hline  % <----
    $\lambda_i$ & 0 & weight on the interest rate in central bank loss   \\ \hline  
    $\rho_k$ & 0.5 & persistence of the gain in the smooth anchoring function   \\ \hline  % <----
    $\gamma_k$ & 0.001 & weight on squared forecast errors in the smooth anchoring function   \\ \hline   % <----
\end{tabular}     
      \caption{Calibrated parameters}  \label{calibration1}
 \end{table}
\end{center}


\vspace{-1.0cm}

I initialize the value of the gain at $\bar{g}=0.145$, the value for the constant gain estimated by \cite{carvalho2019anchored}. As I discuss in Section \ref{opt_TR}, this parameter has important implications for model dynamics when the anchoring function takes the CUSUM-form. However, the target criterion (\ref{target}) requires that the derivatives of the anchoring function exist, wherefore in this section I use a smooth specification for the anchoring function of the form:
\begin{equation}
 k^{-1}_t  = \rho_k k^{-1}_{t-1} + \gamma_k fe_{t-1}^2 \label{smooth} 
\end{equation}
Thus $\bar{g}$ does not have a strong bearing on model dynamics. Instead, $\rho_k$ and $\gamma_k$ become important. As no anchoring function of the form of (\ref{smooth}) has been used in the literature, I choose values for these two parameters by experimentation with simulations of the model using different values. In my current calibration, the gain is generally decreasing, but can be driven up considerably in response to shocks. 

Before moving to the numerical solution for the optimal interest rate sequence, I note that the functional specification of (\ref{smooth}) reflects the criteria that the function should be smooth and imply an overall decreasing gain sequence with occasional jumps up. Nonetheless, other functional forms are possible. Without any theoretical literature on anchoring functions to fall back on, choosing the right specification is an empirical question. For this reason, in Section \textcolor{red}{X} I estimate the anchoring function. 

App. \ref{pea} outlines my preferred solution procedure, the parameterized expectations approach, while App. \ref{vfi} gives the details of the parameterized value function iteration approach I implement as a robustness check. The main output of this procedure is an approximation of the optimal interest rate policy as a function of the vector of state variables. Since the relevant state variables are the gain, expected mean inflation and the exogenous states at time $t$ and $t-1$, the state vector is six-dimensional:

\begin{equation}
X_t = (\bar{\pi}_{t-1}, k_{t-1}^{-1}, r^n_t, u_t, r^n_{t-1}, u_{t-1})
\end{equation}

As a first step, I plot how the approximated policy function depends on $\bar{\pi}_{t-1}$ and $k_{t-1}^{-1}$, while keeping all the other states at their mean. One can thus interpret the result, depicted on Fig. \ref{di}, as a partial derivative of the interest rate with respect to $k_{t-1}^{-1}$ (panel (a)) and $\bar{\pi}_{t-1}$ (panel (b)) respectively.

\begin{figure}[h!]
\subfigure[]{\includegraphics[scale=0.17]{\myFigPath analyze_opt_policy_ik}}
\subfigure[]{\includegraphics[scale=0.17]{\myFigPath analyze_opt_policy_ip}}
\caption{Comparative statics: policy in function of the endogenous states}
\label{di}
\end{figure}

As illustrated by Fig. \ref{di}, optimal interest-rate setting responds linearly and very sensitively to the stance of expectations, captured by $\bar{\pi}_{t-1}$. As panel (b) suggests, if expected mean inflation drifts to just -0.05\%, the interest rate drops to almost -6\%! By contrast, the interest rate response to the gain in isolation is small in magnitude, on the order of $10^{-6}$. 

Let's put some numbers on this, recognizing the fact that the gain and expected mean inflation in the model move in tandem. Regressing $\bar{\pi}$ on $k^{-1}$ in a sample where $\bar{\pi}$ is always positive reveals that for a unit increase in the gain, the long-run inflation expectation increases by 5\%-points.\footnote{To be sure, I include the exogenous states in the regressor matrix to control for the effect of forecast errors on $\bar{\pi}$. Note also that whether $\bar{\pi}$ drifts up or down following in increase in the gain depends on the sign of the current forecast error. To control for this, I consider a sample where $\bar{\pi}$ is always positive.} I then compute the average uptick in $k^{-1}$ in the sample, obtaining 0.00088981. This means that on average in the sample, expectations threaten to become unanchored by an upward shift of the gain of approximately 0.0009, which is associated with a drift in $\bar{\pi}$ of 0.0044. The optimal policy function implies an interest rate response of 0.4547\%-points. The interpretation is that on average, when expectations threaten to become unanchored upward, the central bank should raise the interest rate by approximately 0.45\%-points.\footnote{Because of the non-monotonicity of the interest rate response to the gain, the optimal change in the interest rate for a same size unanchoring downward is a decrease of only 0.4541\%-points.} This is a large response; the message is that monetary policy prefers to act aggressively initially to prevent expectations from becoming unanchored. This way, the bank can avoid much larger interventions once expectations are massively unanchored. 

This notion is reflected in the fact that the partial derivative of the interest rate policy with respect to the gain is non-monotonic. As panel (a) emphasizes, increases in the gain are associated with increases in the interest rate as long as the gain is small. Once the gain exceeds the value of 0.02, the interest rate response associated with increases in the gain becomes negative. This non-monotonicity is due to the fact that high gains lead to oscillatory forecast errors, which feed back into the observables, increasing inflation and output gap volatility. Therefore the central bank acts aggressively in the moment expectations are becoming unanchored to prevent unanchoring. Once expectations are already unanchored, fighting this becomes too costly in terms of inflation and output gap volatility, prompting the bank's less aggressive response. 

This highlights another intertemporal tradeoff that anchoring expectation formation introduces to central banking: the tradeoff between long-run benefits versus short-run costs of anchoring expectations. If expected mean inflation is at the central bank's target, zero, then, provided there are no exogenous disturbances as in Fig. \ref{di}, the monetary authority need not intervene in the economy and can keep the interest rate at the steady state value of zero. From the viewpoint of the central bank, this is a desirable situation since it involves less volatility in the central bank's target variables than if expectations are unanchored. However, achieving this long-run goal requires subduing unanchored expectations. Thus the central bank is willing to temporarily increase volatility in order to reduce it in the long-run. The heightened short-run volatility is the cost the bank has to pay to anchor expectations and enjoy the long-run stability anchored expectations involve. 

The presence of this volatility tradeoff also implies that optimal policy aggressiveness is time-varying. This result emerges clearly if one considers another way to investigate optimal policy in the model. I thus next compare the evolution of observables conditional on a particular history of exogenous disturbances across two specifications of monetary policy: one that follows the target criterion in (\ref{target}) and one that follows a Taylor-rule with standard parameter values. In particular, for this exercise I use $\psi_{\pi}=1.5$ and $\psi_x = 0.01$.\footnote{Given that in my calibration the central bank does not attach any weight to interest rate stabilization ($\lambda_i=0$), this choice of Taylor-rule coefficients would not be optimal in the RE version of the model, since $\lambda_i=0$ implies infinity as the optimal choice for both $\psi_{\pi}$ and $\psi_x$. My choice of $\lambda_i=0$ is motivated by keeping the interpretation as simple as possible; the values $\psi_{\pi}=1.5$ and $\psi_x = 0.01$ reflect standard Taylor rule coefficient values of an inflation-targeting central bank with no explicit mandate for output gap stabilization.}

\begin{figure}[h!]
\subfigure[]{\includegraphics[scale=0.24]{\myFigPath implement_anchTC_obs_TR}}
\hfill % this is great to intro dpace between subfigures
\subfigure[]{\includegraphics[scale=0.24]{\myFigPath implement_anchTC_obs}}
\caption{}
\label{pea_TCvsTR}
\end{figure}

Fig. \ref{pea_TCvsTR} depicts the evolution of inflation, the output gap and the interest rate conditional on a sequence of exogenous disturbances. Panel (a) and (b) show the outcome achieved by adhering to the Taylor rule and the target criterion (\ref{target}) respectively. One clearly sees the features of optimal policy under anchoring compared to a standard Taylor-rule approach. Due to the low value of $\psi_x$ here, the Taylor-rule-following central bank resolves the inflation-output gap tradeoff by allowing the output gap to sink far into negative territory and instead stabilizing inflation. Instead, optimal policy uses the interest rate tool much more aggressively to achieve a nearly identical inflation series, yet a much more stable output gap series. In particular early in the sample, the central bank is willing to raise and lower the interest rate massively in order to eliminate any potential of massive unanchoring. But even late in the sample, the interest rate can fluctuate wildly between $\pm5\%$ in order to quench any potential unanchoring brought about by unforecastable disturbances. Thus one observes both features of optimal policy foreshadowed in the previous discussions: i) anchoring alleviates the intratemporal tradeoff between inflation and the output gap, ii) there is an intertemporal volatility tradeoff between the short-run cost and the long-run benefit of anchoring expectations, for which reason the central bank prefers to avoid expectations getting unanchored in the first place. A time-varying level of aggressiveness is the result.

% Comparisons to a TR?

  
%%%%%%%%%%%%%%%%%%           OPTIMAL TAYLOR RULE           %%%%%%%%%%%%%%%%%% 
\subsection{Optimal Taylor rule under anchoring}\label{opt_TR}
The optimal commitment rational expectations version of the targeting rule (\ref{target}) would take the form of (\ref{cgg_discretion}) with an additional $x_{t-1}$ term capturing the benefits of commitment. Therefore, due to its purely forward-looking nature, a Taylor-type interest rate rule is not fully optimal in the rational expectations NK model. Since Result \ref{result_no_commitment} tells us that in the anchoring model there is no distinction between commitment and discretion, there is a renewed interest in formulating monetary policy as a Taylor rule, as these types of rules are no longer excluded from the class of fully optimal rules. In particular, the targeting rule (\ref{target}) is also purely forward-looking. Therefore I now consider the restricted set of Taylor-type policy rules and ask what values of the Taylor-rule coefficients are optimal in the case of the anchoring model.

I compute the optimal Taylor rule coefficients numerically via grid search for a simulation of both the rational expectations and learning versions of the model.\footnote{To concentrate on the intuition, in this section I implement the learning algorithm such that only the constant is learned. The general formulation has qualitatively similar features but is more impacted by small-sample concerns prevalent in simulations.} Table \ref{calibration2} reproduces the calibration of Table \ref{calibration1}, highlighting in blue differences in parameter values between the two sections. I make the following changes. For this section, I shut off the monetary policy parameters $\lambda_i$, $\lambda_x$ and $\psi_x$ in order to focus on the role of inflation in the central bank's problem and thus on the optimal choice of inflation aggressiveness, $\psi_{\pi}$. As usual, this choice is motivated by the ease of interpretation. I also add a persistent monetary policy shock in order to be able to discuss model responses to innovations to the interest rate.

For the simulations in this section, I use the discrete CUSUM-inspired anchoring function outlined in App. \ref{alternative_criteria}. This allows me to partition impulse responses into two discrete categories depending on whether expectations are anchored or not.\footnote{App. X implements a robustness check using the smooth anchoring function in equation (\ref{smooth}).} The learning parameters $\bar{g}, \tilde{\theta}$ and $\tilde{\kappa}$ require some discussion. While the choice of $\tilde{\kappa}$ only matters for the smoothness of the endogenous gain decision and thus can be set relatively freely, the threshold $\tilde{\theta}$ has more bearing on the behavior of the model. Intuitively, the higher $\tilde{\theta}$, the more forecast error volatility agents in the economy are willing to tolerate before switching to a constant gain. Experimenting with different values reveals that once $\tilde{\theta}$ is higher than a particular threshold, expectations are anchored for any value of $\psi_{\pi}$. Analogously if $\tilde{\theta}$ is below a lower threshold, expectations are always unanchored regardless of the value of $\psi_{\pi}$. My choice of $\tilde{\theta}=2.5$ is thus motivated by assigning a value for which the comparative static of anchoring with respect to $\psi_{\pi}$ is meaningful. 

\begin{center}
\begin{table}
\begin{tabular}{ c | c  | l }
 $\beta$ & 0.99 & stochastic discount factor \\  \hline
 $\sigma$ & 1  & intertemporal elasticity of substitution \\  \hline
 $\alpha$ & 0.5 &  Calvo probability of not adjusting prices \\\hline
 $\psi_{\pi} $& 1.5  & coefficient of inflation in Taylor rule \\\hline
 $\textcolor{blue}{\psi_x}$ & 0   & coefficient of the output gap in Taylor rule  \\\hline
 $\bar{g}$ & $0.145$  & \textcolor{blue}{value of the constant gain} \\\hline
& & \\ [-1em] % this adds an extra empty row, and decreases its size, so it looks as if thetbar's row was higher
 $\textcolor{blue}{\tilde{\theta}}$ &  2.5  & threshold value for criterion of endogenous gain choice \\ \hline
  $\textcolor{blue}{\tilde{\kappa}}$ &  0.2  & scaling parameter of gain for forecast error variance estimation \\ \hline
    $\rho_r$ & 0 &   persistence of natural rate shock \\ \hline
    $\textcolor{blue}{\rho_i}$ & 0.6 &  persistence of monetary policy shock  \\ \hline
    $\rho_u$ & 0  &  persistence of cost-push shock  \\ \hline
    $\sigma_r$ & 1 & standard deviation of natural rate shock  \\ \hline
    $\sigma_i$ &  1  &standard deviation of monetary policy shock  \\ \hline
    $\sigma_u$ & 1 & standard deviation of cost-push shock   \\ \hline  
    $\textcolor{blue}{\lambda_x}$ & 0 & weight on the output gap in central bank loss   \\ \hline  
    $\lambda_i$ & 0 & weight on the interest rate in central bank loss   \\ \hline  
\end{tabular}     
      \caption{Calibrated parameters}  \label{calibration2}
 \end{table}
\end{center}

\vspace{-1.4cm}

Unlike for the smooth anchoring function in (\ref{smooth}), the choice of $\bar{g}$ has considerable implications for model dynamics. In particular, as periodically noted in the adaptive learning literature, constant gain learning models have a tendency to produce impulse responses that exhibit damped oscillations.\footnote{Authors making explicit note of this phenomenon include \cite{evans_honkapohja2001}, \cite{evans2013bayesian} and \cite{anufriev2012evolutionary}.} The reason is that under an adaptive learning framework, forecast errors following an impulse are oscillatory.\footnote{App. \ref{app_oscillations} presents a simple illustration for why this is the case.} In fact, the higher the learning gain, the higher the amplitude of forecast error oscillations. If the gain is high enough, the oscillations become explosive. 

Unfortunately, the model gives no guidance on the appropriate value for $\bar{g}$.\footnote{The analogy of the Kalman gain from the Kalman filter does not prove helpful either because it requires a steady state forecast error variance matrix which is not available in a learning context.} I thus turn to the thin literature on estimating learning gains. I assign the value  0.145, obtained by \cite{carvalho2019anchored}, to my knowledge the only study to estimate the value of the constant gain for an endogenous gain model. It has to be observed, however, that this is a significantly higher value than what was found in the literature estimating gains for constant gain learning. \cite{branch2006simple}'s number of 0.062 is quite close to \cite{eusepi2018limits}'s estimate of 0.05, while \cite{milani2007expectations} finds an even lower number of 0.0183. Studies that use calibrated gains such as \cite{williams2003adaptive} or \cite{orphanides2005decline} tend to experiment with a range of values in the $[0.01,0.1]$ interval. The value of 0.05 has attained particular prominence, but also much lower numbers have been used, such as 0.002 in \cite{eusepi2011expectations}. I speculate that \cite{carvalho2019anchored}'s estimate is so large relative to other estimates because they estimate a switching-gain model, while the rest of the estimates come from constant gain specifications. In an endogenous gain specification, data needs to assign a higher value to the constant gain parameter to rationalize the same average gain in the time series. With these caveats in mind, I adopt \cite{carvalho2019anchored}'s value.

Table \ref{par_opt} presents an overview of the optimal Taylor rule coefficient $\psi_{\pi}$ for the rational expectations and anchoring models. The table also compares the baseline parameterization with several alternatives. One notices that if the central bank has no concern to stabilize the output gap ($\lambda_x = 0$) or the nominal interest rate ($\lambda_i =0$), $\psi_{\pi}^{RE}$ is infinity. This is because if the central bank suffers no loss upon output variation, then the fact that the divine coincidence doesn't hold does not pose a problem. Similarly, if the monetary authority is willing to allow the nominal interest rate to fluctuate vastly in order to stabilize inflation, this also allows the central bank to be infinitely aggressive on inflation. 

\begin{center}
\begin{table}[h!]
\begin{tabular}{ c | c | c }
 & $\psi^{*,RE}_{\pi}$ & $\psi^{*,anchoring}_{\pi}$  \\  \hline
  Baseline  & $\infty$  & 1.6243 \\  \hline
%  $\psi_{x} =1$  & $\infty$  & 1 \\  \hline
 $\lambda_x =1 $ & 2.1042  & 1.0571 \\  \hline
 $\lambda_i =1 $ &  1.1  & 1.0978 \\  \hline
%  $\lambda_x =1, \lambda_i =1.6092 $ & $\infty$  &  1.1580 \\  \hline
\end{tabular}     
      \caption{Optimal coefficient on inflation, RE against learning for alternative parameters}  \label{par_opt}
 \end{table}
\end{center}

\vspace{-1.4cm}
The main observation however is that $\psi_{\pi}$ is always lower for the anchoring model than for the RE model. This is reinforced in Fig. \ref{fig_loss} which plots the central bank's loss in the RE and learning models for various values of $\psi_{\pi}$ but otherwise the baseline specification. The message is clear: while for rational expectations, the loss is strictly decreasing in $\psi_{\pi}$, this is not the case for the anchoring model. 

This echoes the analytical results of Section \ref{implement}: the anchoring mechanism introduces an intertemporal cost-benefit tradeoff for the central bank. On the one hand, as we will see shortly in Fig. \ref{IRF}, having unanchored expectations increases the volatility of the observables. This results in the central bank wishing to anchor expectations. As Fig. \ref{anchor_psi} makes clear, this requires raising $\psi_{\pi}$. But in an environment where agents know the Taylor rule, a higher coefficient on inflation will lead to initially higher volatility due to agents anticipating the endogenous responses of the nominal interest rate far in the future.
\begin{figure}[h!]
\subfigure[RE]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath plot_sim_loss_loss_RE_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_4_thettilde_2_5_kap_0_8_lamx_0_lami_0_2020_02_09}}
\subfigure[Learning]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath plot_sim_loss_loss_again_critCUSUM_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_4_thettilde_2_5_kap_0_8_lamx_0_lami_0_2020_02_09}}
\caption{Central bank loss function as a function of $\psi_{\pi}$}
\floatfoot{}
\label{fig_loss}
\end{figure}

To understand where the intertemporal volatility tradeoff is coming from, consider Fig. \ref{IRF}, portraying the impulse responses of the model after a contractionary monetary policy shock. The red dashed lines show the responses of the observables in the rational expectations version of the model. The blue lines show the responses in the learning model, on panel (a) conditional on expectations being anchored when the shock hits, on panel (b) being unanchored upon the arrival of the shock. 
\begin{figure}[h!]
\subfigure[RE against learning, expectations anchored]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath command_IRFs_anchoring_RIR_LH_anchmonpol_again_critCUSUM_constant_only_2020_02_10}}
\subfigure[RE against learning, expectations unanchored]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath command_IRFs_anchoring_RIR_LH_unanchmonpol_again_critCUSUM_constant_only_2020_02_10}}
\caption{Impulse responses after a contractionary monetary policy shock}
\floatfoot{Shock imposed at $t=25$ of a sample length of $T=400$ (with 100 initial burn-in periods), cross-sectional average with a cross-section size of $N=100$. For the rest of the paper, I keep these simulation values unless otherwise stated. For the learning model, the remark refers to whether expectations are anchored at the time the shock hits.}
\label{IRF}
\end{figure}

Not only do the impulse responses show the usual behavior of learning models - dampened responses and increased persistence. More importantly, responses differ strongly depending on whether expectations are anchored or not when the shock hits. In particular, if expectations are anchored, responses are closer to rational expectations than when expectations are unanchored. Moreover, when expectations are unanchored, the endogenous responses of the observables become much more volatile, indeed, oscillatory. This makes intuitive sense: expectations being unanchored reflects the fact that firms and households are confronted with an environment that does not line up with their currently held perceived law of motion. They thus believe that a structural change has occurred and are therefore revising their expectations. Expectations are therefore fluctuating strongly, and as they feed back to the observables, the latter inherit their volatility. 

\begin{figure}[h!]
\subfigure[$\psi_{\pi} = 1.01$]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCUSUM_constant_only_params_psi_pi_1_01_psi_x_0_gbar_0_145_thetbar_4_thettilde_2_5_kap_0_8_lamx_0_lami_0_2020_02_09}}
\subfigure[$\psi_{\pi} = 1.5$]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCUSUM_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_4_thettilde_2_5_kap_0_8_lamx_0_lami_0_2020_02_09}}
\subfigure[$\psi_{\pi} = 2$]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCUSUM_constant_only_params_psi_pi_2_psi_x_0_gbar_0_145_thetbar_4_thettilde_2_5_kap_0_8_lamx_0_lami_0_2020_02_09}}
\caption{Cross-sectional average inverse gains for various values of $\psi_{\pi}$}
\floatfoot{}
\label{anchor_psi}
\end{figure}
Therefore, to avoid the volatility that results from unanchored expectations, the central bank wishes to anchor expectations. What choice of $\psi_{\pi}$ will do the job? Fig. \ref{anchor_psi} provides the answer. The figure shows the cross-sectional average of inverse gains that result when $\psi_{\pi}$ takes on different values. Clearly, a higher $\psi_{\pi}$ results in lower and decreasing gains.\footnote{As I remark in Section \ref{learning}, if one uses the anchoring criterion of \cite{carvalho2019anchored}, this conclusion is overturned.} Thus a central bank aiming to anchor expectations needs to employ a high $\psi_{\pi}$. This is also intuitive. A more aggressive central bank can signal to the public that it is determined to achieve the announced inflation target. Thus agents can rest assured that their believed inflation target is indeed the one the central bank can and will implement. Whenever $\psi_{\pi}$ is low, however, the central bank is willing to tolerate bigger deviations from the target. This opens the door to speculation about whether the central bank is really committed to the target. In this case, deviations from the believed target of the same magnitude can cast doubt on the central bank's commitment, so that agents decide to monitor recent data closely to learn the seemingly shifting average value of inflation.

But if unanchored expectations cause heightened volatility, and being aggressive on inflation is able to anchor expectations, why does the monetary authority optimally choose a lower value for $\psi_{\pi}$ than under rational expectations? The reason is that conditional on being unanchored, a higher $\psi_{\pi}$ actually causes higher volatility than a lower one. This can be seen on Fig. \ref{IRF_unanchored_psi} which depicts the same impulse responses to a contractionary monetary policy shock as Fig. \ref{IRF}, focusing however only on responses conditional on expectations being unanchored upon the shock. It shows these responses for three different values of $\psi_{\pi}$.

\begin{figure}[h!]
\subfigure[$\psi_{\pi} = 1.01$]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath command_IRFs_anchoring_RIR_LH_unanch_monpol_again_critCUSUM_constant_only_psi_pi_1_01_2020_02_10}}
\subfigure[$\psi_{\pi} = 1.5$]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath command_IRFs_anchoring_RIR_LH_unanch_monpol_again_critCUSUM_constant_only_psi_pi_1_5_2020_02_10}}
\subfigure[$\psi_{\pi} = 2$]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath command_IRFs_anchoring_RIR_LH_unanch_monpol_again_critCUSUM_constant_only_psi_pi_2_2020_02_10}}
\caption{Impulse responses for unanchored expectations for various values of $\psi_{\pi}$}
\floatfoot{}
\label{IRF_unanchored_psi}
\end{figure}

As the figure shows, a high $\psi_{\pi}$ leads to more volatility than a low one does. The intuition is a little subtle. Since expectations are unanchored, they are also volatile. This implies that inflation far ahead in the future is expected to fluctuate strongly. Since agents know the Taylor rule, this also means that they expect the nominal interest rate far in the future to respond. The more aggressive the central bank, the stronger an interest rate response will the agents expect. This however feeds back into current output gaps and thus inflation. Higher overall volatility is the result.

Unexpectedly, the model dynamics here echo the predictions of \cite{ball1994credible}. But the underlying channels are quite different. \cite{ball1994credible} observes that, contrary to conventional wisdom, rational expectations New Keynesian models imply expansionary disinflations. To reconcile this model feature with data pointing to the costliness of disinflations, he concludes that central bank announcements must suffer from credibility issues. 

Note that in the present context, when expectations are anchored (panel (a) of Fig. \ref{IRF}), impulse responses do not exhibit this feature. However, when expectations are unanchored (panel (b) of Fig. \ref{IRF}), impulse responses look exactly as \cite{ball1994credible} predicts: we obtain an expansionary disinflation. 

The reason this is happening is that when agents know the Taylor rule, long-horizon expectations of the interest rate move in tandem with the same expectations of inflation in the far future. A current disinflation lowers long-horizon inflation expectations, leading the public to expect low interest rates far out in the future. Through the NKIS-curve (Equation \ref{NKIS}), this stimulates current output.\footnote{The extension in which the public has to learn the Taylor rule is interesting in this regard. As expected, the Ball-type disinflationary boom does not initially show up in impulses responses obtained in that extension. However, as the agents are learning the Taylor rule, the expansionary disinflation slowly reemerges in the impulse responses.} But the absence of the ``Ball-effect" from the anchored expectations impulse responses indicates that the channel is only operational when expectations are moving sufficiently. Thus I arrive at a different conclusion than \cite{ball1994credible}; instead of credibility issues, it is anchored expectations that are responsible for the absence of expansionary disinflations of the type seen on Fig \ref{IRF}, panel (b).

Thus restricting the central bank's reaction function to a Taylor rule, I uncover an intertemporal volatility tradeoff in addition to the tradeoffs discussed in Section \ref{implement}. Unsurprisingly, it is desirable for the central bank to anchor expectations. It is also intuitive that being aggressive on inflation helps to anchor expectations. However, less intuitive is the fact that the optimal degree of aggressiveness on inflation is lower than under rational expectations. This has to do with the heightened volatility of the expectations process when $\psi_{\pi}$ is high. A higher $\psi_{\pi}$ increases the response of future nominal interest rate expectations, thus raising the feedback from expectations to current observables. Thus the central bank faces an intertemporal tradeoff: to reduce volatility in the long-run, it seeks to anchor expectations. However, the price the bank has to pay in order to get expectations anchored is higher short-run volatility. Thus, the monetary authority trades off the short-run cost with the long-run benefit of anchoring expectations.

\subsection{Discussion}\label{discussion_results}

The analytical and numerical analysis of monetary policy in the anchoring model has led to the following conclusions. As seen in Result \ref{result_no_commitment}, optimal monetary policy is time-consistent and does not exhibit history-dependence. Therefore, optimal responses to shocks feature intertemporal tradeoffs that allow the central bank to postpone the intratemporal tradeoff between inflation and output gap stabilization to the future. Ironically, although the first-order conditions from the Ramsey problem have the flavor of discretion due to the absence of lagged multipliers, the optimal response to shocks prescribed by the target criterion (\ref{target}) thus qualitatively resembles commitment as it spreads out the effect of shocks over time. 

The implementation of the target criterion calls for an interest rate policy function that is very responsive to the stance of expected mean inflation. As we saw in Section \ref{implement}, an average upward unanchoring of expectations induces the central bank to raise the interest rate by 0.4547\%-points.The simulation in Section \ref{implement} also shows that, accordingly, the optimal interest rate path is more volatile than a path generated by a standard Taylor rule, especially early in the sample. This comes from optimal policy responding aggressively to any sign of expectations threatening to become unanchored; a phenomenon that is especially likely early in the learning process.

At the same time, as seen in Section \ref{opt_TR}, restricting the central bank to follow a Taylor rule and choosing its response coefficients to minimize the central bank's loss function leads to lower Taylor-rule coefficients than under rational expectations. This reflects the intertemporal cost-benefit tradeoff the central bank faces as it seeks to anchor expectations but to avoid causing too much volatility as it is doing so.  

Taken together, these results suggest that optimal policy aggressiveness should be time-varying: the central bank should condition its tolerance of inflation on the stance of anchoring. Since in the anchoring model volatility is increasing in aggressiveness, the central bank prefers to be less aggressive on inflation than under rational expectations in general. However, unanchored expectations also lead to higher volatility. To avoid volatility associated with unanchored expectations, then, the central bank needs to act more aggressively any time it sees signs of  unanchoring. The policy function of Section \ref{implement} captures this notion as it expresses the interest rate choice as a function of the gain and expected mean inflation. If a central bank is reluctant to employ a policy function other than a Taylor rule, then my analysis underscores that of \cite{LUBIK201685} in predicting that the optimal Taylor-rule coefficients should be time-varying in a way that reflects the stance of anchoring. 



 %%%%%%%%%%%%%%%%%%           CONCLUSION            %%%%%%%%%%%%%%%%%% 
%\newpage
\section{Conclusion}\label{conclusion}
Central bankers frequently voice a concern to anchor expectations. Absent a behavioral theory of anchored expectations, it is difficult to understand how such a concern affects the conduct of monetary policy. In this paper, I have laid out a simple theory of anchoring expectation formation based on adaptive learning models with an endogenous gain. The analysis of the Ramsey policy through analytical and numerical methods yields a number of differences compared to the rational expectations version of the model. 

Optimal policy in the anchoring model is time consistent, for which reason it can be expressed as a purely forward-looking target criterion for the policymaker to follow. The target criterion prescribes how the policymaker can exploit the learning mechanism to spread out the effect of shocks over time, adding two intermporal channels to the tradeoff between inflation and the output gap. The sensitivity of the policy function to the stance of anchoring highlights, however, that there is no free lunch: the monetary authority needs to dampen volatility at the same time as guard against the possibility of unanchoring expectations. Thus optimal policy faces an intertemporal volatility tradeoff in which long-run stability can only be achieved at the cost of short-run volatility. \cite{carvalho2019anchored}'s estimate of the time series of the learning gain tells exactly this story on the example of the Volcker deflation: the rise and fall of volatility as the Fed sought to subdue unanchored expectations. 

A number of interesting questions emerge from the analysis of monetary policy and the anchoring expectation formation. One may wonder whether the central bank can use tools other than its leading interest rate to anchor expectations. Especially concerns around a binding zero lower bound would motivate the use of alternative monetary policy tools. Thus the interaction between anchoring and central bank communication, in particular forward guidance would be worthwhile to examine. An interesting avenue for future research, then, would be to make explicit the communication policy of the central bank to investigate whether anchoring expectation formation could help to resolve the forward guidance puzzle. 

This, however, requires overcoming the implication of Result \ref{result_no_commitment} that adaptive expectations are not able to incorporate any information that is not embedded in the current state vector. One option is to model central bank communication similarly to news shocks in the sense of \cite{beaudry2006stock}. In this case, the anchoring model is likely to deliver differing predictions regarding the effectiveness of Delphic versus Odyssean forward guidance (\cite{campbell2012macroeconomic}) because sharing the central bank's forecasts would not constitute a questioning of the interest rate reaction function, while committing to a future interest rate path would.

In general, extensions to the anchoring expectation formation proposed here would be of interest. The choice of the gain could be endogenized using approaches that allow the private sector to choose its forecasting behavior in an optimizing fashion, perhaps by selecting among competing forecasting models as in \cite{Branch2011} or by choosing the size of the gain to minimize the estimated forecast error variance. 

Lastly, a foray into the empirics of anchoring expectation formation is important to get a clearer idea of the expectation formation process of the public. In practice, it is likely that this process is heterogenous, not just across households and firms, but also within various demographic groups or sectors of the economy. If so, then monetary policy would need to collect and monitor a host of long-run expectations time series to manage the challenge of anchoring expectations. 



 %%%%%%%%%%%%%%%%%%           BIBLIOGRAPHY            %%%%%%%%%%%%%%%%%% 
\clearpage
\newpage
\bibliographystyle{chicago}
%\bibliography{../../literature/ref_next}
\bibliography{\myBibPath ref_next}
%\nocite{*}

 %%%%%%%%%%%%%%%%%%           APPENDIX            %%%%%%%%%%%%%%%%%% 
\newpage
\appendix

% the following command makes equation numbering include the section first, but just for what follows
\numberwithin{equation}{section}


\section{Compact model notation}\label{app_compact} 
The A-matrices are given by
\begin{align}
%A_p^{RE} & = \begin{pmatrix} \beta + \frac{\kappa\sigma}{w} (1-\psi_{\pi}\beta) & \frac{\kappa}{w} & 0\\
% \frac{\sigma}{w} (1-\psi_{\pi}\beta) & \frac{1}{w}& 0\\ 
%\psi_{\pi}\big( \beta + \frac{\kappa\sigma}{w} (1-\psi_{\pi}\beta) \big) +\psi_x\frac{\sigma}{w} (1-\psi_{\pi}\beta)&  \psi_x (\frac{1}{w})+ \psi_{\pi} (\frac{\kappa}{w})& 0\end{pmatrix} \quad \\
%A_s^{RE} &= \begin{pmatrix}   \frac{\kappa\sigma}{w}  &-\frac{\kappa\sigma}{w}  & 1-\frac{\kappa\sigma\psi_{\pi}}{w}\\
% \frac{ \sigma}{w} &  -\frac{\sigma}{w} & -\frac{\sigma\psi_{\pi}}{w}\\ 
% \psi_x( \frac{\sigma}{w}) + \psi_{\pi}( \frac{\kappa\sigma}{w}) & \psi_x(- \frac{\sigma}{w}) + \psi_{\pi}(- \frac{\kappa\sigma}{w}) +1 &  \psi_x(-\frac{\sigma\psi_{\pi}}{w}) + \psi_{\pi}( 1-\frac{\kappa\sigma\psi_{\pi}}{w})\end{pmatrix}  
%\\
A_a & = \begin{pmatrix} g_{\pi a} \\ g_{x a} \\ \psi_{\pi}g_{\pi a} + \psi_xg_{x a}
\end{pmatrix}
\quad A_b = \begin{pmatrix} g_{\pi b} \\ g_{x b} \\ \psi_{\pi}g_{\pi b} + \psi_xg_{x b}
\end{pmatrix}
 \quad A_s = \begin{pmatrix} g_{\pi s} \\ g_{x s} \\ \psi_{\pi}g_{\pi s} + \psi_xg_{x s} + \begin{bmatrix} 0 & 1& 0\end{bmatrix}
\end{pmatrix} \\
g_{\pi a} & =(1-\frac{\kappa\sigma\psi_{\pi}}{w} )  \begin{bmatrix}(1-\alpha)\beta, \kappa\alpha\beta, 0 \end{bmatrix} \\
g_{x a} & =  \frac{-\sigma\psi_{\pi}}{w} \begin{bmatrix}(1-\alpha)\beta, \kappa\alpha\beta, 0 \end{bmatrix}\\
g_{\pi b} & = \frac{\kappa}{w} \begin{bmatrix}\sigma(1-\beta\psi_{\pi}), (1-\beta-\beta\sigma\psi_x, 0 \end{bmatrix}\\
g_{x b} & = \frac{1}{w} \begin{bmatrix}\sigma(1-\beta\psi_{\pi}), (1-\beta-\beta\sigma\psi_x, 0 \end{bmatrix} \\
g_{\pi s} & = (1-\frac{\kappa\sigma\psi_{\pi}}{w} )\begin{bmatrix} 0&0&1 \end{bmatrix} (I_3 - \alpha\beta P)^{-1} -\frac{\kappa\sigma}{w}\begin{bmatrix} -1&1&0 \end{bmatrix} (I_3 -\beta P)^{-1}\\
g_{x s} & =  \frac{-\sigma\psi_{\pi}}{w} \begin{bmatrix} 0&0&1 \end{bmatrix}(I_3 - \alpha\beta P)^{-1}  -\frac{\sigma}{w}\begin{bmatrix} -1&1&0 \end{bmatrix}(I_3 -\beta P)^{-1}\\
w & = 1+\sigma\psi_x +\kappa\sigma\psi_{\pi}
\end{align}
The matrices of the state transition equation (\ref{LOM_s}) are
 \begin{align}
 h  & \equiv \begin{pmatrix} \rho_r & 0 & 0 \\ 0& \rho_i & 0 \\ 0&0& \rho_u 
 \end{pmatrix}  \quad 
 \epsilon_t \equiv \begin{pmatrix}\varepsilon_t^{r} \\ \varepsilon_t^{i}  \\ \varepsilon_t^{u} 
 \end{pmatrix}  \quad  \text{and } \quad \eta  \equiv \begin{pmatrix} \sigma_r & 0 & 0 \\ 0& \sigma_i & 0 \\ 0&0& \sigma_u 
 \end{pmatrix} 
 \end{align}

Note that this is the formulation for the case where a Taylor rule is in effect and is known by the private sector. It is straightforward to remove any of these two assumptions.



\section{The observation matrix for learning}\label{app_FG}
Instead of the rational expectations observation equation
\begin{equation}
z_t = g s_t \label{RE_obs}
\end{equation}

\begin{equation}
g_{t-1}^l = \begin{bmatrix} F_{t-1} & G_{t-1} \end{bmatrix}
\end{equation}
with
\begin{align}
F_{t-1} & = \bigg(A_a \frac{1}{1-\alpha\beta} + A_b\frac{1}{1-\beta} \bigg)a_{t-1}\\
G_{t-1} & = A_a b_{t-1}\bigg(I_3 - \alpha\beta h \bigg)^{-1} + A_b b_{t-1}\bigg(I_3 - \beta h \bigg)^{-1} + A_s
\end{align}

\section{Alternative specifications for the anchoring function}\label{alternative_criteria}
The baseline specification of the anchoring function in the main text is given by Equation (\ref{smooth}), reproduced here for convenience:
\begin{equation}
k^{-1}_t  = \rho_k k^{-1}_{t-1} + \gamma_k fe_{t-1}^2 
\end{equation}

The only other paper to consider an endogenous gain as a model for anchored expectations is \cite{carvalho2019anchored}. In their model, the anchoring function is a discrete choice function as follows. Let $\theta_t$ be a criterion to be defined. Then, for a threshold value $\tilde{\theta}$, the gain evolves according to
\begin{align*}
k_t & = \begin{cases} k_{t-1}+1 \quad \text{if} \quad \theta_t < \tilde{\theta}  \\ \bar{g}^{-1}  \quad \text{otherwise.}\numberthis \label{anchoring_kinked}
\end{cases} 
\end{align*}
In other words, agents choose a decreasing gain when the criterion $\theta_t$ is lower than the threshold $\tilde{\theta}$; otherwise they choose a constant gain. The criterion employed by \cite{carvalho2019anchored} is computed as the absolute difference between subjective and model-consistent expectations, scaled by the variance of shocks:
\begin{equation}
\theta_t = \max | \Sigma^{-1} ( \phi_{t-1} - \begin{bmatrix} F_{t-1} & G_{t-1} \end{bmatrix}) |
\end{equation}
where $\Sigma$ is the VC matrix of shocks, $\phi$ is the estimated matrix, $[F,G]$ is the ALM (see App. \ref{app_FG}).

As a robustness check, \cite{carvalho2019anchored} also compute an alternative criterion.\footnote{Note that for both criteria, I present the matrix generalizations of the scalar versions considered by \cite{carvalho2019anchored}.} Let $\omega_t$ denote agents' time $t$ estimate of the forecast error variance and $\theta_t$ be a statistic evaluated by agents in every period as
\begin{align}
\omega_t & =  \omega_{t-1} + \tilde{\kappa} k_{t-1}^{-1}(fe_{t|t-1} fe_{t|t-1}'  -\omega_{t-1})\\
\theta_t & =  \theta_{t-1} + \tilde{\kappa} k_{t-1}^{-1}(fe_{t|t-1}'\omega_t^{-1}fe_{t|t-1} -\theta_{t-1}) \label{cusum_crit}
\end{align}
where $\tilde{\kappa}$ is a parameter that allows agents to scale the gain compared to the previous estimation and $fe_{t|t-1}$ is the most recent forecast error, realized at time $t$. Indeed, this is a multivariate time series version of the squared CUSUM test.\footnote{See \cite{brown1975techniques} and \cite{lutkepohl2013introduction} for details.}

It is worthwhile to compare the two criteria. On the one hand, the first specification requires the private sector to evaluate model-consistent expectations, which runs counter to the maintained informational assumptions. It is more consistent with the present model, then, to assume that firms and households employ a statistical test of structural change. Therefore the CUSUM-based criterion is more appealing on conceptual grounds.

On the other hand, simulation of the model using \cite{carvalho2019anchored}'s preferred criterion reveals that it leads to the opposite comparative statics of anchoring with respect to monetary policy aggressiveness. In particular, while policy that is more aggressive on inflation (a higher $\psi_{\pi}$ in the Taylor rule) leads to more anchoring in a model with the CUSUM-inspired criterion, if one uses \cite{carvalho2019anchored}'s criterion, the same comparative static involves \emph{less} anchoring. This comes from the fact that \cite{carvalho2019anchored}'s criterion endows the public sector with capabilities to disentangle volatility due to the learning mechanism from that owing to exogenous disturbances. Thus agents in the \cite{carvalho2019anchored} model are able to make more advanced inferences about the performance of their forecasting rule and understand that a higher $\psi_{\pi}$ causes more learning-induced volatility. This is however not possible for agents who process data in real time without knowledge of the model. Therefore the CUSUM-inspired criterion is preferable both on conceptual and quantitative grounds.

\section{The policy problem in the simplified baseline model }\label{app_midsimple_problem}
Denote by $\mathbf{g}_{i,t} \in (0,1), \; i=\pi, \bar{\pi}$, the potentially time-varying derivatives of the anchoring function $\mathbf{g}_{t}$. In this simplified setting, $\bar{\pi}_t = e_1 a_t$, the estimated constant for the inflation process. $e_i$ is a selector vector, selecting row $i$ of the subsequent matrix. I also use the notation $b_i \equiv e_i b$.   The planner chooses $\{\pi_t, x_t, f_{a,t},  f_{b,t}, \bar{\pi}_t, k_t^{-1}\}_{t=t_0}^{\infty}$ to minimize

 \begin{align}
\mathcal{L} &= \E_{t_0}\sum_{t=t_0}^{\infty} \beta^{t-t_0}\bigg\{  (\pi_t^2  + \lambda x_t^2 )  \\
 & + \varphi_{1,t} \bigg(\pi_t - \kappa x_t -(1-\alpha)\beta f_a(t) -\kappa\alpha\beta b_2 (I_3 - \alpha\beta h_x)^{-1}s_t - e_3(I_3 - \alpha\beta h_x)^{-1}s_t \bigg) \label{midsimple_first}\\
 & + \varphi_{2,t} \bigg(x_t + \sigma i_t -\sigma f_b(t)  -  (1-\beta)b_2 (I_3 - \beta h_x)^{-1}s_t + \sigma\beta b_3 (I_3 - \beta h_x)^{-1}s_t -\sigma e_1(I_3 - \beta h_x)^{-1}s_t  \big)\bigg) \\
 & +  \varphi_{3,t}  \bigg(f_a(t) - \frac{1}{1-\alpha\beta}\bar{\pi}_{t-1}  - b_1(I_3 - \alpha\beta h_x)^{-1}s_t  \bigg) \\
 & + \varphi_{4,t}  \bigg(f_b(t) - \frac{1}{1-\beta}\bar{\pi}_{t-1}  - b_1(I_3 - \beta h_x)^{-1}s_t \bigg)  \\
  & + \varphi_{5,t}  \bigg(  \bar{\pi}_{t} - \bar{\pi}_{t-1} - k_t^{-1}\big(\pi_{t} -(\bar{\pi}_{t-1}+b_1 s_{t-1}) \big)   \bigg)  \\
  & + \varphi_{6,t}  \bigg(k_t^{-1} - \mathbf{g}(\pi_t - \bar{\pi}_{t-1}-b_1 s_{t-1})  \bigg)
  \bigg\} \label{midsimple_last}
\end{align}
After a little bit of simplifying, the first-order conditions boil down to the following three equations:
\begin{align}
& 2\pi_t + 2\frac{\lambda}{\kappa}x_t -\varphi_{5,t} k_t^{-1} - \varphi_{6,t} \mathbf{g}_{\pi,t} = 0 \label{gaspar22}\\
& -\frac{2(1-\alpha)\beta}{1-\alpha\beta}\frac{\lambda}{\kappa}x_{t+1} + \varphi_{5,t} -(1-k_t^{-1})\varphi_{5,t+1} +\mathbf{g}_{\bar{\pi},t}\varphi_{6,t+1} = 0 \label{gaspar21}\\
& \varphi_{6,t} = (\pi_t - \bar{\pi}_{t-1}-b_1 s_{t-1}) \varphi_{5,t} \label{constraints}
\end{align}
Note that Equation (\ref{gaspar22}) is the analogue of \cite{gaspar2010inflation}'s Equation (22) (or, equivalently, of  \cite{molnar2014optimal}'s (16)), except that there's an additional multiplier, $\varphi_6$. This multiplier reflects the fact that in addition to the constraint coming from the expectation process itself, with shadow value $\varphi_5$, learning involves the gain equation as a constraint as well. One can also clearly read off Result 2: when the learning process has converged such that neither expectations nor the gain process are constraints ($\varphi_5 =\varphi_6 = 0$), the discretionary inflation-output gap tradeoff familiar from \cite{clarida1999science} obtains. Combining the above three equations and solving for $\varphi_{5,t}$, using the notation that $\prod_{j=1}^{0} = 1$, one obtains the target criterion (\ref{target}).

The system of first order conditions (\ref{simpleFOC1})-(\ref{simpleFOC2}) and model equations for this simplified system also reveal how the endogenous gain introduces nonlinearity to the equation system. In particular, notice how in equations (\ref{simpleFOC1})-(\ref{simpleFOC2}) the gain $k_t^{-1}$ shows up multiplicatively with the Lagrange multiplier, $\varphi_{2,t}$. In fact, the origin of the problem is the recursive least squares learning equation
\begin{equation}
f_t = f_{t-1} + k_t^{-1}(\pi_t - f_{t-1}) \label{simpleRLS}
\end{equation}
where the first interaction terms between the gain and other endogenous variables show up. This results in an equation system of nonlinear difference equations that does not admit an analytical solution. 

Considering equation (\ref{simpleRLS}) is instructive to see how it is indeed the endogeneity of the gain that causes these troubles. Were we to specify a constant gain setup, $k_t^{-1}$ would merely equal the constant $\bar{g}$ and the anchoring function $\mathbf{g}$ would trivially reduce to $\bar{g}$ as well. In such a case, all interaction terms would reduce to multiplication between endogenous variables and parameters; linearity would be restored and a solution for the optimal time paths of endogenous variables would be obtainable. Similarly, a decreasing gain specification would also be manageable since for all $t$, the gain would simply be given by $t^{-1}$, and the anchoring function would also be deterministic and exogenous. 

Although a full optimal time path for the endogenous variables is thus not available for the anchoring model, it is still possible to characterize optimal monetary policy in terms of a target criterion. That is, one can express a relationship between inflation and output gaps from the first order conditions that characterizes the optimal plan that the monetary authority can rely on to implement and to communicate its policy.\footnote{See \cite{woodford2011interest} for a discussion of the desirability of target criteria for robustly optimal policy as well as ease of communication with the public.}

\section{A target criterion for an anchoring mechanism specified in terms of gain changes}\label{app_generalTC}
Consider the general anchoring mechanism of equation (\ref{anchoring}):
\begin{equation}
k_t = k_{t-1} + \mathbf{g}(fe_{t|t-1})
\end{equation}
With this assumption, the FOCs of the Ramsey problem are
\begin{align}
& 2\pi_t + 2\frac{\lambda}{\kappa}x_t -k_t^{-1} \varphi_{5,t} - \mathbf{g}_{\pi,t}\varphi_{6,t}  = 0 \label{gaspar22_general}\\
& c x_{t+1} + \varphi_{5,t} -(1-k_t^{-1})\varphi_{5,t+1} +\mathbf{g}_{\bar{\pi},t}\varphi_{6,t+1} = 0 \label{gaspar21_general}\\
& \varphi_{6,t} \; \textcolor{red}{+\; \varphi_{6,t+1}} = fe_{t|t-1} \varphi_{5,t} \label{constraints_general}
\end{align}
where the red multiplier is the new element vis-\`a-vis the case where the anchoring function is specified in levels ($k_t^{-1} = \mathbf{g}(fe_{t|t-1})$), and I'm using the shorthand notation
\begin{align}
c & = -\frac{2(1-\alpha)\beta}{1-\alpha\beta}\frac{\lambda}{\kappa} \\ 
fe_{t|t-1} & = \pi_t - \bar{\pi}_{t-1}-b_1 s_{t-1}
\end{align}
(\ref{gaspar22}) says that in anchoring, the discretion tradeoff is complemented with tradeoffs coming from learning ($\varphi_{5,t}$), which are more binding when expectations are unanchored ($k_{t}^{-1}$ high). Moreover, the change in the anchoring of expectations imposes an additional constraint ($\varphi_{6,t}$), which is more strongly binding if the gain responds strongly to inflation ($\mathbf{g}_{\pi,t}$).
One can simplify this three-equation-system to:
\begin{align}
\varphi_{6,t} & = -c fe_{t|t-1} x_{t+1} + \bigg(1+ \frac{fe_{t|t-1}}{fe_{t+1|t}}(1-k_{t+1}^{-1}) -fe_{t|t-1} \mathbf{g}_{\bar{\pi},t} \bigg) \varphi_{6,t+1} -\frac{fe_{t|t-1}}{fe_{t+1|t}}(1-k_{t+1}^{-1})\varphi_{6,t+2}\label{6'} \\
0 & = 2\pi_t + 2\frac{\lambda}{\kappa}x_t   - \bigg( \frac{k_t^{-1}}{fe_{t|t-1}} + \mathbf{g}_{\pi,t}\bigg)\varphi_{6,t} + \frac{k_t^{-1}}{fe_{t|t-1}}\varphi_{6,t+1}\label{1'}
\end{align}
Thus a central bank that follows the target criterion has to compute $\varphi_{6,t}$ as the solution to (\ref{1'}), and then evaluate (\ref{6'}) as a target criterion. The solution to (\ref{1'}) is given by:
\begin{equation}
\varphi_{6,t} = -2\E_t\sum_{i=0}^{\infty}(\pi_{t+i}+\frac{\lambda_x}{\kappa}x_{t+i})\prod_{j=0}^{i-1}\frac{\frac{k_{t+j}^{-1}}{fe_{t+j|t}}}{\frac{k_{t+j}^{-1}}{fe_{t+j|t}} + \mathbf{g}_{\pi,t+j}} \label{sol1'}
\end{equation}
Interpretation: the anchoring constraint is not binding ($\varphi_{6,t}=0$) if the central bank always hits the target (
$\pi_{t+i}+\frac{\lambda_x}{\kappa}x_{t+i} = 0, \; \forall i$); or expectations are always anchored ($k_{t+j}^{-1}=0, \; \forall j$). 

\section{Parameterized expectations algorithm (PEA)} \label{pea}
The objective of the parameterized expectations algorithm is to solve for the sequence of interest rates that solves the model equations including the target criterion, representing the first-order condition of the Ramsey problem. For convenience, I list the model equations:
\begin{align*}
x_t &=  -\sigma i_t + \begin{bmatrix} \sigma & 1-\beta & -\sigma\beta \end{bmatrix} f_{b,t} + \sigma \begin{bmatrix} 1 & 0 & 0 \end{bmatrix} (\mathbb{I}_{nx} - \beta h_x)^{-1} s_t \label{A9} \numberthis \\
\pi_t &= \kappa x_t  + \begin{bmatrix} (1-\alpha)\beta & \kappa\alpha\beta & 0 \end{bmatrix}  f_{a,t} + \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}  (\mathbb{I}_{nx} - \alpha \beta h_x)^{-1}  s_t \label{A10} \numberthis\\
f_{a,t} & = \frac{1}{1-\alpha\beta}\bar{\pi}_{t-1}  + b(\mathbb{I}_{nx} - \alpha\beta h)^{-1}s_t \numberthis \\
f_{b,t} & = \frac{1}{1-\beta}\bar{\pi}_{t-1}  + b(\mathbb{I}_{nx} - \beta h)^{-1}s_t  \label{A8} \numberthis \\
 fe_{t|t-1} &  = \pi_t - (\bar{\pi}_{t-1}+b_1 s_{t-1}) \label{A7} \numberthis \\
 k^{-1}_t & = \gamma_k fe_{t|t-1}^2 \numberthis  \\
 \bar{\pi}_{t} &  = \bar{\pi}_{t-1} +k_t^{-1}fe_{t|t-1} \numberthis \\
 \pi_t & = -\frac{\lambda_x}{\kappa}\bigg\{x_t - \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+fe_{t|t-1}\mathbf{g}_{\pi,t} \bigg) \bigg(\E_t\sum_{i=1}^{\infty}x_{t+i}\prod_{j=0}^{i-1}(1-k_{t+1+j}^{-1} - (fe_{t+1+j|t+j})\mathbf{g_{\bar{\pi}, t+j}}) \bigg)
\bigg\} \numberthis \label{B1}
\end{align*}

Denote the expectation on the right hand side of (\ref{B1}) as $E_t$. The idea of the PEA is to approximate this expectation and to solve model equations given the approximation $\hat{E}$. The algorithm is as follows:\footnote{For a thorough treatment of the PEA the interested reader is referred to \cite{christiano2000occasionally}.}
\begin{enumerate}
\item[Objective:] Obtain the sequence $\{i_t\}_{t=1}^T$ that solves Equations (\ref{A9}) - (\ref{B1}) for a a history of exogenous shocks $\{s_t\}_{t=1}^T$ of length $T$. 
\item Conjecture an initial expectation $\hat{E}_t=\beta^0 s(X_t)$ \\
The expectation is approximated as a projection on a basis, $s(X_t)$, where $\beta^0$ are initial projection coefficients, and $X_t = (k_t,\bar{\pi}_{t-1}, r^n_t, u_t)$ is the state vector. I use a monomial basis consisting of the first, second and third powers of $X_t$.
\item Solve model equations given conjectured $\hat{E}_t$ for a given sequence of shocks $\{s_t\}_{t=1}^T$\\
Compute residuals to the model equations (\ref{A9}) - (\ref{B1}) given $\{s_t\}_{t=1}^T$ and $\{\hat{E}_t\}_{t=1}^T$. Obtain a sequence $\{i_t\}_{t=1}^T$ that sets the residuals to zero. The output of this step is $\{v_t\}_{t=1}^T$, the simulated history of endogenous variables (\cite{christiano2000occasionally} refer to this as ``synthetic time series"). 
\item Compute realized analogues to $\{E_t\}_{t=1}^T$ given $\{v_t\}_{t=1}^T$
\item Update $\beta$ regressing the synthetic $E_t$ on $s(X_t)$.\\
The coefficient update is $\beta^{i+1} = (s(X_t)'s(X_t))^{-1}s(X_t)'E_t$. Then iterate until convergence by evaluating at every step $||\beta^i-\beta^{i+1}||$.
\end{enumerate}

\section{Parameterized value function iteration} \label{vfi}
This is an alternative approach I implement as a robustness check to the PEA. The objective is thus the same: to obtain the interest rate sequence that solves the model equations. The general value iteration approach is fairly standard, for which reason I refer to the \cite{judd1998numerical} textbook for details. Specific to my application is that the state vector is six-dimensional, $X_t = (k_t,\bar{\pi}_{t-1}, r^n_t, u_t, r^n_{t-1}, u_{t-1})$, and I approximate the value function using a cubic spline. Thus the output of the algorithm is a cubic spline approximation of the value function and a policy function for each node on the grid of states. Next, I interpolate the policy function using a cubic spline. As a last step I pass the state vector from the PEA simulation, obtaining an interest rate sequence conditional on the history of states. Fig. \ref{compare_pea_vfi} shows the resulting interest rate sequence, obtained through the two approaches, conditional on two different simulated sequences for the states.

\begin{figure}[h!]
\subfigure[$X_1$]{\includegraphics[scale=0.2]{\myFigPath materials32_policy}}
\subfigure[$X_2$]{\includegraphics[scale=0.2]{\myFigPath compare_value_pea_results_value_outputs_server32_accelerated_pea_outputs_30_May_2020_10_18_28}}
\caption{Policy function for two particular histories of states, $X$, solved using parameterized expectations (PEA) and parametric value function iteration (VFI)} 
\label{compare_pea_vfi}
\end{figure}

\section{Oscillatory dynamics in adaptive learning models} \label{app_oscillations}
Here I present an illustration for why adaptive learning models produce oscillatory impulse responses if the gain is high enough. Consider a stylized adaptive learning model in two equations:
\begin{align}
\pi_t & = \beta f_t + u_t \label{simple_NKPC} \\
f_t & = f_{t-1} + k^{-1}(\pi_t - f_{t-1}) \label{simple_expectations}
\end{align}
The reader can recognize in (\ref{simple_NKPC}) a simplified Phillips curve in which I am abstracting from output gaps to keep the presentation as clear as possible. Like in the simple model of Section \ref{ramsey} in the main text, $f_t$ represents the one-period inflation expectation $\hat{\E}_t \pi_{t+1}$. (\ref{simple_expectations}), then, represents the simplest possible recursive updating of the expectations $f_t$. My notation of the gain as $k^{-1}$ indicates a constant gain specification, but the intuition remains unchanged for decreasing (or endogenous) gains. 

Combining the two equations allows one to solve for the time series of expectations
\begin{equation}
f_t = \frac{1-k^{-1}}{1-k^{-1}\beta}f_{t-1} + \frac{k^{-1}}{1-k^{-1}\beta}u_t
\end{equation}
which, for $\beta$ close but smaller than 1, is a near-unit-root process. (In fact, if the gain were to go to zero, this would be a unit root process.) Defining the forecast error as $fe_{t|t-1} \equiv \pi_t - f_{t-1}$, one obtains
\begin{equation}
fe_{t|t-1} = -\frac{1-\beta}{1-k^{-1}\beta}f_{t-1} + \frac{1}{1-k^{-1}\beta}u_t \label{oscillating_fe}
\end{equation}
Equation (\ref{oscillating_fe}) shows that in this simple model, the forecast error loads on a near-unit-root process with a coefficient that is negative and less than one in absolute value. Damped oscillations are the result. 

Note that even if the gain would converge to zero, the coefficient on $f_{t-1}$ would be negative and less than one in absolute value. Thus even for decreasing gain learning, one obtains oscillations, but the lower the gain, the more damped the oscillations become. This corroborates my findings in the impulse responses of Fig. \ref{IRF}. But importantly, the opposite extreme, when $k^{-1}\rightarrow 1$, results in a coefficient of exactly $-1$, giving perpetual oscillations. This clearly illustrates how the oscillatory behavior of impulse responses comes from the oscillations in the forecast error that obtain when the gain is sufficiently large. 


\end{document}





