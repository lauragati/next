\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb,lscape, natbib}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{setspace}
\usepackage[final]{pdfpages}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm, bottom=2.5cm]{geometry}
\usepackage{natbib} 
\usepackage{bibentry} 
\newcommand{\bibverse}[1]{\begin{verse} \bibentry{#1} \end{verse}}
\newcommand{\vs}{\vspace{.3in}}
\renewcommand{\ni}{\noindent}
\usepackage{xr-hyper}
\usepackage[]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}
\usepackage[capposition=top]{floatrow}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
\usepackage{tikz}
 
\pagestyle{fancy} % customize header and footer
\fancyhf{} % clear initial header and footer
%\rhead{Overleaf}
\lhead{\centering \rightmark} % this adds subsection number and name
\lfoot{\centering \rightmark} 
\rfoot{\thepage} % put page number (the centering command puts it in the middle, don't matter if you put it in right or left footer)

\def \myFigPath {../figures/} 
% BE CAREFUL WITH FIGNAMES, IN LATEX THEY'RE NOT CASE SENSITIVE!!
\def \myTablePath {../tables/} 

%\definecolor{mygreen}{RGB}{0, 100, 0}
\definecolor{mygreen}{RGB}{0, 128, 0}

\definecolor{citec}{rgb}{0,0,.5}
\definecolor{linkc}{rgb}{0,0,.6}
\definecolor{bcolor}{rgb}{1,1,1}
\hypersetup{
%hidelinks = true
  colorlinks = true,
  urlcolor=linkc,
  linkcolor=linkc,
  citecolor = citec,
  filecolor = linkc,
  pdfauthor={Laura G\'ati},
}


\geometry{left=.83in,right=.89in,top=1in,
bottom=1in}
\linespread{1.5}
\renewcommand{\[}{\begin{equation}}
\renewcommand{\]}{\end{equation}}

% New Options
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

%\newtheorem{theorem}{Theorem}[section] % the third argument specifies that their number will be adopted to the section
%\newtheorem{corollary}{Corollary}[theorem]
%\newtheorem{lemma}[theorem]{Lemma}
%\declaretheorem{proposition}
%\linespread{1.3}
%\raggedbottom
%\font\reali=msbm10 at 12pt

% New Commands
\newcommand{\real}{\hbox{\reali R}}
\newcommand{\realp}{\hbox{\reali R}_{\scriptscriptstyle +}}
\newcommand{\realpp}{\hbox{\reali R}_{\scriptscriptstyle ++}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\argmin}{arg\,min}
\newcommand\w{3.0in}
\newcommand\wnum{3.0}
\def\myFigWidth{5.3in}
\def\mySmallerFigWidth{2.1in}
\def\myEvenBiggerFigScale{0.8}
\def\myPointSixFigScale{0.6}
\def\myBiggerFigScale{0.4}
\def\myFigScale{0.3}
\def\myMediumFigScale{0.25}
\def\mySmallFigScale{0.22}
\def\mySmallerFigScale{0.18}
\def\myTinyFigScale{0.16}
\def\myPointFourteenFigScale{0.14}
\def\myTinierFigScale{0.12}
\def\myAdjustableFigScale{0.14}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % this defines a command to make align only number this line
\newcommand{\code}[1]{\texttt{#1}} %code %

\renewcommand*\contentsname{Overview}
\setcounter{tocdepth}{2}

% define a command to make a huge question mark (it works in math mode)
\newcommand{\bigqm}[1][1]{\text{\larger[#1]{\textbf{?}}}}

\begin{document}

\linespread{1.0}

\title{Materials 18 - Optimal monpol under learning is an infinitely repeated prisoner's dilemma without the grim trigger}
\author{Laura G\'ati} 
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%             DOCUMENT           %%%%%%%%%%%%%%%%%% 

%\tableofcontents

%\listoffigures


Take a very simple optimal policy problem where the planner chooses $\{\pi_t\}_{t=t_0}^{\infty}$ to minimize
\begin{align}
\mathcal{L} &= \E_{t_0}\sum_{t=t_0}^{\infty} \beta^{t-t_0}\bigg\{ \pi_t^2  + \varphi_{t} (\pi_t - \beta \pi_{t+1} ) \bigg\} \label{commitment}
 \end{align}
 Consider this same problem under three cases:
 \begin{enumerate}
 \item \textbf{RE and commitment} \\
 FOC:
 \begin{equation}
 2\pi_t +\varphi_t - \varphi_{t-1} = 0 \label{FOCcommitment}
 \end{equation}

 \item \textbf{RE and discretion} \\
 Write expected inflation as a variable $f_t$ that the authority takes as given:
\begin{align}
\mathcal{L} &= \E_{t_0}\sum_{t=t_0}^{\infty} \beta^{t-t_0}\bigg\{ \pi_t^2  + \varphi_{t} (\pi_t - \beta f_t) \bigg\} \label{discretion}
 \end{align}
FOC:
 \begin{equation}
 2\pi_t +\varphi_t = 0 \label{FOCdiscretion}
 \end{equation}
 \item \textbf{Learning and commitment} 
 \begin{align}
\mathcal{L} &= \E_{t_0}\sum_{t=t_0}^{\infty} \beta^{t-t_0}\bigg\{ \pi_t^2  + \varphi_{1,t} (\pi_t - \beta f_t) + \varphi_{2,t}(f_t - f_{t-1} -k^{-1}(\pi_t - f_{t-1})) \bigg\} \label{learning}
 \end{align}
 FOCs:
 \begin{align}
  2\pi_t +\varphi_{1,t} -\varphi_{2,t}k^{-1} & = 0 \label{FOC1learn} \\
  -\beta\varphi_{1,t} + \varphi_{2,t} + \E_{t}\varphi_{2,t+1}(-1 + k^{-1})  & = 0 \label{FOC2learn} 
 \end{align}
$\rightarrow$ no lagged multiplier despite taking formation of expectations into account!
 \end{enumerate}
 
 Mele, Moln\'ar \& Santoro (2019): optimal monetary policy w/ learning does not involve commitment! %Indeed it cannot because the learning mechanism takes away the CB's commitment technology. (In their lingo, learning agents don't have ``off-equilibrium strategies.'') This is a Stackelberg infinitely repeated game where b/c of learning, the private sector is not strategic: it looses access to the grim trigger threat strategy. Therefore the CB will always play its best response and we land in the suboptimal Nash in the long-run!
 
 In a sense, this was anticipated in the ``Ramsey policy is indeterminate under RE'' literature:
 \begin{itemize}
 \item Ramsey policy is time-inconsistent (Kydland \& Prescott, 1977, Barro \& Gordon, 1983). So invent RE to endow agents with threats for deviating $\rightarrow$ a commitment device.
 \item Now the problem is that the commitment solution is a Nash, but it's indeterminate (b/c purely forward-looking).
 \item Invent learning to introduce backward-lookingness: allows you to select the RE with commitment solution (focus of Evans \& Honkapohja 2001 on E-stability).
 \item Problem: that solution is no longer optimal b/c the PS cannot enforce it! (``machine'')
 \item Circumvent that by introducing backward-looking expectation formation that satisfies some sense of optimality (Cho and Matsui, 1995, ``inductive expectations'') so that the PS regains some of its strategic nature.
 \end{itemize}
 
 Here's a step of bold interpretation:
 \begin{itemize}
 \item Maybe Ramsey under RE is indeterminate because of the folk theorem: any feasible and individually rational payoff profile can be a Nash or a subgame perfect equilibrium of the infinitely repeated prisoner's dilemma.
 \item Learning breaks the folk theorem b/c the assumption of credible threats is gone when one of the players is an automaton and in particular, not an optimal one. 
 \end{itemize}

\newpage
\section{Optimal policy problem under learning w/ a smooth gain function}
%Postulate a gain function like $k_t =d\theta_t + c$. Note that since we're learning $\pi$ only here $f_t$ and $\omega_t$ are scalars. Moreover, to make matters simple, I shut off all shocks except for the cost-push shock.
% \begin{align}
%\mathcal{L} &= \E_{t_0}\sum_{t=t_0}^{\infty} \beta^{t-t_0}\bigg\{  (\pi_t^2  + \lambda x_t^2 )  \\
% & + \varphi_{1,t} \bigg(\pi_t - \kappa x_t -(1-\alpha)\beta f_a(t) -\kappa\alpha\beta b_2 (I_3 - \alpha\beta h_x)^{-1}s_t - e_2(I_3 - \alpha\beta h_x)^{-1}s_t \bigg) \\
% & + \varphi_{2,t} \bigg(x_t + \sigma i_t -\sigma f_b(t)  -  (1-\beta)b_2 (I_3 - \beta h_x)^{-1}s_t + \sigma\beta b_2 (I_3 - \beta h_x)^{-1}s_t -\sigma e_1(I_3 - \alpha\beta h_x)^{-1}s_t  \big)\bigg) \\
% & +  \varphi_{3,t}  \bigg(f_a(t) - \frac{1}{1-\alpha\beta}\bar{\pi}_{t-1}  - b_1(I_3 - \alpha\beta h_x)^{-1}s_t  \bigg) \\
% & + \varphi_{4,t}  \bigg(f_b(t) - \frac{1}{1-\beta}\bar{\pi}_{t-1}  - b_1(I_3 - \beta h_x)^{-1}s_t \bigg)  \\
%  & + \varphi_{5,t}  \bigg(  \bar{\pi}_{t} - \bar{\pi}_{t-1} - k_t^{-1}\big(\pi_{t} -(\bar{\pi}_{t-1}+bs_{t-1}) \big)   \bigg)  \\
%  & + \varphi_{6,t}  \bigg(k_t - d\theta_t - c  \bigg)\\
%  & + \varphi_{7,t}  \bigg(\theta_t -  \theta_{t-1} - \tilde{\kappa} k_{t-1}^{-1}(f_t^2/\omega_t -\theta_{t-1})  \bigg)\\
%  & + \varphi_{8,t}  \bigg(\omega_t -  \omega_{t-1} -\tilde{\kappa} k_{t-1}^{-1}(f_t^2 -\omega_{t-1})  \bigg)\\
%  & + \varphi_{9,t}  \bigg(f_t - \pi_t + (\bar{\pi}_{t-1} + b_1s_{t-1})  \bigg)
%  \bigg\}
%\end{align}
%
%Yet this is actually quite unwieldy. 


A simpler version of the problem is: postulate an endogenous gain choice as some differentiable function of the forecast error: $k_t^{-1} = f(\pi_t - \bar{\pi}_{t-1}-b s_{t-1})$. I have the CUSUM-mechanism in mind, but am agnostic about it to simplify the math. (Treat $k_t^{-1}$ as a variable.)
 \begin{align}
\mathcal{L} &= \E_{t_0}\sum_{t=t_0}^{\infty} \beta^{t-t_0}\bigg\{  (\pi_t^2  + \lambda x_t^2 )  \\
 & + \varphi_{1,t} \bigg(\pi_t - \kappa x_t -(1-\alpha)\beta f_a(t) -\kappa\alpha\beta b_2 (I_3 - \alpha\beta h_x)^{-1}s_t - e_2(I_3 - \alpha\beta h_x)^{-1}s_t \bigg) \\
 & + \varphi_{2,t} \bigg(x_t + \sigma i_t -\sigma f_b(t)  -  (1-\beta)b_2 (I_3 - \beta h_x)^{-1}s_t + \sigma\beta b_2 (I_3 - \beta h_x)^{-1}s_t -\sigma e_1(I_3 - \alpha\beta h_x)^{-1}s_t  \big)\bigg) \\
 & +  \varphi_{3,t}  \bigg(f_a(t) - \frac{1}{1-\alpha\beta}\bar{\pi}_{t-1}  - b_1(I_3 - \alpha\beta h_x)^{-1}s_t  \bigg) \\
 & + \varphi_{4,t}  \bigg(f_b(t) - \frac{1}{1-\beta}\bar{\pi}_{t-1}  - b_1(I_3 - \beta h_x)^{-1}s_t \bigg)  \\
  & + \varphi_{5,t}  \bigg(  \bar{\pi}_{t} - \bar{\pi}_{t-1} - k_t^{-1}\big(\pi_{t} -(\bar{\pi}_{t-1}+bs_{t-1}) \big)   \bigg)  \\
  & + \varphi_{6,t}  \bigg(k_t^{-1} - f(\pi_t - \bar{\pi}_{t-1}-b s_{t-1})  \bigg)
  \bigg\}
\end{align}
where I denote by $f_i(t) \in (0,1), \; i=\pi, \bar{\pi}$, the potentially time-varying derivatives of the anchoring function $f(t)$. After a little bit of simplifying, the FOCs boil down to the following three equations:
\begin{align}
& 2\pi_t + 2\frac{\lambda}{\kappa}x_t -\varphi_{5,t} k_t^{-1} - \varphi_{6,t} f_{\pi}(t) = 0 \label{gaspar22}\\
& -\frac{2(1-\alpha)\beta}{1-\alpha\beta}\frac{\lambda}{\kappa}x_{t+1} + \varphi_{5,t} -(1-k_t^{-1})\varphi_{5,t+1} +f_{\bar{\pi}}(t)\varphi_{6,t+1} = 0 \label{gaspar21}\\
& \varphi_{6,t} = (\pi_t - \bar{\pi}_{t-1}-b s_{t-1}) \varphi_{5,t} \label{constraints}
\end{align}
Equation (\ref{gaspar22}) is the analogue of Gaspar et al's Equation (22) (= Moln\'ar \& Santoro's (16)), except that there's an additional multiplier, $\varphi_6$, reflecting the fact that learning involves the gain equation as an additional constraint here. Note that when learning has converged $(\varphi_{5,t}=\varphi_{6,t}=0)$, this boils down to the discretionary RE case. \\
Equation (\ref{gaspar21}) is the analogue of Gaspar et al's Equation (21). The novelty in the system is (\ref{constraints}) which reflects the relationship between the constraints imposed on the system by learning. Combining the above three equations and solving for $\varphi_{5,t}$, using the notation that $\prod_{j=1}^{0} = 1$, I get what I will call the anchoring analogue of Gaspar et al's (24), the target criterion (ignoring $\E_t$):
\begin{align}
\pi_t  = -\frac{\lambda}{\kappa}\bigg\{x_t - \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+((\pi_t - \bar{\pi}_{t-1}-b s_{t-1}))f_{\pi}(t) \bigg) 
\bigg(\sum_{i=1}^{\infty}x_{t+i}\prod_{j=1}^{i-1}(1-k_{t+j}^{-1}(\pi_{t+1+j} - \bar{\pi}_{t+j}-b s_{t+j})) \bigg)
\bigg\} \label{target}
\end{align}

To get the interest rate rule, seems like I have to solve for the optimal plans. I just don't understand: if I have optimal plans and expectations, wouldn't the IS curve suffice to determine the interest rate path? What do we gain from the target criterion? 

Also, getting optimal plans involves solving the system of FOCs and model equations. Colloquially, it should take the form $z_t = P \Lambda z_0$. Ugh .. that looks tough. 


\end{document}





