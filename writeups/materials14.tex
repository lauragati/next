\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb,lscape, natbib}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{setspace}
\usepackage[final]{pdfpages}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm, bottom=2.5cm]{geometry}
\usepackage{natbib} 
\usepackage{bibentry} 
\newcommand{\bibverse}[1]{\begin{verse} \bibentry{#1} \end{verse}}
\newcommand{\vs}{\vspace{.3in}}
\renewcommand{\ni}{\noindent}
\usepackage{xr-hyper}
\usepackage[]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}
\usepackage[capposition=top]{floatrow}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
\usepackage{tikz}
 
\pagestyle{fancy} % customize header and footer
\fancyhf{} % clear initial header and footer
%\rhead{Overleaf}
\lhead{\centering \rightmark} % this adds subsection number and name
\lfoot{\centering \rightmark} 
\rfoot{\thepage} % put page number (the centering command puts it in the middle, don't matter if you put it in right or left footer)

\def \myFigPath {../figures/} 
% BE CAREFUL WITH FIGNAMES, IN LATEX THEY'RE NOT CASE SENSITIVE!!
\def \myTablePath {../tables/} 

%\definecolor{mygreen}{RGB}{0, 100, 0}
\definecolor{mygreen}{RGB}{0, 128, 0}

\definecolor{citec}{rgb}{0,0,.5}
\definecolor{linkc}{rgb}{0,0,.6}
\definecolor{bcolor}{rgb}{1,1,1}
\hypersetup{
%hidelinks = true
  colorlinks = true,
  urlcolor=linkc,
  linkcolor=linkc,
  citecolor = citec,
  filecolor = linkc,
  pdfauthor={Laura G\'ati},
}


\geometry{left=.83in,right=.89in,top=1in,
bottom=1in}
\linespread{1.5}
\renewcommand{\[}{\begin{equation}}
\renewcommand{\]}{\end{equation}}

% New Options
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

%\newtheorem{theorem}{Theorem}[section] % the third argument specifies that their number will be adopted to the section
%\newtheorem{corollary}{Corollary}[theorem]
%\newtheorem{lemma}[theorem]{Lemma}
%\declaretheorem{proposition}
%\linespread{1.3}
%\raggedbottom
%\font\reali=msbm10 at 12pt

% New Commands
\newcommand{\real}{\hbox{\reali R}}
\newcommand{\realp}{\hbox{\reali R}_{\scriptscriptstyle +}}
\newcommand{\realpp}{\hbox{\reali R}_{\scriptscriptstyle ++}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\argmin}{arg\,min}
\newcommand\w{3.0in}
\newcommand\wnum{3.0}
\def\myFigWidth{5.3in}
\def\mySmallerFigWidth{2.1in}
\def\myEvenBiggerFigScale{0.8}
\def\myPointSixFigScale{0.6}
\def\myBiggerFigScale{0.4}
\def\myFigScale{0.3}
\def\myMediumFigScale{0.25}
\def\mySmallFigScale{0.22}
\def\mySmallerFigScale{0.18}
\def\myTinyFigScale{0.16}
\def\myPointFourteenFigScale{0.14}
\def\myTinierFigScale{0.12}
\def\myAdjustableFigScale{0.14}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % this defines a command to make align only number this line
\newcommand{\code}[1]{\texttt{#1}} %code %

\renewcommand*\contentsname{Overview}
\setcounter{tocdepth}{2}

% define a command to make a huge question mark (it works in math mode)
\newcommand{\bigqm}[1][1]{\text{\larger[#1]{\textbf{?}}}}

\begin{document}

\linespread{1.0}

\title{Materials 14 - Maybe a last attempt to get rid of the overshooting}
\author{Laura G\'ati} 
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%             DOCUMENT           %%%%%%%%%%%%%%%%%% 

\tableofcontents

%\listoffigures

\newpage
\section{Model summary}
\begin{align}
x_t &=  -\sigma i_t +\hat{\E}_t \sum_{T=t}^{\infty} \beta^{T-t }\big( (1-\beta)x_{T+1} - \sigma(\beta i_{T+1} - \pi_{T+1}) +\sigma r_T^n \big)  \label{prestons18}  \\
\pi_t &= \kappa x_t +\hat{\E}_t \sum_{T=t}^{\infty} (\alpha\beta)^{T-t }\big( \kappa \alpha \beta x_{T+1} + (1-\alpha)\beta \pi_{T+1} + u_T\big) \label{prestons19}  \\
i_t &= \psi_{\pi}\pi_t + \psi_{x} x_t  + \bar{i}_t \label{TR}
\end{align}
\begin{equation}
\hat{\E}_t z_{t+h} =  \bar{z}_{t-1} + bh_x^{h-1}s_t  \quad \forall h\geq 1 \quad \quad b = g_x\; h_x \quad \quad \text{PLM} \label{PLM}
\end{equation}
\begin{equation}
\bar{z}_{t} = \bar{z}_{t-1} +k_t^{-1}\underbrace{\big(z_{t} -(\bar{z}_{t-1}+bs_{t-1}) \big)}_{\text{fcst error using (\ref{PLM})} } 
\end{equation}
(Vector learning. For scalar learning, $\bar{z}= \begin{pmatrix} \bar{\pi} & 0 & 0\end{pmatrix}' $. I'm also not writing the case where the slope $b$ is also learned.)
 \begin{align*}
k_t & = \begin{cases} k_{t-1}+1 \quad \text{for decreasing gain learning}  \\ \bar{g}^{-1}  \quad \text{for constant gain learning.}\numberthis
\end{cases} 
\end{align*}

\newpage
\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_true_baseline_no_info_ass_constant_only}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_true_baseline_no_info_ass_slope_and_constant}}
\caption{Reference: baseline model}
\end{figure}


\section{Regime-switching}
\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath Markov__initial_active_constant_only}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath Markov__initial_active_slope_and_constant}}
\caption{ Markov-switching Taylor rule, baseline, learning initialized at active state, conditional on mixed regime sequence}
\end{figure}

\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Markov_switchingTR_true_baseline_no_info_ass_constant_only}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Markov_switchingTR_true_baseline_no_info_ass_slope_and_constant}}
\caption{ Markov-switching Taylor rule, baseline, learning initialized at passive state, conditional on mixed regime sequence}
\end{figure}

\begin{itemize}
\item Different initialization of learning doesn't make a whole lot of difference.
\item It just changes where you start, but doesn't fundamentally affect dynamics.
\end{itemize}


\begin{figure}[h!] 
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Markov_switchingTR_true_baseline_no_info_ass_constant_only_passive}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Markov_switchingTR_true_baseline_no_info_ass_slope_and_constant_passive}}
\caption{ Markov-switching Taylor rule, baseline, learning initialized at passive state, conditional on passive regime only}
\end{figure}

\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Markov_switchingTR_true_baseline_no_info_ass_constant_only_active}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Markov_switchingTR_true_baseline_no_info_ass_slope_and_constant_active}}
\caption{ Markov-switching Taylor rule, baseline, learning initialized at passive state, conditional on active regime only}
\end{figure}

\begin{itemize}
\item I'm surprised that the all-passive state is unstable. I've checked and it's not E-stable: the difference in the learning matrix $\phi$ grows over time, even with decreasing gain learning.
\item The all-active is very volatile. 
\end{itemize}


\section{Projection facility: checking \texttt{eig(phi)} when $\phi$ isn't square?}
What I do now is I check \texttt{eig(R)} because that is always square, and when $\phi$ explodes, usually R does too.  Of course I can't do this for learning the constant only, but according to my experience, that's where the projection facility is least likely to ever be needed. Of course, this doesn't always work - for interest rate smoothing, it doesn't.

\section{Endogenous states don't evolve as they should}
Now they do!

\newpage
\section{Bringing anchoring back}
I've reworked the choice functions for the endogenous gain, CEMP's criterion and the CUSUM test, to better incorporate vector learning (``out of the difference between PLM and E(ALM) or out of forecast error variances respectively, pick the largest (or the mean) and test whether that's bigger than the threshold value''). 

CEMP's criterion was 
\begin{align}
\theta_t & = |\hat{\E}_{t-1}\pi_t - \E_{t-1}\pi_t | / (\text{Var(shocks)}) \\
\text{i.e.} \quad &\text{PLM- E[ALM], scaled by shocks}
\end{align}

To extend this to vector learning and in general to a model with a vector of observables I first rewrite the ALM
\begin{align}
z_t & = A_a f_a + A_b f_b + A_s s_t \\
\text{as} \quad & z_t =  F  +Gs_t  \\
\Leftrightarrow \quad & z_t = \begin{bmatrix} F & G \end{bmatrix} \begin{bmatrix} 1 \\ s_t\end{bmatrix}
\end{align}
Then, since the PLM is $z_t = \phi  \begin{bmatrix} 1 \\ s_t\end{bmatrix}$, the generalized CEMP criterion becomes
\begin{equation}
\theta_t = \max | \Sigma^{-1} ( \phi - \begin{bmatrix} F & G \end{bmatrix}) |
\end{equation}
where $\Sigma$ is the VC matrix of shocks.
As for the CUSUM criterion, what I did in Materials 5 was
\begin{align}
 \omega_t & =  \omega_{t-1} + \kappa k_{t-1}^{-1}(FE_t^2 -\omega_{t-1})\\
\theta_t & =  \theta_{t-1} + \kappa k_{t-1}^{-1}(FE_t^2/\omega_t -\theta_{t-1})\
\end{align}
where $FE_t$ is the most recent short-run forecast error ($ny\times 1$), and $\omega_t$ is the agents' estimate of the forecast error variance ($ny \times ny$). To take into account that these are now matrices, I now write
\begin{align}
\omega_t & =  \omega_{t-1} + \kappa k_{t-1}^{-1}(FE_t FE_t' -\omega_{t-1})\\
\theta_t & =  \theta_{t-1} + \kappa k_{t-1}^{-1}\text{mean}((\omega_t^{-1}FE_t FE_t' -\theta_{t-1}))
\end{align}

Both work but require quite different threshold parameters to behave similarly.

%\newpage
\section{The central bank's loss function}
Relying on Clarida, Gali and Gertler (1999), I start with the simple quadratic loss:
\begin{equation}
\mathcal{L} = \frac{1}{2} \E_t\sum_{T=t}^{\infty} \bigg( \alpha^{CB} x_T^2 + \pi_T^2\bigg)
\end{equation}

\begin{figure}[h!]
\includegraphics[scale=\mySmallerFigScale]{\myFigPath grid_search_loss_again_critCEMP_constant_only_2020_01_26}
\caption{Central bank loss for $\psi_x=0, \alpha^{CB} = 0.5$, computed as a cross-sectional average, $N=100, T=400$.}
\end{figure}

Fmincon says $\psi_{\pi}^* = 1.0724, \psi_x^* =    0.0490$.

\begin{figure}[h!]
\includegraphics[scale=\mySmallerFigScale]{\myFigPath grid_search_loss_again_critCUSUM_constant_only_2020_01_26}
\caption{Central bank loss for $\psi_x=0, \alpha^{CB} = 0.5$, computed as a cross-sectional average, $N=100, T=400$.}
\end{figure}

Fmincon says $\psi_{\pi}^* = 1.0646, \psi_x^* =    0.0451$.
     
An issue with CUSUM: $\omega$ often becomes singular, so for fmincon to be able to run, I took the average element value of $\omega$. But this seems to really matter: it never produces anchoring for anything.

Striking is that these optimal values do not produce anchored sequences. But of course this is super-sensitive to threshold criterion values which are completely arbitrary!


\newpage
\section{Behavior of CEMP and CUSUM criteria as functions of their parameters}

$\psi_{\pi}=1.5, \psi_x = 0$.

\begin{figure}[h!]
\subfigure[$\tilde{\theta} = 0.1, \tilde{\kappa} = 0.2$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCUSUM_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_1_thettilde_0_1_kap_0_2_alph_CB_0_2020_01_27}}
\subfigure[$\tilde{\theta} = 0.1, \tilde{\kappa} = 0.8$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCUSUM_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_1_thettilde_0_1_kap_0_8_alph_CB_0_2020_01_27}}
\subfigure[$\tilde{\theta} = 0.2, \tilde{\kappa} = 0.2$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCUSUM_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_1_thettilde_0_2_kap_0_2_alph_CB_0_2020_01_27}}
\subfigure[$\tilde{\theta} = 0.2, \tilde{\kappa} = 0.8$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCUSUM_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_1_thettilde_0_2_kap_0_8_alph_CB_0_2020_01_27}}
\subfigure[$\tilde{\theta} = 0.4, \tilde{\kappa} = 0.2$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCUSUM_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_1_thettilde_0_4_kap_0_2_alph_CB_0_2020_01_27}}
\subfigure[$\tilde{\theta} = 1, \tilde{\kappa} = 0.8$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCUSUM_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_1_thettilde_0_4_kap_0_8_alph_CB_0_2020_01_27}}
\caption{Inverse gains CUSUM, learning the constant only}
\end{figure}

A higher $\tilde{\kappa}$ just increases the action.

\begin{figure}[h!]
\subfigure[$\bar{\theta} = 1$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCEMP_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_1_thettilde_1_6_kap_0_8_alph_CB_0_2020_01_27}}
\subfigure[$\bar{\theta} = 3$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCEMP_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_3_thettilde_1_6_kap_0_8_alph_CB_0_2020_01_27}}
\subfigure[$\bar{\theta} = 4$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCEMP_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_4_thettilde_1_6_kap_0_8_alph_CB_0_2020_01_27}}
\subfigure[$\bar{\theta} = 5$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_anchoring_loss_again_critCEMP_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_5_thettilde_1_6_kap_0_8_alph_CB_0_2020_01_27}}
\caption{Inverse gains CEMP, learning the constant only}
\end{figure}

\clearpage
\newpage
\section{Anchoring as a function of $\psi_{\pi}$}
\begin{figure}[h!]
\subfigure[CEMP's criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\subfigure[CUSUM criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\caption{Inverse gains, $\psi_{\pi} =1.01, \psi_x=0$}
\end{figure}

\begin{figure}[h!]
\subfigure[CEMP's criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\subfigure[CUSUM criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\caption{Inverse gains, $\psi_{\pi} =1.1, \psi_x=0$}
\end{figure}

\begin{figure}[h!]
\subfigure[CEMP's criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\subfigure[CUSUM criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\caption{Inverse gains, $\psi_{\pi} =1.2, \psi_x=0$}
\end{figure}

\begin{figure}[h!]
\subfigure[CEMP's criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\subfigure[CUSUM criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\caption{Inverse gains, $\psi_{\pi} =1.5, \psi_x=0$}
\end{figure}

\begin{figure}[h!]
\subfigure[CEMP's criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\subfigure[CUSUM criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\caption{Inverse gains, $\psi_{\pi} =1.8, \psi_x=0$}
\end{figure}

\begin{figure}[h!]
\subfigure[CEMP's criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\subfigure[CUSUM criterion]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\caption{Inverse gains, $\psi_{\pi} =2, \psi_x=0$}
\end{figure}
\clearpage

\newpage
\section{Reference plots}
\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_true_baseline_no_info_ass_constant_only_2020_01_24}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_true_baseline_no_info_ass_slope_and_constant_2020_01_24}}
\caption{Baseline}
\end{figure}

\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_pil_suboptimal_fcst_constant_only_2020_01_24}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_pil_suboptimal_fcst_slope_and_constant_2020_01_24}}
\caption{Lagged inflation in Taylor rule, ``suboptimal forecasters" info assumption}
\end{figure}

\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_il_suboptimal_fcst_constant_only_2020_01_24}}
\subfigure[Learning slope and constant - no projection facility in the world can make this not explode]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath placeholder}}
\caption{Interest rate smoothing, ``suboptimal forecasters" info assumption}
\end{figure}

\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Epi_no_info_ass_constant_only_2020_01_24}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Epi_no_info_ass_slope_and_constant_2020_01_24}}
\caption{Expected inflation in Taylor rule}
\end{figure}

\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_indexation_suboptimal_fcst_constant_only_2020_01_24}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_indexation_suboptimal_fcst_slope_and_constant_2020_01_24}}
\caption{Indexation in NKPC, ``suboptimal forecasters" info assumption}
\end{figure}

\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_true_baseline_dont_know_TR_constant_only_2020_01_24}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_true_baseline_dont_know_TR_slope_and_constant_2020_01_24}}
\caption{Learn Taylor rule}
\end{figure}

\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_learn_hx_true_baseline_no_info_ass_constant_only_2020_01_24}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_learn_hx_true_baseline_no_info_ass_slope_and_constant_2020_01_24}}
\caption{Learn $h_x$}
\end{figure}

\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Markov_switchingTR_true_baseline_no_info_ass_constant_only_2020_01_24_passive}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Markov_switchingTR_true_baseline_no_info_ass_slope_and_constant_2020_01_24_passive}}
\caption{Markov-switching Taylor rule, conditional on passive regime only, learning initialized at passive regime}
\end{figure}


\end{document}





