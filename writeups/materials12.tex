\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb,lscape, natbib}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{setspace}
\usepackage[final]{pdfpages}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm, bottom=2.5cm]{geometry}
\usepackage{natbib} 
\usepackage{bibentry} 
\newcommand{\bibverse}[1]{\begin{verse} \bibentry{#1} \end{verse}}
\newcommand{\vs}{\vspace{.3in}}
\renewcommand{\ni}{\noindent}
\usepackage{xr-hyper}
\usepackage[]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}
\usepackage[capposition=top]{floatrow}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
\usepackage{tikz}
 
\pagestyle{fancy} % customize header and footer
\fancyhf{} % clear initial header and footer
%\rhead{Overleaf}
\lhead{\centering \rightmark} % this adds subsection number and name
\lfoot{\centering \rightmark} 
\rfoot{\thepage} % put page number (the centering command puts it in the middle, don't matter if you put it in right or left footer)

\def \myFigPath {../figures/} 
% BE CAREFUL WITH FIGNAMES, IN LATEX THEY'RE NOT CASE SENSITIVE!!
\def \myTablePath {../tables/} 

%\definecolor{mygreen}{RGB}{0, 100, 0}
\definecolor{mygreen}{RGB}{0, 128, 0}

\definecolor{citec}{rgb}{0,0,.5}
\definecolor{linkc}{rgb}{0,0,.6}
\definecolor{bcolor}{rgb}{1,1,1}
\hypersetup{
%hidelinks = true
  colorlinks = true,
  urlcolor=linkc,
  linkcolor=linkc,
  citecolor = citec,
  filecolor = linkc,
  pdfauthor={Laura G\'ati},
}


\geometry{left=.83in,right=.89in,top=1in,
bottom=1in}
\linespread{1.5}
\renewcommand{\[}{\begin{equation}}
\renewcommand{\]}{\end{equation}}

% New Options
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

%\newtheorem{theorem}{Theorem}[section] % the third argument specifies that their number will be adopted to the section
%\newtheorem{corollary}{Corollary}[theorem]
%\newtheorem{lemma}[theorem]{Lemma}
%\declaretheorem{proposition}
%\linespread{1.3}
%\raggedbottom
%\font\reali=msbm10 at 12pt

% New Commands
\newcommand{\real}{\hbox{\reali R}}
\newcommand{\realp}{\hbox{\reali R}_{\scriptscriptstyle +}}
\newcommand{\realpp}{\hbox{\reali R}_{\scriptscriptstyle ++}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\argmin}{arg\,min}
\newcommand\w{3.0in}
\newcommand\wnum{3.0}
\def\myFigWidth{5.3in}
\def\mySmallerFigWidth{2.1in}
\def\myEvenBiggerFigScale{0.8}
\def\myPointSixFigScale{0.6}
\def\myBiggerFigScale{0.4}
\def\myFigScale{0.3}
\def\myMediumFigScale{0.25}
\def\mySmallFigScale{0.22}
\def\mySmallerFigScale{0.18}
\def\myTinyFigScale{0.16}
\def\myPointFourteenFigScale{0.14}
\def\myTinierFigScale{0.12}
\def\myAdjustableFigScale{0.18}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % this defines a command to make align only number this line
\newcommand{\code}[1]{\texttt{#1}} %code %

\renewcommand*\contentsname{Overview}
\setcounter{tocdepth}{2}

% define a command to make a huge question mark (it works in math mode)
\newcommand{\bigqm}[1][1]{\text{\larger[#1]{\textbf{?}}}}

\begin{document}

\linespread{1.0}

\title{Materials 12 - tinkering around with policy and expectation formation}
\author{Laura G\'ati} 
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%             DOCUMENT           %%%%%%%%%%%%%%%%%% 

\tableofcontents

%\listoffigures

\section{Model summary}
\begin{align}
x_t &=  -\sigma i_t +\hat{\E}_t \sum_{T=t}^{\infty} \beta^{T-t }\big( (1-\beta)x_{T+1} - \sigma(\beta i_{T+1} - \pi_{T+1}) +\sigma r_T^n \big)  \label{prestons18}  \\
\pi_t &= \kappa x_t +\hat{\E}_t \sum_{T=t}^{\infty} (\alpha\beta)^{T-t }\big( \kappa \alpha \beta x_{T+1} + (1-\alpha)\beta \pi_{T+1} + u_T\big) \label{prestons19}  \\
i_t &= \psi_{\pi}\pi_t + \psi_{x} x_t  + \rho i_{t-1} + \bar{i}_t \label{TR}
\end{align}
\begin{equation}
\hat{\E}_t z_{t+h} =  \begin{bmatrix}\bar{\pi}_{t-1} \\ 0 \\ 0 \end{bmatrix}+ bh_x^{h-1}s_t  \quad \forall h\geq 1 \quad \quad b = g_x \; h_x \quad \quad \text{PLM} \label{PLM}
\end{equation}
\begin{equation}
\bar{\pi}_{t} = \bar{\pi}_{t-1} +k_t^{-1}\underbrace{\big(\pi_{t} -(\bar{\pi}_{t-1}+b_1s_{t-1}) \big)}_{\text{fcst error using (\ref{PLM})} } \quad \quad  \text{($b_1$ is the first row of $b$)}
\end{equation}
 \begin{align*}
k_t & = \begin{cases} k_{t-1}+1 \quad \text{for decreasing gain learning}  \\ \bar{g}^{-1}  \quad \text{for constant gain learning.}\numberthis
\end{cases} 
\end{align*}

\newpage
\section{Some initial quick changes}\label{changes}
\begin{enumerate}
\item To policy
\begin{enumerate}
\item $\E(\pi)$ instead of $\pi$ in TR
\item Check the fake $\psi_{\pi} < 1$ exercise.
\end{enumerate}
\item To expectation formation
\begin{enumerate}
\item Curiosity: check IRFs from Euler equation learning
\item IRFs from vector learning (meaning learn all observables)
\item Different implications from Bayesian learning?
\end{enumerate}
\end{enumerate}

Some reasoning (motivation and results):
\begin{enumerate}
\item $\E(\pi)$ instead of $\pi$ in TR: indeed makes overshooting larger in magnitude b/c policy is reacting to something that moves more.
\item $\psi_{\pi} \leq 1$: indeed kills the overshooting, but - no surprise - makes observables unstable (IRFs don't return to steady state). Why does it work to kill the overshooting? B/c the Ball-effect of anticipated interest rate reactions no longer overweighs (less expectational feedback).
\item Townsend (1983) investigates ``forecasting the forecasts of others" and finds damped oscillations $\rightarrow$ do higher-order beliefs play a role for causing oscillations in learning? If so, EE learning IRFs should exhibit no oscillations (and indeed they do not!)
\item Vector learning: are model implications different when agents learn the LOM of not only inflation but also of the other variables? $\rightarrow$ No. (Note: I'm using the same gain for all variables.)
\item Does learning both slope and constant make a difference? $\rightarrow$ Yes, in particular for constant gain learning. 2 effects: 1) less foresight, so $i$ needs to be less expansionary 2) more bumpy IRFs.
\begin{itemize}
\item[] 1) I think what might be going on here is that the only thing agents now know is $h_x$. Therefore the Ball-type ``disinflationary boom"-effect happens to a lesser extent b/c agents do not internalize movements in the interest rate in response to future inflation as much as they would otherwise (feedback from expectations is lower).
\item[] 2) More bumpy because since you're learning $b$, the loading on shocks, the specific sequence of shocks matters. Increasing the size of the cross-section, $N$, mitigates this somewhat. 
\end{itemize}
\end{enumerate}


\newpage
\section{IRFs from vector learning: EE and LH, $T=400, N=100$}
\begin{figure}[h!]
\subfigure[EE, dgain]{
\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_IRFs_many_learning_RIR_EE_monpolconstant_onlydgain_gbar_0_145}}
\subfigure[LH, dgain]{
\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpolconstant_onlydgain_gbar_0_145}}
\subfigure[EE, cgain]{
\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_IRFs_many_learning_RIR_EE_monpolconstant_onlycgain_gbar_0_145}}
\subfigure[LH, cgain]{
\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpolconstant_onlycgain_gbar_0_145}}
\caption{Learning constant only}
\end{figure}

\begin{figure}[h!]
\subfigure[EE, dgain]{
\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_IRFs_many_learning_RIR_EE_monpolslope_and_constantdgain_gbar_0_145}}
\subfigure[LH, dgain]{
\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpolslope_and_constantdgain_gbar_0_145}}
\subfigure[EE, cgain]{
\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_IRFs_many_learning_RIR_EE_monpolslope_and_constantcgain_gbar_0_145}}
\subfigure[LH, cgain]{
\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpolslope_and_constantcgain_gbar_0_145}}
\caption{Learning slope and constant}
\end{figure}


\section{A technical note on the projection facility}
Contrary to Liam Graham, I never have explosive path issues for long-horizon learning, but I do sometimes for Euler-equation learning (Graham claims this is never an issue for EE learning). Graham's solution for the projection facility is to check the eigenvalues of the learning matrix $\phi$. My silly issue is that $\phi$ is not square. Therefore what I do is I check the eigenvalues of the following cheating matrix $(\phi\phi').^{1/2}$, where ``$.{1/2}$'' denotes the square root of the elements. Thoughts?

\section{A note on the empirical counterpart: IRFs in the data}
I note in passing that contrary to my initial priors, one possibility is that the theoretical IRFs generated by learning models actually fit empirical IRFs if the latter are properly computed. Now I base this argument not only on your oscillating IRFs from your CEE-replication, but more broadly on the discussion of computing IRFs in Valerie Ramey's handbook chapter (pointed to by Susanto):
\begin{itemize}
\item ``iterated'' forecasting vs. ``direct'' forecasting
\item[] Ramey, p. 84: ``[O]ne can forecast future values of a variable using either a horizon-specific regression (``direct'' forecasting) or iterating on a one-period ahead estimated model (``iterated'' forecasting).'' Ramey suggests that calculating an IRF from a SVAR is analogous to iterated forecasting while using \textbf{local projection} \`{a} la Jord\`{a} (2005) is analogous to direct forecasting. As pointed out by Susanto as well, using local projection to compute IRFs is likely to yield more bumpy IRFs because the direct estimation of each horizon avoids the smoothness assumption embedded in an IRF coming from a SVAR.
\item[$\rightarrow$] One battle one could fight, which however I am reluctant to fight at this point, is to argue that the tradition of smooth, non-oscillatory IRFs in the data is wrong. Nonetheless I think it's worth keeping in mind. (And at least I can hint that I've at least looked at data at one point in my life.)
\end{itemize}

\section{Changes to expectation formation}
In order to dampen overshooting...
\begin{itemize}
\item Analysis in Section \ref{changes} suggests: learning should be about the slope as well, not just the constant
\item Ball (1994) suggests: learning should work in a way to decrease the movement of expectations, not to increase it $\rightarrow$ can we get anchored ``at the wrong place'' so that expectations do not move?
\item Or if expectations move just as much, we need to dampen the feedback from expectations.
\end{itemize}

What I've looked into:
\begin{itemize}
\item Bayesian learning (Gerko 2019, Collins-Dufresne et al 2016, Evans, Honkapohja \& Sargent 2016)
\begin{itemize}
\item I'm not sure if this is entirely correct, but I understand Bayesian learning to be the same as adaptive learning with the following two differences:
\begin{enumerate}
\item Initialize using priors, and update using the Kalman filter instead of least-squares
\item Can incorporate parameter uncertainty: can get around anticipated utility (although some confusion on whether this is simply an analogue of constant gain learning)
\end{enumerate}
\item The hope here is to dampen overshooting. I see some chance for it for 2 reasons:
\begin{enumerate} 
\item because the Kalman gain gives us a model-specific way of choosing the value of the constant gain, dampening the movement in expectations
\item because parameter uncertainty, if it allows me to get lower effects on future expectations than anticipated utility would, could dampen the interest rate feedback
\end{enumerate}
\item A note: Evans et al make a strange side comment: when the feedback effect from expectations is negative and sufficiently strong (in the cobweb model $\alpha < -1$) ``a possible problem of overshooting can emerge when agents overparametrize the PLM'' - oh really?????
\end{itemize}
\item Nonparametric learning (Kozlowski et al 2019)
\begin{itemize}
\item In Bayesian and LS learning, agents are learning the matrix $g_x$ which is known to be a linear, $ny \times (nx+1)$ matrix.
\item In nonparametric learning, agents instead are learning the function $g$ (in Koz et al, they are learning the distribution of shocks).
\item The recursion they use is a recursive kernel density estimator, giving $\hat{g}_t$ in period $t$.
\item So this is a frequentist approach; Orlik \& Veldkamp 2014 do the same with a Bayesian recursion, imposing a trick to avoid the particle filter (since the estimated function $f$ is potentially nonlinear).
\item Here I don't quite see how this could dampen feedback effects from expectations.
\end{itemize}
\item My overall feeling: the only way to slow down learning is to decrease the gain, so what I need to do instead is to make agents know as little as possible so they can't infer things about the Taylor rule in the future
\begin{itemize}
\item Learn $h_x$ too so you don't internalize how a shock will pan out
\item Have them not know the Taylor rule (how to do that?)
\item Put $\pi_{t-1}$ instead of $\pi_t$ in the Taylor rule, especially if agents don't know $h_x$.
\end{itemize}

\end{itemize}


\section{$\pi_{t-1}$ instead of $\pi_t$ in the Taylor rule}

\begin{figure}[h!]
\includegraphics[scale=\mySmallFigScale]{\myFigPath command_pilTR_RE_pil_in_TR}
\caption{Inflation and inflation expectations in RE for the two Taylor rules}
\end{figure}

Well, the only thing that makes sense to me here is the fact that inflation jumps up in period 2 more under lagged inflation than $\pi_t$. But otherwise I struggle to explain why inflation doesn't fall more on impact when the CB doesn't react to it. 



\end{document}





