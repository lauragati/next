\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb,lscape, natbib}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{setspace}
\usepackage[final]{pdfpages}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm, bottom=2.5cm]{geometry}
\usepackage{natbib} 
\usepackage{bibentry} 
\newcommand{\bibverse}[1]{\begin{verse} \bibentry{#1} \end{verse}}
\newcommand{\vs}{\vspace{.3in}}
\renewcommand{\ni}{\noindent}
\usepackage{xr-hyper}
\usepackage[]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}
\usepackage[capposition=top]{floatrow}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
\usepackage{tikz}
 
\pagestyle{fancy} % customize header and footer
\fancyhf{} % clear initial header and footer
%\rhead{Overleaf}
\lhead{\centering \rightmark} % this adds subsection number and name
\lfoot{\centering \rightmark} 
\rfoot{\thepage} % put page number (the centering command puts it in the middle, don't matter if you put it in right or left footer)

\def \myFigPath {../figures/} 
% BE CAREFUL WITH FIGNAMES, IN LATEX THEY'RE NOT CASE SENSITIVE!!
\def \myTablePath {../tables/} 

%\definecolor{mygreen}{RGB}{0, 100, 0}
\definecolor{mygreen}{RGB}{0, 128, 0}

\definecolor{citec}{rgb}{0,0,.5}
\definecolor{linkc}{rgb}{0,0,.6}
\definecolor{bcolor}{rgb}{1,1,1}
\hypersetup{
%hidelinks = true
  colorlinks = true,
  urlcolor=linkc,
  linkcolor=linkc,
  citecolor = citec,
  filecolor = linkc,
  pdfauthor={Laura G\'ati},
}


\geometry{left=.83in,right=.89in,top=1in,
bottom=1in}
\linespread{1.5}
\renewcommand{\[}{\begin{equation}}
\renewcommand{\]}{\end{equation}}

% New Options
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

%\newtheorem{theorem}{Theorem}[section] % the third argument specifies that their number will be adopted to the section
%\newtheorem{corollary}{Corollary}[theorem]
%\newtheorem{lemma}[theorem]{Lemma}
%\declaretheorem{proposition}
%\linespread{1.3}
%\raggedbottom
%\font\reali=msbm10 at 12pt

% New Commands
\newcommand{\real}{\hbox{\reali R}}
\newcommand{\realp}{\hbox{\reali R}_{\scriptscriptstyle +}}
\newcommand{\realpp}{\hbox{\reali R}_{\scriptscriptstyle ++}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\argmin}{arg\,min}
\newcommand\w{3.0in}
\newcommand\wnum{3.0}
\def\myFigWidth{5.3in}
\def\mySmallerFigWidth{2.1in}
\def\myEvenBiggerFigScale{0.8}
\def\myPointSixFigScale{0.6}
\def\myBiggerFigScale{0.4}
\def\myFigScale{0.3}
\def\myMediumFigScale{0.25}
\def\mySmallFigScale{0.22}
\def\mySmallerFigScale{0.18}
\def\myTinyFigScale{0.16}
\def\myPointFourteenFigScale{0.14}
\def\myTinierFigScale{0.12}
\def\myAdjustableFigScale{0.14}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % this defines a command to make align only number this line
\newcommand{\code}[1]{\texttt{#1}} %code %

\renewcommand*\contentsname{Overview}
\setcounter{tocdepth}{2}

% define a command to make a huge question mark (it works in math mode)
\newcommand{\bigqm}[1][1]{\text{\larger[#1]{\textbf{?}}}}

\begin{document}

\linespread{1.0}

\title{Materials 13 - Still looking for a version of the model w/o overshooting}
\author{Laura G\'ati} 
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%             DOCUMENT           %%%%%%%%%%%%%%%%%% 

\tableofcontents

%\listoffigures

%\newpage
\section{Model summary}
\begin{align}
x_t &=  -\sigma i_t +\hat{\E}_t \sum_{T=t}^{\infty} \beta^{T-t }\big( (1-\beta)x_{T+1} - \sigma(\beta i_{T+1} - \pi_{T+1}) +\sigma r_T^n \big)  \label{prestons18}  \\
\pi_t &= \kappa x_t +\hat{\E}_t \sum_{T=t}^{\infty} (\alpha\beta)^{T-t }\big( \kappa \alpha \beta x_{T+1} + (1-\alpha)\beta \pi_{T+1} + u_T\big) \label{prestons19}  \\
i_t &= \psi_{\pi}\pi_t + \psi_{x} x_t  + \bar{i}_t \label{TR}
\end{align}
\begin{equation}
\hat{\E}_t z_{t+h} =  \begin{bmatrix}\bar{\pi}_{t-1} \\ 0 \; \textcolor{red}{(\bar{x}_{t-1}) } \\ 0 \; \textcolor{red}{(\bar{i}_{t-1}) } \end{bmatrix}+ bh_x^{h-1}s_t  \quad \forall h\geq 1 \quad \quad b = g_x \; h_x \quad \quad \text{PLM} \label{PLM}
\end{equation}
\begin{equation}
\bar{\pi}_{t} = \bar{\pi}_{t-1} +k_t^{-1}\underbrace{\big(\pi_{t} -(\bar{\pi}_{t-1}+b_1s_{t-1}) \big)}_{\text{fcst error using (\ref{PLM})} } \quad \quad  \text{($b_1$ is the first row of $b$)}
\end{equation}
 \begin{align*}
k_t & = \begin{cases} k_{t-1}+1 \quad \text{for decreasing gain learning}  \\ \bar{g}^{-1}  \quad \text{for constant gain learning.}\numberthis
\end{cases} 
\end{align*}

\newpage
\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_true_baseline_no_info_ass_constant_only}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_true_baseline_no_info_ass_slope_and_constant}}
\caption{Reference: baseline model}
\end{figure}


\section{Ideas}
\begin{enumerate}
\item Check $\psi_{\pi}$ above but close to 1 \\
$\rightarrow$ works but only quantitatively; qualitatively, the overshooting is still there, likely because this only cancels out one of the two channels through which $\E{\pi}$ affects $x_t$ negatively.
\item Fix shock for simulation \\
Indeed the issue was that for learning, I accidentally scaled down the shock by $\sigma_i < 1$, while for RE I had maintained $\sigma_i = 1$.
\item Interest rate smoothing as  $i_t = \rho i_{t-1} + (1-\rho)(\psi_{\pi}\pi_t + \psi_x x_t) + \bar{i}_t$ \\
Doesn't work either - it doesn't change the model except reduces $\psi_{\pi}$.
\item Indexation in NKPC \\
Doesn't work either - same model dynamics.
%\item Find optimal gain as analog to Kalman gain

\item Learn $h_x$
\begin{figure}[h!]
\subfigure[Learning constant only]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_learn_hx_true_baseline_no_info_ass_constant_only}}
\subfigure[Learning slope and constant]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_learn_hx_true_baseline_no_info_ass_slope_and_constant}}
\caption{Learning $h_x$, baseline}
\end{figure} \\
Like learning the Taylor rule b/c agents initially don't know if the shock will continue.
\item Central bank's $\E\pi_{t+1}$ in TR? \\
Done a correction for $\hat{\E}\pi_{t+1}$ in TR, now both are stable, but overshooting is still there in both. Not so dissimilar to baseline except that the periods are shifted.

\item Initialize beliefs away from RE somehow\\
Slobodyan \& Wouters do this, but in an estimation context, which I think is necessary because you need pre-sample data to condition priors on.
\item Slobodyan \& Wouters' ``VAR-learning": use lagged observables to learn from, not from states.  \\
\begin{figure}[h!]
\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_VARlearn_true_baseline_no_info_ass_constant_only}
\caption{ VAR learning, baseline, learning only constant}
\end{figure} \\
For learning both slope and constant, not E-stable. Kind of makes sense since I'd think that this amplifies positive feedback.
\item Davig \& Leeper-style switching Taylor rule where only long-run Taylor principle holds?
\begin{figure}[h!]
\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_IRFs_many_learning_RIR_LH_monpol_cgain_gbar_0_145_default_learning_Markov_switchingTR_true_baseline_no_info_ass_constant_only}
\caption{ Markov-switching Taylor rule, baseline, learning only constant (slope learning unstable, why?)}
\end{figure}
\item Some kind of moving average of inflation (or average) in the TR?
\end{enumerate}

A quick question on projection facility: checking \texttt{eig(phi)} when $\phi$ isn't square? \\
Right now I'm splitting up $\phi$ as \texttt{a = phi(:,1), b = phi(:, 2:end)} and then checking \texttt{eig(b)} and \texttt{eig(diag(a))}.

\section{Details on the Markov-switching setup}
Model equations remain the same, except the Taylor rule now is:
\begin{align}
i_t & = \psi_{\pi}(r_t) \pi_t + \psi_x x_t + \bar{i}_t \\
r_t & = \begin{cases} 1 \quad \text{active regime} \quad  \psi_{\pi} = \psi_1 = 2.19  \\  2 \quad \text{passive regime} \quad  \psi_{\pi} = \psi_2= 0.89 \end{cases} \\
r_{t+1} &= \begin{cases} p_{11} 1 + (1-p_{11}) 2 \quad \text{if} \quad r_t = 1\\ (1-p_{22}) 1 + p_{22} 2 \quad \text{if} \quad r_t = 2\end{cases} \quad \text{where} \quad p_{ji}\equiv Prob(s_{t+1}=j | s_t = i)
\end{align}

So I solve the RE model by introducing the new jump variables $\pi_{it}, x_{it}, i_{it}, \; i =1,2$ and writing the model equations as
\begin{align}
x_{it} &=  (p_{1i}\E_t x_{1t+1} +p_{2i}\E_t x_{2t+1})  - \sigma(i_{it} - (p_{1i}\E_t \pi_{1t+1} +p_{2i}\E_t \pi_{2t+1})) +\sigma r_t^n   \\
\pi_{it} &= \kappa x_{it} +\beta (p_{1i}\E_t \pi_{1t+1} +p_{2i}\E_t \pi_{2t+1})) + u_t   \\
i_{it} &= \psi_{i}\pi_{it} + \psi_{x} x_{it}  + \bar{i}_t \label{TR}
\end{align}

Now I unleash the usual method of solving for the observable and state transition matrix $g_x, h_x$. The only difference will be that since the number of jumps now is double the old number, $g_x$ will be $2n_y \times n_x$. Is it correct to interpret $g_{x}(1) \equiv$ \texttt{gx(1:ny,:)} as pertaining to regime 1, and $g_{x}(2) \equiv$ \texttt{gx(ny+1:end,:)} to regime 2?

Then, generating an exogenous regime sequence $r$, I compute RE IRFs as usual for the state block, but depending on the state, I use the corresponding block of $g_x$. With $x_0$ being the impulse, so that $IR^x_1 = x_0$, I do the following:
\begin{align}
IR^y_t & = \begin{cases} g_{x}(1) x_t \quad \text{if} \quad r_t =1 \\ g_{x}(2) x_t \quad \text{if} \quad r_t =2 \end{cases} \\
IR^x_{t+1} & = h_x x_t 
\end{align}

As for the learning model, the compact notation for the model was:
\begin{equation}
z_t  = A_a f_a(t) + A_b f_b(t) + A_s s_t \label{LOM_LR} 
\end{equation}

where the $A$-matrices are functions of the parameters, including $\psi_{\pi}$. The LH expectations $f_a$ and $f_b$ are only functions of the learning coefficients $a,b$ and of the state transition matrix $h_x$. So the only salient difference is that agents react to expectations differently depending on the regime: the $A$-matrices become state-dependent.

Is it correct then to simulate the model as
\begin{equation}
z_t  = A_a(i) f_a(t) + A_b(i) f_b(t) + A_s(i) s_t \quad \text{for} \quad  i=1,2 
\end{equation}
?

\end{document}





