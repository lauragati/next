\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb,lscape, natbib}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{setspace}
\usepackage[final]{pdfpages}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm, bottom=2.5cm]{geometry}
\usepackage{natbib} 
\usepackage{bibentry} 
\newcommand{\bibverse}[1]{\begin{verse} \bibentry{#1} \end{verse}}
\newcommand{\vs}{\vspace{.3in}}
\renewcommand{\ni}{\noindent}
\usepackage{xr-hyper}
\usepackage[]{hyperref}
\usepackage[capposition=top]{floatrow}
\usepackage{amssymb}
%\usepackage[capposition=top]{floatrow}
%\usepackage{palatino} % hell yeah, it looks great


\def \myFigPath {../../figures/} 
% BE CAREFUL WITH FIGNAMES, IN LATEX THEY'RE NOT CASE SENSITIVE!!
\def \myTablePath {../../tables/} 
\def \myBibPath {../../literature/} 


\definecolor{citec}{rgb}{0,0,.5}
\definecolor{linkc}{rgb}{0,0,.6}
\definecolor{bcolor}{rgb}{1,1,1}
\hypersetup{
%hidelinks = true
  colorlinks = true,
  urlcolor=linkc,
  linkcolor=linkc,
  citecolor = citec,
  filecolor = linkc,
  pdfauthor={Laura G\'ati},
}


\geometry{left=.83in,right=.89in,top=1in,
bottom=1in}
\renewcommand{\[}{\begin{equation}}
\renewcommand{\]}{\end{equation}}

% New Options
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{result}{Result}

%\newtheorem{theorem}{Theorem}[section] % the third argument specifies that their number will be adopted to the section
%\newtheorem{corollary}{Corollary}[theorem]
%\newtheorem{lemma}[theorem]{Lemma}
%\declaretheorem{proposition}
%\linespread{1.3}
%\raggedbottom
%\font\reali=msbm10 at 12pt

% New Commands
\newcommand{\real}{\hbox{\reali R}}
\newcommand{\realp}{\hbox{\reali R}_{\scriptscriptstyle +}}
\newcommand{\realpp}{\hbox{\reali R}_{\scriptscriptstyle ++}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\argmin}{arg\,min}
\newcommand\w{3.0in}
\newcommand\wnum{3.0}
\def\myFigWidth{5.3in}
\def\mySmallerFigWidth{2.1in}
\def\myEvenBiggerFigScale{0.8}
\def\myPointSixFigScale{0.6}
\def\myBiggerFigScale{0.4}
\def\myFigScale{0.3}
\def\mySmallFigScale{0.22}
\def\mySmallerFigScale{0.18}
\def\myTinyFigScale{0.16}
\def\myPointFourteenFigScale{0.14}
\def\myTinierFigScale{0.12}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % this defines a command to make align only number this line
\newcommand{\code}[1]{\texttt{#1}} %code %

\renewcommand*\contentsname{Overview}
\setcounter{tocdepth}{2}
\linespread{1.2}

% a set of nice blue, nice red and nice green
\definecolor{dodgerblue}{RGB}{16,78,139}
\definecolor{aquamarinegreen}{RGB}{69,139,116}
\definecolor{brownredlight}{RGB}{238,59,59}
\definecolor{brownreddark}{RGB}{205,51,51}

%%%%%%%%%%%%%%%%%%%%
% DEFINE ALL VARIABLE NUMBERS HERE
%%%%%%%%%%%%%%%%%%%%

% numbers update: 27 August 2020 using Calibration C from Materials 43

% Empirical numbers
% mean estimated alpha
\def\meanalph{(0.42; 0.14; 0; 0; 0.1)}
\def\calibCalph{(0.8; 0.4; 0; 0.4; 0.8)}
% for 1% fe, you get a gain of
\def\oneppFEgives{0.05}
% pp forecast error that unanchors
\def\ppFEunanchors{1 }

% retired
%% for 1% fe, you get a change in pibar of
%\def\betPibFe{0.0906 } % analyze_opt_policy.m
%% raise/lower interest rate by x pp when fe moves up/down by \ppFEunanchors, that is, when LR-exp move by \betPibFe
%\def\ppMoveFFR{4.53 }

% raise/lower interest rate by x pp when when LR-exp move by 0.1
\def\movei{5 }

%%%%%%%%%%%%%%%%%%%%
% DEFINE ALL FIGNAMES HERE
%%%%%%%%%%%%%%%%%%%%

% Market-based expectations
\def\fignameMarketEPi{epi10_2020_06_04}
% cleaned from liq premium
\def\fignameMarketEPiCleaned{cleaned_epi10_2020_07_28}


% Estimated coefficients alpha
% materials 37, Fig 13. Mean(alph) = (0.3509;0.1427;0.0180;0.0005;0.0204;0.1406;0.3411)
\def\fignameAlphaHat{alph_opt_constant_only_pi_only_N_100_nfe_5femax_2_loss_382_gridspacing_manual_Wdiffs2_100000_Wmid_1000_Nsimulations_scaleW_0_use_expectations_0_use_meas_error_0_command_GMM_LOMgain_univariate_12_Aug_2020_09_16_29}

% Comparative statics of optimal policy
% di/dpibar
\def\fignameDiDpibar{analyze_opt_policy_ip27_Aug_2020}

% Observables in PEA for TR
\def\fignamePEAobsTR{implement_anchTC_obs_TR_approx27_Aug_2020}
% Observables in PEA for anchoring
\def\fignamePEAobsAnch{implement_anchTC_obs_approx27_Aug_2020_16_21_47}

% Central bank loss as a function of psi_pi in RE vs. anchoring
% Note 30 July 2020: objective_CB_approx.m should use sim_learn_approx_univariate.m to use the univariate anchoring function, but it leads to a bunch of explosions, so I'm not using the correction.
% Note 23 August 2020: now using sim_learn_approx_univariate.m and it simply leads to lots of explosions (I guess alpha and sig are high, and they can't tolerate psi_pi>1.4). But that's fine.
% Note 25 August: update with convention: RE red, anchoring blue.
\def\fignameCBlossnilnil{plot_sim_loss_approx_pretty_losses_again_critsmooth_constant_only_pi_only_lamx0_lami0_2020_08_27}
\def\fignameCBlossonenil{plot_sim_loss_approx_pretty_losses_again_critsmooth_constant_only_pi_only_lamx1_lami0_2020_08_27}
\def\fignameCBlossnilone{plot_sim_loss_approx_pretty_losses_again_critsmooth_constant_only_pi_only_lamx0_lami1_2020_08_27}
\def\fignameCBlossoneone{plot_sim_loss_approx_pretty_losses_again_critsmooth_constant_only_pi_only_lamx1_lami1_2020_08_27}



% Autocovariogram
\def\fignameAutocov{autocovariogram_N_100_nfe_5_gridspacing_manual_Wdiffs2_100000_Wmid_1000_Nsimulations_command_sigmas_27_Aug_2020_11_29_47}

% Compare PEA and VFI policy for sequence of shocks X1 (rng 2)
\def\fignamePEAvsVFIfirstX{compare_value_pea_results_approx_value_outputs_approx27_Aug_2020_14_28_32_pea_outputs_approx27_Aug_2020_14_45_53_pretty_27_Aug_2020_16_21_47}

% IRFs anchored vs unanchored
% done
\def\fignameIRFanchored{command_IFS_anchoring_pretty_RIR_LH_anch_monpol_again_critCUSUM_constant_only_T_400_N_1000_burnin_5_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_lami_0_date_2020_06_05}
\def\fignameIRFunanchored{command_IFS_anchoring_pretty_RIR_LH_unanch_monpol_again_critCUSUM_constant_only_T_400_N_1000_burnin_5_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_lami_0_date_2020_06_05}

% Cross-sectional average gains for various psi_pi
% done
\def\fignameGainPsiSmall{command_IFS_anchoring_pretty_invgain_again_critCUSUM_constant_only_params_psi_pi_1_01_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_lami_0_2020_06_05}
\def\fignameGainPsiMedium{command_IFS_anchoring_pretty_invgain_again_critCUSUM_constant_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_lami_0_2020_06_05}
\def\fignameGainPsiBig{command_IFS_anchoring_pretty_invgain_again_critCUSUM_constant_only_params_psi_pi_2_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_lami_0_2020_06_05}

% IRFs unanchored for various psi_pi
% done
\def\fignameIRFpsipiSmall{command_IFS_anchoring_pretty_RIR_LH_unanch_monpol_again_critCUSUM_constant_only_T_400_N_1000_burnin_5_params_psi_pi_1_01_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_lami_0_date_2020_06_05}
\def\fignameIRFpsipiMedium{command_IFS_anchoring_pretty_RIR_LH_unanch_monpol_again_critCUSUM_constant_only_T_400_N_1000_burnin_5_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_lami_0_date_2020_06_05}
\def\fignameIRFpsipiBig{command_IFS_anchoring_pretty_RIR_LH_unanch_monpol_again_critCUSUM_constant_only_T_400_N_1000_burnin_5_params_psi_pi_2_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_lami_0_date_2020_06_05}

%%%%%%%%%%%%%%%%%%%%


\begin{document}



\title{Monetary Policy \& Anchored Expectations \\
An Endogenous Gain Learning Model \\
\vspace{0.8cm}
\small{Preliminary and Incomplete}}
\author{Laura G\'ati} 
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%             DOCUMENT           %%%%%%%%%%%%%%%%%% 

\begin{abstract}
This paper analyzes optimal monetary policy in a behavioral model where expectation formation is characterized by potential anchoring of expectations. Expectations are anchored when in an adaptive learning setting, the private sector chooses a decreasing learning gain so that long-run expectations are stable. Within the context of an otherwise standard macro model with nominal rigidities and natural-rate and cost-push shocks, I find that the anchoring mechanism introduces two new intertemporal tradeoffs: a stabilization and a volatility tradeoff. Optimal policy is thus time-varying in two ways. First, the anchoring expectation formation allows the central bank to postpone current intratemporal tradeoffs to the future. Second, while concerns for stabilization lead the central bank to seek to anchor expectations in the long run, getting them anchored is costlier the more current expectations are unanchored. Therefore optimal policy involves an aggressive interest rate response to any threat of expectations unanchoring. My estimate of the anchoring function and the optimal policy function together imply that the bank lowers the interest rate by \movei percentage points if long-run inflation expectations drop by 0.1 pp. Moreover, in the presence of an anchoring expectation formation, already switching to an anchoring-model-optimal Taylor-rule coefficient on inflation achieves a reduction in volatility cost of about 50\%.
\end{abstract}



%\tableofcontents

%\listoffigures

 %%%%%%%%%%%%%%%%%%           INTRO            %%%%%%%%%%%%%%%%%% 
\newpage
\section{Introduction}\label{introduction}

\begin{quote}
I must - and I do - take seriously the risk that inflation shortfalls that persist even in a robust economy could precipitate a difficult-to-arrest downward drift in inflation expectations. At the heart of [our] review is the evaluation of potential changes to our strategy designed to strengthen the credibility of our [...] 2 percent inflation objective [such that] inflation expectations [remain] well anchored.\\
Jerome Powell, Chairman of the Federal Reserve \footnote{Federal Reserve ``Conference on Monetary Policy Strategy, Tools, and Communications Practices,''  June 4, 2019, Opening Remarks.}
\end{quote}
%\begin{quote}
%Policymakers came out of the Great Inflation era with a clear understanding that it was essential to anchor inflation expectations at some low level. \\
%Jerome Powell, Chairman of the Federal Reserve \footnote{Federal Reserve ``Challenges for Monetary Policy,''  August 23, 2019, Opening Remarks.}
%\end{quote}

In his opening remarks to the Federal Reserve's recent review of its policy tools, Fed Chairman Jerome Powell echoes the common sentiment among central bankers that anchoring inflation expectations is ``at the heart'' of monetary policy. Yet the leading macroeconomic models of monetary policy are based on the assumption of model-consistent expectations. This assumption, often termed ``rational expectations'' (RE), does not capture unanchored expectations because expectations of long-run variables are by assumption equal to their steady state values - indeed, the steady state values ``anchor'' expectations. 

But a look at the data reveals that expectations of long-run inflation move around a lot, and, in particular, may not always be anchored at the steady state target of 2\%.\footnote{By ``expectations of long-run inflation'' I mean the private sector's expectation of where inflation will be on average over a longer period in the future. Therefore I also refer to this as ``expected mean inflation.''} Fig. \ref{epi} portrays this series for the last decade in the US. As the figure shows, long-run inflation expectations of the public, averaging a little above the 2\% target prior to 2015, display a marked downward drift since 2015. 

\begin{figure}[h!]
\includegraphics[scale = \mySmallFigScale]{\myFigPath \fignameMarketEPi}
\caption{Market-based inflation expectations, 10 year, average, \%}
\floatfoot{Breakeven inflation, constructed as the difference between the yields of 10-year Treasuries and 10-year Treasury Inflation Protected Securities (TIPS). For a discussion of a potential negative bias in this series, see App. \ref{TIPS}.}
\label{epi}
\end{figure}

This paper addresses the need for a theory of optimal monetary policy in an environment where expectations may become unanchored. In my behavioral model of expectation formation, the notion of anchoring is the stability of the private sector's expectations of mean inflation over the long run. As in \cite{carvalho2019anchored}, whether expectations are anchored or not depends on the size of the learning gain, the weight that the public places on current forecast errors when updating its expectation of mean inflation. Since the size of the gain depends on forecast errors, the central bank's interest-rate setting interacts with expectation formation, leading to anchoring or unanchoring of expectations.

The main contribution of the paper is to investigate how the anchoring expectation formation affects the optimal conduct of monetary policy. For this end, I first infer the functional form of the anchoring function - the relationship between forecasting errors and learning gain -  from data. I subsequently solve the Ramsey problem of the central bank in the model of anchoring to present the properties of optimal monetary policy analytically. Due to the endogeneity of the gain, the problem of finding the optimal interest rate setting of the central bank is nonlinear. I therefore rely on global methods to solve for the optimal policy function numerically.

The key takeaway is that monetary policy faces an intertemporal volatility tradeoff due to the anchoring mechanism which it resolves by being exceedingly aggressive on unanchored expectations in the short run. Because of the positive feedback between expectations and observables, anchored expectations yield lower economic volatility than unanchored ones do. But because long-run expectations move more when expectations are unanchored, an aggressive interest rate response meant to anchor expectations induces heightened short-run volatility. Therefore the central bank prefers to avoid unanchoring expectations in the first place, and reacts aggressively to subdue any potential unanchoring. For this reason, to an econometrician estimating central bank behavior using a Taylor-type rule, optimal monetary policy in the anchoring model would appear as a rule with time-varying coefficients. 

Quantitatively, the estimation reveals that already forecast errors above \ppFEunanchors percentage point are sufficient to unanchor expectations. Moreover, the optimal policy implies that if long-run expectations drift down by 0.1 pp, the optimal response is to lower the interest rate by \movei pp.

The remaining results can be summarized as follows.  First, the analytical results show that the optimal Ramsey policy follows a targeting rule that spells out how the intratemporal tradeoff between inflation and unemployment is complimented by novel intertemporal tradeoffs due to the anchoring expectation formation. The extent of these tradeoffs is determined by the current and expected future stance of anchoring. Second, adhering to a Taylor rule is more costly in the anchoring model than it is under rational expectations because a Taylor rule deprives policy from responding to fluctuations in long-run expectations. Lastly, while the fully optimal policy can eliminate the bulk of costly fluctuations, already a change to Taylor-rule coefficients that are optimal under an anchoring expectation formation can reduce volatility costs by a little above 50\%. This means that if, as my estimation indicates, the true model of expectations is closer to the anchoring model than to rational expectations, a small policy change of adapting Taylor-rule coefficients to the anchoring model would result in significant improvement in welfare.

%The second analytical result is a reminder that without the rational expectations assumption, many widely assumed properties of monetary models vanish. In particular, unlike time-zero optimal commitment under rational expectations, optimal policy will be time-consistent in the sense of \cite{kydland1977rules}. But not only that. Optimal policy in the anchoring model is not history-dependent: the commitment and discretion solutions of the Ramsey problem are indistinguishable because lagged multipliers are absent from the solution. Intuitively, expectation formation based on past data cannot incorporate promises on the part of the policymaker regarding the course of future policy. 
%
%Interestingly, this means that the novel intertemporal tradeoff due to anchoring alleviates the intratemporal tradeoff between inflation and the output gap. From this standpoint, it is therefore not unambiguously desirable to anchor expectations because as long as the private sector is learning, the presence of the intertemporal tradeoff allows the central bank to postpone the current intratemporal tradeoff to the future. On the other hand, unanchored expectations imply faster learning and thus faster convergence, eliminating the intertemporal margin. 



Overall, my work uncovers that in the presence of expectations that can become unanchored, the monetary authority needs to monitor the evolution of long-run expectations and adjust its policy instrument in accordance with their stance. In particular, the central bank needs to act aggressively if it perceives signs of potential unanchoring to foreclose long-run expectations drifting away from the bank's target. This way, it can maintain the low-volatility benefits of having anchored expectations.  
 %%%%%%%%%%%%%%%%%%           RELATED LITERATURE            %%%%%%%%%%%%%%%%%% 
%\subsection{Related literature}

The model I use to study the interaction between monetary policy and anchoring is a behavioral version of the standard New Keynesian (NK) model of the type widely used for monetary policy analysis. Monetary policy in the rational expectations version of this model has been studied extensively, for example in \cite{clarida1999science} or \cite{woodford2011interest}, whose exposition I follow. The formulation of a target criterion to implement optimal policy is in the tradition of \cite{svensson1999inflation}.

The behavioral part of the model is the departure from rational expectations on the part of the private sector. Instead, I allow the private sector to form expectations via an adaptive learning scheme, where the learning gain - the parameter governing the extent to which forecasting rules are updated - is endogenous. The learning framework belongs to the statistical learning literature advocated in the book by \cite{evans_honkapohja2001}. This literature replaces the rational expectations assumption by postulating an ad-hoc forecasting rule, the perceived law of motion (PLM), as the expectation-formation process. Agents use the PLM to form expectations and update it in every period using recursive estimation techniques. My contribution to this literature is to study optimal monetary policy in a learning model with an endogenous gain.

Adaptive learning is an attractive alternative to rational expectations for several reasons. The first reason is that it is intuitive to think that individuals in the economy do not know the true underlying model. Economists do not know the true model of the economy, so why should firms and households? And just like an econometrician estimates statistical models to form forecasts of relevant variables, it is reasonable to suppose that the private sector does so too. In fact, the anchoring expectation formation in this paper allows the private sector not only to estimate a simple statistical model, but also to adapt its forecasting model according to the volatility of the economic environment. 

Secondly, many studies document the ability of adaptive learning models to match properties of expectations data and of macro aggregates. First and foremost, \cite{milani2007expectations} demonstrates that estimated constant gain learning models match the persistence of inflation without recourse to backward-looking elements in the Phillips curve. \cite{eusepi2011expectations} show how a calibrated adaptive learning version of the real business cycle (RBC) model outperforms the rational expectations version. In particular, even with a small gain, the learning model leads to persistent and hump-shaped responses to iid shocks, resolving the long-standing critique of RBC models of \cite{cogley1993impulse}. 

Having an endogenous gain improves the empirical properties of adaptive learning models further. \cite{milani2014learning} documents that endogenous gain models can generate endogenous time-varying volatility  in models without any exogenous time-variance. Lastly, most related to my work is the paper by \cite{carvalho2019anchored} that estimates the evolution of the endogenous gain for the last fifty years in the US. Not only does the model display excellent out-of-sample forecasting performance in terms of matching long-run expectations, but the gain time series invites a reinterpretation of the Great Inflation as a period of unanchored expectations. %\cite{carvalho2019anchored} thus provide empirical backing to the ideas popularized by \cite{sargent1999} that the conquest of American inflation consisted of anchoring inflation expectations.

The paper is structured as follows. Section \ref{NK} introduces the model. Section \ref{learning} describes the learning framework and spells out the anchoring mechanism. Section \ref{estimation} estimates the anchoring function. Section \ref{analytical} presents the results in four parts. First, Section \ref{ramsey} discusses an analytical characterization of the Ramsey policy. Second, Section \ref{implement} solves for the interest rate sequence that implements the optimal Ramsey allocation using global methods. Third, Section \ref{opt_TR} investigates the optimal choice of response coefficients if monetary policy is restricted to follow a Taylor rule. Fourth, Section \ref{welfare} investigates the welfare gains of switching to the optimal policy under anchoring. Section \ref{conclusion} concludes.

 %%%%%%%%%%%%%%%%%%           NK MODEL            %%%%%%%%%%%%%%%%%% 
%\newpage
\section{The model}\label{NK}
Apart from expectation formation, the model is a standard New Keynesian (NK) model with nominal frictions \`a la \cite{calvo1983staggered}. The advantage of having a standard NK backbone to the model is that one can neatly isolate the way the anchoring mechanism alters the behavior of the model. Since the mechanics of the rational expectations version of this model are well understood, I only lay out the model briefly and pinpoint the places where the assumption of nonrational expectations matters.\footnote{For the specifics of the NK model the reader is referred to \cite{woodford2011interest}.}
\subsection{Households}
The representative household is infinitely-lived and maximizes expected discounted lifetime utility from consumption net of the disutility of supplying labor hours:
\begin{equation}
\hat{\E}_t\sum^{\infty}_{T=t}\beta^{T-t} \bigg[ U(C^i_T) - \int_0^1 v(h^i_T(j)) dj \bigg]
\label{lifetime_U}
\end{equation}
$U(\cdot)$ and $v(\cdot)$ denote the utility of consumption and disutility of labor respectively and $\beta$ is the discount factor of the household. $h^i_t(j)$ denotes the supply of labor hours of household $i$ at time $t$ to the production of good $j$, and the household participates in the production of all goods $j$. Similarly, household $i$'s consumption bundle at time $t$,  $C_t^i$, is a Dixit-Stiglitz composite of all goods in the economy:
\begin{equation}
C^i_t =  \bigg[  \int_0^1 c^i_t(j)^{\frac{\theta-1}{\theta}} dj \bigg]^{\frac{\theta}{\theta-1}}\label{dixit}
\end{equation}
$\theta>1$ is the elasticity of substitution between the varieties of consumption goods. Denoting by $p_t(j)$ the time-$t$ price of good $j$, the aggregate price level in the economy can then be written as
\begin{equation}
P_t =  \bigg[  \int_0^1 p_t(j)^{1-\theta} dj \bigg]^{\frac{1}{\theta-1}}
\label{agg_price}
\end{equation}
The budget constraint of household $i$ is given by
\begin{equation}
 B^i_t \leq (1+i_{t-1})B^i_{t-1} + \int_0^1 w_t(j)h^i_t(j) + \Pi_t^i(j)  dj-T_t -P_tC^i_t
 \label{BC}
\end{equation}
where $\Pi_t^i(j)$ denotes profits from firm $j$ remitted to household $i$, $T_t$ taxes, and $B^i_t$ the riskless bond purchases at time $t$.\footnote{For ease of exposition I have suppressed potential money assets here. This has no bearing on the model implications since it represents the cashless limit of an economy with explicit money balances.}

The only difference to the standard New Keynesian model is the expectations operator, $\hat{\E}$. This is the subjective expectations operator that differs from its rational expectations counterpart, $\E$, in that it does not encompass knowledge of the model. In particular, households have no knowledge of the fact that they are identical. By extension, they also do not internalize that they hold identical beliefs about the evolution of the economy. As we will see in Section \ref{FOCs}, this has implications for their forecasting behavior and will result in decision rules that differ from those of the rational expectations version of the model.

\subsection{Firms}

Firms are monopolistically competitive producers of the differentiated varieties $y_t(j)$. The production technology of firm $j$ is $y_t(j)=A_tf(h_t(j))$, whose inverse, $f^{-1}(\cdot)$, signifies the amount of labor input. Noting that $A_t$ is the level of technology and that $w_t(j)$ is the wage per labor hour, firm $j$ profits at time $t$ can be written as
\begin{equation}
\Pi_t^j = p_t(j)y_t(j) -w_t(j)f^{-1}(y_t(j)/A_t)
\end{equation}
Firm $j$'s problem then is to set the price of the variety it produces, $p_t(j)$, to maximize the present discounted value of profit streams
\begin{equation}
\hat{\E}_t\sum^{\infty}_{T=t}\alpha^{T-t} Q_{t,T} \bigg[ \Pi^j_t(p_t(j))\bigg]
\label{lifetime_profits}
\end{equation}
subject to the downward-sloping demand curve
\begin{equation}
y_t(j) = Y_t \bigg(\frac{p_t(j)}{P_t}\bigg)^{-\theta}
\end{equation}
where 
\begin{equation}
Q_{t,T} = \beta^{T-t} \frac{P_t U_c(C_T)}{P_T U_c(C_t)}
\end{equation}
is the stochastic discount factor from households. Nominal frictions enter the model through the parameter $\alpha$ in Equation (\ref{lifetime_profits}). This is the Calvo probability that firm $j$ is not able to adjust its price in a given period. 

Analogously to households, the setup of the production side of the economy is standard up to the expectation operator. Also here the model-consistent expectations operator $\E$ has been replaced by the subjective expectations operator $\hat{\E}$. This implies that firms, like households, do not know the model equations and fail to internalize that they are identical. Thus their decision rules, just like those of the households, will be distinct from their rational expectations counterparts. 

\subsection{Aggregate laws of motion}\label{FOCs}
The model solution procedure entails deriving first-order conditions, taking a loglinear approximation around the nonstochastic steady state and imposing market clearing conditions to reduce the system to two equations, the New Keynesian Phillips curve (NKPC) and IS curve (NKIS). The presence of subjective expectations, however, implies that firms and households are not aware of the fact that they are identical. Thus, as \cite{preston2005} points out, imposing market clearing conditions in the expectations of agents is inconsistent with the assumed information structure.\footnote{The target of \cite{preston2005}'s critique is the Euler-equation approach as exemplified for example by \cite{bullard2002learning}. This approach involves writing down the loglinearized first-order conditions of the model, and simply replacing the rational expectations operators with subjective ones. In a separate paper, I demonstrate that the Euler-equation approach is not only inconsistent on conceptual grounds as \cite{preston2005} shows, but also delivers substantially different quantitative dynamics in a simulated New Keynesian model. Thus relying on the Euler-equation approach when investigating the role of learning is not only incorrect in terms of microfoundations, but also leads to mistaken quantitative inferences. In the context of this model, the problem becomes more acute when expectations are unanchored.} 

Instead, I prevent firms and households from internalizing market clearing conditions.\footnote{There are several ways of doing this. An alternative to \cite{preston2005}'s long-horizon approach pursued here is the shadow price learning framework advocated by \cite{evans2009SP}.} As \cite{preston2005} demonstrates, this leads to long-horizon forecasts showing up in firms' and households' first-order conditions. As a consequence, instead of the familiar expressions, the NKIS and NKPC take the following form:
 \begin{align}
x_t &=  -\sigma i_t +\hat{\E}_t \sum_{T=t}^{\infty} \beta^{T-t }\big( (1-\beta)x_{T+1} - \sigma(\beta i_{T+1} - \pi_{T+1}) +\sigma r_T^n \big)  \label{NKIS}  \\
\pi_t &= \kappa x_t +\hat{\E}_t \sum_{T=t}^{\infty} (\alpha\beta)^{T-t }\big( \kappa \alpha \beta x_{T+1} + (1-\alpha)\beta \pi_{T+1} + u_T\big) \label{NKPC} 
\end{align}
Here $x_t$, $\pi_t$ and $i_t$ are the log-deviations of the output gap, inflation and the nominal interest rate from their steady state values, and $\sigma$ is the intertemporal elasticity of substitution.\footnote{I am using standard CRRA utility of the form $U(c_t) = \frac{c_t^{1-\sigma}}{1-\sigma}$.} The variables $r_t^n$ and $u_t$ are exogenous disturbances representing a natural rate shock and a cost-push shock respectively. 

The laws of motion (\ref{NKIS}) and (\ref{NKPC}) are obtained by deriving individual firms' and households' decision rules, which involve long-horizon expectations, and aggregating across the cross-section. Importantly, agents in the economy have no knowledge of these relations since they do not know that they are identical and thus are not able to impose market clearing conditions required to arrive at (\ref{NKIS}) and (\ref{NKPC}). Thus, although the evolution of the observables $(\pi,x)$ is governed by the exogenous state variables $(r^n, u)$ and long-horizon expectations via these two equations, agents in the economy are unaware of this. As I will spell out more formally in Section \ref{learning}, it is indeed the equilibrium mapping between states and jump variables the agents are attempting to learn.\footnote{The learning of (\ref{NKIS}) and (\ref{NKPC}) is complicated by the fact that the current stance of expectations figures into the equations, resulting in the well-known positive feedback effects of learning.} 

To simplify notation, I gather the exogenous state variables in the vector $s_t$ and observables in the vector $z_t$ as
\begin{equation}
s_t =  \begin{bmatrix}r_t^n \\ \bar{i}_t \\ u_t \end{bmatrix} \quad \quad \quad \quad  z_t = \begin{bmatrix}\pi_t \\ x_t \\ i_t \end{bmatrix}
\end{equation}
where $\bar{i}_t$ is a shock to the interest rate that only shows up in the model for particular specifications of monetary policy.\footnote{For generality, I treat the exogenous state vector as three-dimensional throughout the paper, even when the monetary policy shock is absent.} This allows me to denote long-horizon expectations by 
 \begin{align}
f_{a,t}  \equiv  \hat{\E}_t\sum_{T=t}^{\infty} (\alpha\beta)^{T-t } z_{T+1} \quad \quad \quad \quad f_{b,t}  \equiv \hat{\E}_t\sum_{T=t}^{\infty} (\beta)^{T-t } z_{T+1} \label{fafb}
\end{align}
As detailed in App. \ref{app_compact}, one can use this notation to reformulate the laws of motion of jump variables (Equations (\ref{NKIS}), (\ref{NKPC}) and (\ref{TR})) compactly as
\begin{equation}
z_t  = A_af_{a,t} + A_b f_{b,t} + A_s s_t \label{LOM_LR} \\
\end{equation}
where the matrices $A_i, \; i=\{a,b,s\}$ gather coefficients and are given in App. \ref{app_compact}. Assuming that exogenous variables evolve according to independent AR(1) processes, I write the state transition matrix equation as
 \begin{equation}
 s_t  = h s_{t-1} + \epsilon_t  \quad \quad \epsilon_t \sim \mathcal{N}(\mathbf{0}, \Sigma) \label{LOM_s}
 \end{equation}
where $h$ gathers the autoregressive coefficients $\rho_j$, $\epsilon_t$ the Gaussian innovations $\varepsilon_t^j$, and $\eta$ the standard deviations $\sigma_t^j$, for $j=\{r,i,u\}$. $\Sigma = \eta \eta'$  is the variance-covariance matrix of disturbances.\footnote{For the sake of conciseness, I have suppressed the expressions for these in the main text. See App. \ref{app_compact}.}


 %%%%%%%%%%%%%%%%%%           LEARNING            %%%%%%%%%%%%%%%%%% 
%\newpage
\section{Learning with an anchoring mechanism}\label{learning}
The informational assumption of the model is that agents have no knowledge of the equilibrium mapping between states and jumps in the model. Without knowing the form of the observation equation, then, they are not able to form rational expectations forecasts.
%\footnote{To see this, observe that an agent with rational expectations would internalize all of the rational expectations state-space system. Therefore, she would use the state transition equation (\ref{LOM_s}) together with the observation equation (\ref{RE_obs}) in App. \ref{app_FG} to forecast future jumps as $\E_t z_{t+k} = g^{RE}\E_ts_{t+k} = g^{RE}h^{k}s_t$, where $g^{RE}$ is the mapping between states and jumps under rational expectations. Agents in the learning model however do not know (\ref{RE_obs}) and are thus indeed unable to form the rational expectations forecast.} 
Instead, agents postulate an ad-hoc forecasting relationship between states and jumps and seek to refine it in light of incoming data. In other words, they act like an econometrician: they estimate a simple statistical model and attempt to improve the fit of their model.
\subsection{Perceived law of motion}
I assume agents consider a forecasting model for jumps of the form
\begin{equation}
\hat{\E}_{t}z_{t+1} = a_{t-1} + b_{t-1} s_{t} \label{PLM}  
\end{equation}
where $a$ and $b$ are estimated coefficients of dimensions $3\times1$ and $3\times3$ respectively. This perceived law of motion (PLM) reflects the assumption that agents forecast jumps using a linear function of current states and a constant, with last period's estimated coefficients. Summarizing the estimated coefficients as $\phi_{t-1} \equiv \begin{bmatrix}a_{t-1} & b_{t-1}\end{bmatrix}$, here $3\times 4$, I can rewrite Equation (\ref{PLM}) as 
\begin{equation} 
\hat{\E}_t z_{t+1} = \phi_{t-1}\begin{bmatrix} 1 \\ s_{t} \end{bmatrix} \label{PLMcompact}
\end{equation}
I also assume that 
\begin{equation}
\hat{\E}_{t}{\phi_{t+k}} = \phi_{t} \quad \forall \; k\geq0 
\end{equation}
This assumption, known in the learning literature as anticipated utility (\cite{kreps1998anticipated}), means that agents fail to internalize that they will update the forecasting rule in the future. This is the most behavioral element in the expectation-formation process since it postulates that agents think differently about their own behavior than how they actually act. Clearly, this poses a higher level of irrationality than not knowing the model and using statistical techniques to attempt to learn it. Because it has been demonstrated not to alter the dynamics of the model (\cite{sargent1999}), anticipated utility has become a standard assumption in the adaptive learning literature in order to simplify the algebra.

Assuming that agents know the evolution of states, that is they have knowledge of Equation (\ref{LOM_s})\footnote{This is another common simplifying assumption in studies of adaptive learning. In an extension, I relax this assumption and find that it has similar implications as having agents learn the Taylor rule: initial responses to shocks lack intertemporal expectation effects, but these reemerge as the evolution of state variables is learned.}, the PLM together with anticipated utility implies that $k$-period ahead forecasts are constructed as
\begin{equation}
\hat{\E}_t z_{t+k} = a_{t-1} + b_{t-1}h^{k-1}s_t  \quad \forall k\geq 1 \label{PLM_fcst_general}
\end{equation}

The timing assumptions of the model are as follows. In the beginning of period $t$, the current state $s_t$ is realized. Agents then form expectations according to (\ref{PLM}) using last period's estimate $\phi_{t-1}$ and the current state $s_t$. Given exogenous states and expectations, today's jump vector $z_t$ is realized. This allows agents to evaluate the most recent forecast error $fe_{t|t-1} \equiv z_t - \phi_{t-1}\begin{bmatrix} 1\\ s_{t-1}\end{bmatrix}$ to update their forecasting rule. The estimate is updated according to the following recursive least-squares algorithm:
\begin{align}
\phi_t  & = \bigg( \phi_{t-1}' + k_t R_t^{-1}\begin{bmatrix} 1 \\ s_{t-1} \end{bmatrix}\bigg(z_{t} - \phi_{t-1} \begin{bmatrix} 1 \\ s_{t-1} \end{bmatrix} \bigg)' \bigg)' \label{RLS} \\
R_t &= R_{t-1} +  k_t \bigg( \begin{bmatrix} 1 \\ s_{t-1} \end{bmatrix} \begin{bmatrix} 1 & s_{t-1} \end{bmatrix}  - R_{t-1} \bigg)
\end{align}
where $R_t$ is the $4\times 4$ variance-covariance matrix of the regressors and $k_t$ is the learning gain, specifying to what extent the updated estimate loads on the forecast error. Clearly, a high gain implies high loadings and thus strong changes in the estimated coefficients $\phi_t$. A low gain, by contrast, means that the current forecast error only has a small effect on $\phi_t$.

\subsection{Endogenous gain as anchoring mechanism}

The vast majority of the learning literature specifies the gain either as a constant, $\bar{g}$, or decreasing with time, so that $k_t = (k_{t-1}^{-1}+1)^{-1}$. Instead, to capture the notion of anchoring, I follow \cite{carvalho2019anchored} to allow firms and households in the model to choose the value of the gain. I use the following endogenous gain specification: let $fe_{t|t-1}$ denote the forecast error of time $t$ variables given information at $t-1$. Then the gain evolves as
\begin{equation}
k_t  = \mathbf{g}(k_{t-1},fe_{t|t-1}) 
\label{anchoring}
\end{equation}
where $\mathbf{g(\cdot)}$ is a smooth, increasing function in both arguments that I refer to as the anchoring function. App. \ref{alternative_criteria} compares alternative specifications for $\mathbf{g(\cdot)}$ and discusses the motivation behind using specific functional forms.\footnote{Equation (\ref{anchoring}) nests decreasing gains or constant gains as special cases. In both cases, the derivative of the anchoring function with respect to its second argument is 0, but a decreasing gain implies $\mathbf{g}(k_{t-1},fe_{t|t-1})  = (k_{t-1}^{-1}+1)^{-1}$, while a constant gain sets $\mathbf{g}(k_{t-1},fe_{t|t-1})  = \bar{g}$.}. I let my choice of functional form for $\mathbf{g(\cdot)}$ be guided by data, wherefore I start my inquiry by estimating the anchoring function (see Section \ref{estimation}).

Having an endogenous gain has the interpretation of agents being able to adapt their forecasting behavior to the volatility of their environment. For a given previous gain, if agents observe small forecast errors, $\mathbf{g(\cdot)}$ is small. Firms and households thus use a small gain to update their forecasting rule, reflecting their belief that the underlying data-generating process (DGP) has not changed compared to their earlier held beliefs. I refer to this case as \emph{anchored expectations} because it captures the notion that the private sector's long-run expectations of the observables are stable. 

By contrast, observing large forecast errors leads to $\mathbf{g}(\cdot)$ being sufficiently large so that the gain increases. This corresponds to assigning a higher weight to more recent observations than old ones. Such a forecasting scheme outperforms a lower gain scheme when the environment is volatile, reflecting a possible regime switch. If the previous DGP has been replaced by a new one, having a high gain allows agents to discount old observations generated by the previous DGP, and rely more on the newest observations that come from the current DGP. In this way, agents can learn the new DGP faster, correcting their previously held long-run expectations. This is the case of \emph{unanchored expectations}, and it induces long-run expectations to drift away from their previous values. 

It is intuitive why the central bank might care whether expectations are anchored or not. When expectations are unanchored at time $t$, the private sector believes that the true DGP involves a different mapping between states and jumps than they previously maintained. Private sector forecasts will thus drift in the direction of the update, implying that the observables will also shift in the same direction owing to the law of motion (\ref{LOM_LR}). From the perspective of the central bank, stabilization of the observables therefore implies stabilization of expectations. However, it is not obvious that the central bank prefers to anchor expectations at all points in time because regime shifts in model parameters might warrant letting the private sector learn the new DGP fast. Indeed, the contribution of this paper is to analyze formally the nature of the monetary policy problem when expectation formation is characterized by the  anchoring mechanism.

\subsection{Actual law of motion}
To complete the model, I now use the specifics of the anchoring expectation formation to characterize the evolution of the jump variables under learning. Using the PLM from Equation (\ref{PLM}), I write the long-horizon expectations in (\ref{fafb}) as
\begin{equation}
f_{a,t} \equiv \frac{1}{1-\alpha\beta}a_{t-1}  + b_{t-1}(I_3 - \alpha\beta h)^{-1}s_t \quad \quad \quad f_{b,t} \equiv \frac{1}{1-\beta}a_{t-1}  + b_{t-1}(I_3 - \beta h)^{-1}s_t  \label{fafb_anal}
\end{equation}
Substituting these into the law of motion of observables (Equation (\ref{LOM_LR})) yields the actual law of motion (ALM):
\begin{equation}
z_t = g_{t-1}^l \begin{bmatrix} 1 \\ s_t
\end{bmatrix}
\label{ALM}
\end{equation}
where $g^l$ is a $3\times4$ matrix given in App. \ref{app_FG}. Thus, instead of the state-space solution of the RE version of the model (Equations (\ref{LOM_s}) and (\ref{RE_obs})), the state-space solution for the learning model is characterized by the pair of equations (\ref{LOM_s}) and (\ref{ALM}). 


 %%%%%%%%%%%%%%%%%%           ESTIMATION            %%%%%%%%%%%%%%%%%% 
%\newpage
\section{Estimating the anchoring function}\label{estimation}
The numerical analysis of monetary policy requires a functional specification for the anchoring function $\mathbf{g}(\cdot)$ of Equation (\ref{anchoring}). But one may be interested in the form of the anchoring function in its own right because it carries the answer to a question crucial to central bankers: for what sign and size forecast errors do expectations become unanchored?

Because the analytical results in Section \ref{ramsey} are based on a specification of the gain as a function of forecast errors only, I here present estimation results for that restricted specification.\footnote{Estimates of the general functional form are available on request.} Since the shape of the anchoring function is meaningful, I employ a piecewise linear approximation of the form:

\begin{equation}
\mathbf{g}(fe_{t|t-1}) = \alpha b(fe_{t|t-1})\label{gain}
\end{equation}

where $b(\cdot)$ is a second order spline basis and $\alpha$ is a vector of approximating coefficients. I estimate $\alpha$ by simulated method of moments \`a la \cite{lee1991simulation}, \cite{duffie1990simulated} and \cite{smith1993SMM}. I target the autocovariance structure of CPI inflation from the Bureau of Labor Statistics (BEA), the output gap, the federal funds rate from the Board of Governors of the Federal Reserve System and 12-month-ahead inflation forecasts from the Survey of Professional Forecasters (SPF).\footnote{The output gap measure is constructed as the difference between real GDP from the Bureau of Economic Analysis (BEA) and the Congressional Budget Office's (CBO) estimate of real potential output.} The dataset is quarterly and ranges from 1980-Q3 to 2020-Q1. App. \ref{SMM} contains a detailed description of the estimation methodology.

\begin{figure}[h!]
\includegraphics[scale=\mySmallFigScale]{\myFigPath \fignameAlphaHat}
\caption{Estimated $\hat{\alpha}$: $k_t$ as a function of $fe_{t|t-1}$}
\label{alpha_hat}
\floatfoot{Estimates for 5 knots, cross-section of size $N=1000$, imposing convexity with weight 100000}
\end{figure}

Fig. \ref{alpha_hat} presents the estimated coefficients: $\hat{\alpha}=$ \meanalph. The interpretation of the elements of $\hat{\alpha}$ is the value of the gain the private sector choses when it observers a forecast error of a particular magnitude. For example, a forecast error of -5 pp in inflation is associated with a gain of 0.35. Thus, the data suggest that small forecast errors in absolute value are associated with a small gain, while larger ones with a more sizable gain. Negative forecast errors imply slightly larger gains than positive ones of the same magnitude, but in general, the relationship is symmetric.

%that small forecast errors, below \ppFEunanchors pp in magnitude, do not unanchor expectations as the corresponding gain is very small, \oneppFEgives. Above \ppFEunanchors pp, however, the gain increases rapidly.

What amounts to a large gain? In particular, what size of the gain should be considered as signifying unanchored expectations? The consensus in the literature on estimating learning gains lies between 0.01-0.05. On the higher end, \cite{branch2006simple} estimate a constant gain on inflation of 0.062. \cite{milani2007expectations} finds 0.0183. The estimates of the maximal value for endogenous gains lie somewhat higher, at 0.082 in \cite{milani2014learning} and at 0.145 in \cite{carvalho2019anchored}. Calibrated models tend to use the consensus values between 0.01-0.05. However, \cite{eusepi2011expectations} find that the value of 0.002 is sufficient to significantly alter the dynamic behavior of their RBC model. 

An intuitive interpretation of the gain is that its inverse gives the number of past observations the private sector uses to form its current forecasts.  \cite{eusepi2011expectations}'s number, 0.002, thus implies that firms and households, if they see no forecast errors, rely on the last 125 years of data.\footnote{The frequency of my model and data, like that of  \cite{eusepi2011expectations}, is quarterly.} By contrast, in my estimation, forecast errors of 5 pp in absolute value lead to a number around 0.35, which means that agents discount any observations older than about 9 months.

So the unanchoring threshold is around a value of 0.02, which translates to discounting data older than 12 years. That is not a long time horizon; for a household or firm using a gain of 0.02 today, this would imply that the onset of the Great Recession is no longer in its dataset. As the estimation reveals, forecast errors of \ppFEunanchors pp result in gains of approximately \oneppFEgives pp. This allows me to interpret forecast errors exceeding \ppFEunanchors pp as unanchoring expectations.  


 %%%%%%%%%%%%%%%%%%            RESULTS            %%%%%%%%%%%%%%%%%% 
%\newpage
\section{Monetary policy and anchoring}\label{analytical}

This section sets up and solves the optimal monetary policy problem in the model with the anchoring expectation formation. In Section \ref{ramsey}, I begin by analyzing the Ramsey problem of determining optimal paths for the endogenous variables that policy seeks to bring about.  While the anchoring mechanism introduces substantial nonlinearity into the model, it is possible to derive analytically an optimal target criterion for the policymaker to follow. As we shall see, the optimal rule prescribes for monetary policy to act conditionally on the stance of expectations, and will thus be time-varying. In particular, whether expectations are anchored or not matters for the extent to which there is a tradeoff between inflation and output gap stabilization, and also for the volatility cost of getting expectations anchored.  

I then turn to the question of how to implement optimal policy. Section \ref{implement} uses global methods to solve for the interest rate sequence that implements the target criterion. The optimal sequence is contrasted with a Taylor rule with standard parameters. I then discuss the properties that the optimal interest rate policy should have. 

Since history-dependence is not a feature of the optimal solution, purely forward-looking Taylor rules are no longer excluded from the class of rules that can implement the Ramsey solution. In Section \ref{opt_TR}, I therefore restrict attention to Taylor-type feedback rules for the interest rate. I solve for the optimal Taylor-rule coefficient on inflation numerically and investigate how this choice affects the anchoring mechanism. As I expand upon further in Section \ref{welfare}, an identical Taylor rule involves higher fluctuations in the anchoring model than under rational expectations because it does not allow the central bank to respond to long-run expectations. At the same time, it involves responding to inflation even in periods when expectations are anchored, causing excess volatility. For this reason, substantial welfare-improvements are open to the policymaker ready to reconsider the current Taylor-rule coefficients or the policy function more broadly.

  %%%%%%%%%%%%%%%%%%           MON.POL. PROBLEM            %%%%%%%%%%%%%%%%%% 
%\newpage
\subsection{The Ramsey policy under anchoring}\label{ramsey}
I assume the monetary authority seeks to maximize welfare of the representative household under commitment. As shown in \cite{woodford2011interest}, a second-oder Taylor approximation of household utility delivers a central bank loss function of the form
\begin{equation}
L^{CB} =\E_t \sum_{T=t}^{\infty}\{\pi_T^2 +\lambda_x(x_T - x^*)^2 +\lambda_i(i_T - i^*)\} \label{CBloss}
\end{equation}
where $\lambda_j \; j=\{x,i\}$ is the weight the central bank assigns to stabilizing variable $j$ and $j^*$ is its target value.\footnote{To be precise, the second-order approximation to household utility involves $\lambda_i = 0$. In practice, $\lambda_i > 0$ is often assumed to avoid an optimal interest rate path with infinitely large fluctuations.} The central bank's problem, then, is to determine paths for inflation, the output gap and the interest rate that minimize the loss in Equation (\ref{CBloss}), subject to the model equations (\ref{NKIS}) and (\ref{NKPC}), as well as the evolution of long-horizon expectations, spelled out in Section \ref{learning}. A second question is how to implement the optimal allocation; that is, to find a response function for the policy instrument $i_t$ that implements the optimal sequences of the observables.

While for most of the paper I consider a general specification for monetary policy, in Section \ref{opt_TR}, I will restrict attention to a standard Taylor rule:
\begin{equation}
i_t = \psi_{\pi}(\pi_t -\pi^*) + \psi_{x} (x_t -x^*) + \bar{i}_t  \label{TR}
\end{equation}
where $\psi_{\pi}$ and $\psi_{x}$ represent the responsiveness of monetary policy to inflation and the output gap respectively, $\pi^*$ and $x^*$ are the central bank's targets. Lastly, $\bar{i}_t$ is a monetary policy shock. I also assume that when the Taylor rule is in effect, the central bank publicly announces this. Thus Equation (\ref{TR}) is common knowledge and is therefore not the object of learning.\footnote{In an extension I consider the case where the Taylor rule is not known (or not believed) by the public and therefore is learned together with the relations (\ref{NKIS}) and (\ref{NKPC}). This dampens intertemporal expectation effects as long as the Taylor rule is not learned; afterwards, the model dynamics are identical to those of the baseline. } 
 

% %%%%%%%%%%%%%%%%%%           TARGET CRITERION            %%%%%%%%%%%%%%%%%% 

\subsubsection{Optimal Ramsey policy as a target criterion}

Appendix \ref{app_midsimple_problem} lays out the policy problem for a simplified version of the baseline model. It also depicts how the endogeneity of the gain introduces nonlinearity into the model. This prevents an analytical solution to the Ramsey problem. Instead, I characterize the first-order conditions of the problem analytically, and proceed in Section \ref{implement} to solve the full problem numerically. 

To simplify the analytical treatment, I make three additional assumptions compared to the baseline model. First, I assume that only the inflation process is learned; expectations about the output gap and the interest rate are rational evaluations of the infinite sum of future expectations.\footnote{By ``rational'' I here mean the expectations that agents would hold in the rational expectations equilibrium (REE). Because the model is not in the REE, these ``rational'' expectations are not model-consistent.}  Second, I assume that only the constant of the inflation process is learned. These simplifications allow me to focus on the minimal deviation from rational expectations necessary to discuss the unanchoring of inflation expectations. Third, I consider a specification of the anchoring function where the current gain depends on the most recent forecast error only:\footnote{These assumptions are made for algebraic convenience only and do not alter the qualitative implications of the model. For analytical results for the more general version of the anchoring function, see App. \ref{app_generalTC}.}

\begin{equation}
k_t = \mathbf{g}(fe_{t|t-1}) \label{anchoring_simple}
\end{equation}
The solution of the Ramsey problem under these assumptions is stated in the following result.

\begin{result} Target criterion in the anchoring model \\
The targeting rule in the simplified learning model with anchoring is given by
\begin{align*}
& \pi_t = -\frac{\lambda_x}{\kappa}x_t + \frac{\lambda_x}{\kappa}\frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t+fe_{t|t-1}\mathbf{g}_{\pi,t} \bigg)\bigg(\E_t\sum_{i=1}^{\infty}x_{t+i}\prod_{j=0}^{i-1}(1-k_{t+1+j} - fe_{t+j|t+j-1}\mathbf{g_{\bar{\pi},t+j}}) \bigg)
 \numberthis \label{target} \\
 & \text{with} \quad \quad fe_{t+j|t+j-1} = \pi_{t+1+j} - \bar{\pi}_{t+j}-b_1 s_{t+j}
\end{align*}

For the derivation, see Appendix \ref{app_midsimple_problem}. For a general target criterion without assumption (\ref{anchoring_simple}), see Appendix \ref{app_generalTC}. Note the notation that $\prod_{j=1}^{0} \equiv 1$.
\label{result_target_anchoring}
\end{result}
The interpretation of Equation (\ref{target}) is that the \emph{intra}temporal tradeoff between inflation and the output gap due to cost-push shocks is complemented by two \emph{intertemporal} tradeoffs: one due to learning in general, and one due to anchoring in particular. 

The first intertemporal effect comes from the current level of the gain,  $k_t$, which captures how far learning is from converging to rational expectations. The second intertemporal tradeoff is manifest in the derivative of the anchoring function today,  $\mathbf{g}_{\pi,t}$, as well as in all expected levels and changes in the gain in the future in the expression $(1-k_{t+1+j}- fe_{t+j|t+j-1}\mathbf{g_{\bar{\pi},t+j}})$ in the second bracket on the right-hand side. These expressions say that the presence of anchoring qualify the first intertemporal tradeoff because now the degree and direction in which the gain changes today and is expected to change in the future matter too. In other words, the central bank needs to consider whether its chosen interest rate sequence contributes to anchoring expectations in future periods, or whether it actually serves to unanchor them.

Let me investigate these channels in isolation. To see exactly what the role of anchoring is in the target criterion, consider first the special case of exogenous gain adaptive learning, for simplicity with a constant gain specification.\footnote{The intuition is identical if the public were using a decreasing gain but the mathematics would not convey that same intuition as cleanly.} In this case the anchoring function and the forecast error are irrelevant (since $\mathbf{g_i}=0, i=\pi,\bar{\pi}$) and (\ref{target}) boils down to
\begin{align}
\pi_t  = -\frac{\lambda_x}{\kappa}x_t + \frac{\lambda_x}{\kappa} \frac{(1-\alpha)\beta}{1-\alpha\beta} k
\bigg(\sum_{i=1}^{\infty}x_{t+i}(1-k)^i \bigg)
\label{target_molnar} % I think this is still correct, despite the corrections of March 30, 2020.
\end{align}
which is the analogue of \cite{gaspar2010inflation}'s Equation (24).\footnote{In their Handbook chapter, \cite{gaspar2010inflation} provide a parsimonious treatment of \cite{molnar2014optimal}. I am referring to their expression for the target criterion because \cite{molnar2014optimal} do not provide one explicitly.} This result, found also by \cite{molnar2014optimal}, suggests that already the presence of learning by itself is responsible for the first intertemporal tradeoff between inflation and output gap stabilization. However, the fact that the central bank now has future output gaps as a margin of adjustment means that it does not have to face the full tradeoff in the current period. Learning allows the central bank to improve the current output gap without sacrificing inflation stability today; however, this results in a worsened tradeoff in the future. In other words, adaptive learning by itself allows the central bank to postpone the current tradeoff to later periods. 

Intuitively, this happens because adaptive expectations are slow in converging to rational expectations. In the transition, the private sector's expectations do not adjust to fully internalize the intratemporal tradeoff. This gives the monetary authority room to transfer the tradeoff to the future.

Contrasting Equations (\ref{target_molnar}) and (\ref{target}) highlights the role of anchoring. With anchoring, the extent to which policy can transfer the intratemporal tradeoff to future periods depends not only on the stance of the learning process, as in (\ref{target_molnar}), but also on whether expectations are anchored or not and in which direction they are moving. In fact, not only the current stance and change of anchoring matters, but also all expected future levels and changes. 

Anchoring, however, complicates the possibility of transferring today's tradeoff to the future. One can see this in the fact that forecast errors and the derivatives of the anchoring function are able to flip the sign of the second term in (\ref{target}). This means that anchoring can alleviate or worsen the intertemporal tradeoff. To see the intuition, consider the equation system of first-order conditions from solving the Ramsey problem. While the full system is presented in App. \ref{app_midsimple_problem}, I would like to focus on Equation (\ref{gaspar22}), the equation governing the dynamics of observables in the model:
\begin{equation}
2\pi_t = - 2\frac{\lambda_x}{\kappa}x_t +\varphi_{5,t} k_t + \varphi_{6,t} \mathbf{g}_{\pi,t} 
\end{equation}
The Lagrange multipliers $\varphi_5 \geq 0$ and $\varphi_6 \geq 0$ are the multipliers of the RLS updating and the anchoring function respectively. This equation, upon substitution of the solutions for the two multipliers, yields the target criterion. It is therefore easy to read off the intuition at a glance. First, since $\varphi_{5,t}k_t > 0$, one immediately obtains the above-discussed conclusion that as long as the adaptive learning equation is a constraint to the policymaker ($\varphi_{5,t} > 0$), the central bank has more room to transfer the contemporaneous tradeoff between inflation and the output gap to the future.\footnote{Strictly speaking, $\varphi_5$ and $\varphi_6$ are never zero in this model. The reason is that the anchoring model is a convex combination of decreasing and constant gain learning and is thus only weakly E-stable. The former has the RE equilibrium as a limit (strong E-stability), while the latter fluctuates around the REE with bounded variance (weak E-stability). In the anchoring setting, if expectations are anchored and there are no inflation surprises, the gain converges to zero and the decreasing gain limit of discretionary RE obtains. However, exogenous disturbances induce unforecastable variation, producing forecast errors that will unanchor expectations, restarting the learning process. This is in stark contrast with \cite{carvalho2019anchored}, where the anchoring function is the map between the PLM and the expected ALM and thus only depends on the endogenous component of the forecast error. Therefore, in their model, absent regime switches, expectations can never become unanchored once learning has converged.}

However, whether the anchoring equation alleviates or exacerbates the inflation-output gap tradeoff depends on the sign of $\mathbf{g_{\pi,t}}$. If the derivative is positive, the effect is the same as above, and the central bank has more leeway to postpone the tradeoff to the future. By contrast, if the derivative is negative, that is expectations are becoming anchored, the intratemporal tradeoff is worsened.

Why do unanchored expectations give the central bank the possibility to postpone its current inflation-output gap tradeoff? The reason is that when expectations become unanchored, the learning process is restarted. A not-yet converged learning process implies, as discussed above, that postponing the tradeoff is possible. Restarting the convergence process thus unlocks this possibility. 

This seems to suggest that from a stabilization standpoint, the central bank should prefer to have unanchored expectations. As will be shown in Sections \ref{implement}-\ref{opt_TR}, volatility considerations will suggest otherwise. But in fact, even the stabilization viewpoint involves some ambiguity on whether whether expectations should be anchored from the perspective of the central bank. Clearly, the central bank prefers to face a learning process that on the one hand has not yet converged, and on the other is converging only slowly. A high gain under unanchored expectations implies both a sizable distance from convergence as well as faster learning and thus faster convergence. Therefore, ideally the central bank would like to have expectations anchored but the gain far from zero; a contradiction. Once the gain approaches zero, only unanchored expectations can raise it again to restart the learning process. But once the gain is large, the only way to slow down learning is to anchor expectations, that is, to lower the gain.


\subsubsection{Time-consistence of optimal plans under adaptive learning}
Now simplify the target criterion further, assuming that learning has converged, $k_t = \mathbf{g_\pi} = 0$. We are left with 
\begin{equation}
\pi_t  = -\frac{\lambda_x}{\kappa}x_t \label{cgg_discretion}
\end{equation}
which corresponds to the optimal discretionary solution for rational expectations in \cite{clarida1999science}. This is formalized in the following result.

\begin{result} Coincidence of commitment and discretion under adaptive learning \\
In an adaptive learning model with exogenous or endogenous gain, the optimal Ramsey policies under commitment and discretion coincide. The optimal Ramsey plan is more akin to discretion than to commitment as it does not involve making promises about future policy actions. Optimal policy is thus not subject to the time-inconsistency problem of \cite{kydland1977rules}.
\label{result_no_commitment}
\end{result}

To illustrate this result in a parsimonious manner, consider a simplified version of the model. The planner chooses $\{\pi_t, x_t, f_t, k_t\}_{t=t_0}^{\infty}$ to minimize
 \begin{align*}
\mathcal{L} &= \E_{t_0}\sum_{t=t_0}^{\infty} \beta^{t-t_0}\bigg\{ \pi_t^2  + \lambda x_t^2 + \varphi_{1,t} (\pi_t -\kappa x_t- \beta f_t +u_t) \\ &+ \varphi_{2,t}(f_t - f_{t-1} -k_t(\pi_t - f_{t-1})) + \varphi_{3,t}(k_t - \mathbf{g}(\pi_t - f_{t-1})) \bigg\}
 \end{align*}
 where the IS-curve, $x_t = \E_t x_{t+1}+\sigma f_t -\sigma i_t +\sigma r_t^n$, is a non-binding constraint, and is therefore excluded from the problem. $\varphi_i$ are Lagrange-multipliers and $\E_t x_{t+1}$ is rational.\footnote{Again, the use of the term ``rational'' in this context reflects corresponding to expectations in a REE.} In this simplified setting, $f_t$ is a stand-in variable capturing inflation expectations and evolves according to a recursive least squares algorithm. The anchoring function $\mathbf{g}(\cdot)$ specifies how the gain $k_t$ changes as a function of the current forecast error according to assumption (\ref{anchoring_simple}).\footnote{Here I maintain assumption (\ref{anchoring_simple}) for presentational purposes. It has no bearing on the results.} Note that the problem involves commitment because the monetary authority internalizes the effects of its actions both on the evolution of expectations and on that of the gain. 
 
 After some manipulation, first-order conditions reduce to:
 \begin{align}
  2\pi_t +2\frac{\lambda_x}{\kappa}x_t -\varphi_{2,t}(k_t + \mathbf{g_{\pi}}(\pi_t -f_{t-1}))& = 0 \label{simpleFOC1} \\
  -2\beta\frac{\lambda_x}{\kappa}x_t + \varphi_{2,t} -\varphi_{2,t+1}(1-k_{t+1} -\mathbf{g_{f}}(\pi_{t+1} -f_{t})) & = 0 \label{simpleFOC2} 
 \end{align}
Inspection of this system reveals that, unlike the rational expectations case ($f_t = \E_t{\pi_{t+1}}$), the optimal solution does not involve lagged multipliers.\footnote{This echoes the findings of \cite{molnar2014optimal}.} This implies that the monetary authority cannot condition the optimal time path of inflation and the output gap on the past; optimal policy is not history-dependent. 

The intuition for this result is easy to see if one compares rational expectations and learning in an infinitely repeated game setting. Under rational expectations, the lagged multiplier appears in the solution because expected inflation is a jump variable. This reflects that expectations fulfill a form of optimality, rendering the private sector a strategic player. The adjustment of expectations under rational expectations enables the central bank to make promises about future policy that are incorporated into expectations. 

Not so for adaptive expectations. Learning agents look exclusively to past data to form expectations. Their expectations thus cannot incorporate the policymaker's promises about the future course of policy. In fact, a private sector with adaptive expectations has a pre-specified, non-strategic expectation formation. Therefore, households and firms act as an automaton, leaving the central bank unable to make promises that have any effect on expectations.\footnote{\cite{mele2019perils} report a similar finding in a decreasing gain learning model. Their terminology of contrasting ``inflation-targeting'' with ``price-level targeting" policy rules renders the connection to commitment less immediate, but it is helpful to recall that in the rational expectations NK model, optimal discretionary monetary policy involves inflation stabilization, while optimal commitment entails full price-level stabilization.}

Should we be surprised that adaptive learning involves no distinction between discretion and commitment? Not at all if we recall that the rational expectations revolution had as one of its aims to remedy this feature of expectations. The seminal Lucas-critique, for instance, emphasizes that reduced-form regressions are not ideal guiding principles for policy precisely because they miss the time-varying nature of estimated coefficients due to model-consistent expectations that incorporate policy action (\cite{lucas1976econometric}).

One might be concerned that a model of anchoring that is not immune to the Lucas-critique may not be a desirable normative model for policy. But recall that the anchoring model is a description of the economy in transition, not of one at its ergodic mean. It characterizes expectation formation en route to becoming model-consistent as the private sector learns the underlying DGP. In the long run, then, rational expectations offers a good characterization of the expectation formation of learning agents. It is in the short run that the predictions of the two expectations schemes differ. Data suggest that in terms of positive implications, learning models fare much better than rational expectations models do. Since the seminal work by \cite{coibion2015information}, rational expectations has been rejected in numerous empirical papers. At the same time, the literature cited in the Introduction demonstrates the success of learning models in fitting both the properties of expectations in the data and in improving the business cycle dynamics of other model variables. 

There are also avenues to address the Lucas-critique in learning models. One option is to reintroduce a sense of optimality to expectation formation directly, as in the literature on central bank reputation. (See \cite{cho1995induction} and \cite{ireland2000expectations}). Another possible remedy is to retain a sufficient degree of forward-looking expectations as in the finite-horizon planning approach advocated by \cite{woodford2019monetary}. A third possibility is to model communication by the central bank in the form of news shocks that enter the state vector, and thus show up in the information set of agents, as in \cite{dombeck2017effects}. All in all, Result \ref{result_no_commitment} does not render the advances of the rational expectations revolution void. Instead, it points to the fact in the short run, expectations too slow to update fully will quantitatively resemble adaptive expectations. Thus in the short run, the policy predictions of adaptive expectations may be more quantitative relevant than what is generally acknowledged.


%%%%%%%%%%%%%%%%%%           IMPLEMENTING THE TC           %%%%%%%%%%%%%%%%%% 

\subsection{Implementing the Ramsey policy: the optimal interest rate sequence}\label{implement}
Having a characterization of optimal policy in the anchoring model as a first-order condition, the next relevant question is how the central bank should set its interest rate tool in order to implement the target criterion in (\ref{target}). In other words, we would like to know what time-path of interest rates implements the optimal sequence of inflation and output gaps. As emphasized in Section \ref{ramsey}, the nonlinearity of the model does not admit an analytical answer to this question. I therefore solve for the optimal interest rate policy numerically using global methods. 

For the numerical approach I use the calibration of model parameters outlined in Table \ref{calibration}. Where possible, I adopt standard parameters from the literature. In particular, for $\beta, \sigma$ and other parameters underlying $\kappa$, the slope of the Phillips curve, I rely on the parameterization of \cite{chari2000sticky}, advocated in \cite{woodford2011interest}.\footnote{The composite parameter $\kappa$ is given by  $\kappa = \frac{(1-\alpha)(1-\alpha\beta)}{\alpha} \zeta$, where $\zeta$ is a measure of strategic complementarity in price setting. Assuming specific factor markets, constant desired markups with respect to output levels and no intermediate inputs, $\zeta = \frac{\omega + \sigma^{-1}}{1+\omega\theta}$. Here $\theta$ is the price elasticity of demand and $\omega$ is the elasticity of the marginal cost function with respect to output. \cite{chari2000sticky}'s calibration involves  $\theta =10, \sigma =1, \omega = 1.25, \beta = 0.99$, so that together with my choice of $\alpha$, $\kappa$ is pinned down. Note that I lower $\beta$ slightly (0.98 instead of \cite{chari2000sticky}'s 0.99). This allows the model to better match the autocovariance structure of the output gap because it lowers the pass-through of long-horizon expectations in the IS-curve.} The probability of not adjusting prices, $\alpha$, is set to match an average price duration of two quarters, which is a little below the numbers found in empirical studies.\footnote{The evidence on the average duration of prices can be summarized as follows. On the lower end, \cite{bils2004some} find a mean duration of 4.3 months and \cite{klenow2010microeconomic} find 6.9 months. \cite{klenow2008state} and \cite{nakamura2008five} agree on between 7-9 months, while \cite{eichenbaum2011reference}'s number is 10.6 months. This averages to a little above 7 months. My number for $\alpha$ corresponds to 6 months. I choose the lower end of this spectrum in order to allow the learning mechanism, and not price stickiness, to drive the bulk of the model's persistence.} 
I set $\lambda_x$ to 0.05, the value estimated by \cite{rotemberg1997optimization}. A low, nonzero value for $\lambda_x$ is also desirable because it implies that a concern for output gap stabilization is not the main driver of the central bank's actions, yet the right hand side of the target criterion in (\ref{target}) is not trivial either.

To simplify the numerical analysis as well as interpretation, I restrict the shocks to be iid. The volatilities of the disturbances and, where applicable, the outout-coefficient of the Taylor rule, are set to match the autocovariances of Baxter-King filtered inflation, output gap and interest rate series. This implies standard deviations of 0.01 for the natural rate and monetary policy shock and 0.5 for the cost-push shock. In sections of the paper where I assume a Taylor rule, the matching exercise results in a 0.3 coefficient on the output gap, and unless otherwise specified, I set the inflation coefficient of the Taylor rule to 1.5, the value recommended by \cite{taylor1993discretion}. Note that the asterisks in Table \ref{calibration} demarcate parameters that pertain to the Taylor rule and thus only to sections of the paper which assume that a Taylor rule is in effect.

\begin{center}
\begin{table}
\begin{tabular}{ c | c  | l }
 $\beta$ & 0.98 & stochastic discount factor \\  \hline
 $\sigma$ & 1  & intertemporal elasticity of substitution \\  \hline
 $\alpha$ & 0.5 &  Calvo probability of not adjusting prices \\\hline
 $\kappa$ & 0.0842 &  slope of the Phillips curve \\\hline
 $\psi_{\pi} $& 1.5  & coefficient of inflation in Taylor rule*\\\hline
 $\psi_x$ & 0.3   & coefficient of the output gap in Taylor rule*  \\\hline %% <---- Diffs. wrt calibration2
 $\bar{g}$ & $0.145$  & initial value of the gain \\\hline %% <----
%& & \\ [-1em] % this adds an extra empty row, and decreases its size, so it looks as if thetbar's row was higher
% $\tilde{\theta}$ &  2.5  & threshold value for criterion of endogenous gain choice \\ \hline %% <----
%  $\tilde{\kappa}$ &  0.2  & scaling parameter of gain for forecast error variance estimation \\ \hline %% <----
    $\rho_r$ & 0 &   persistence of natural rate shock \\ \hline %% <---
    $\rho_i$ & 0 &  persistence of monetary policy shock*  \\ \hline
    $\rho_u$ & 0  &  persistence of cost-push shock  \\ \hline
    $\sigma_r$ & 0.01 & standard deviation of natural rate shock  \\ \hline
    $\sigma_i$ &  0.01  &standard deviation of monetary policy shock*  \\ \hline %% <----
    $\sigma_u$ & 0.5 & standard deviation of cost-push shock   \\ \hline  
    $\lambda_x$ & 0.05 & weight on the output gap in central bank loss   \\ \hline  % <----
    $\lambda_i$ & 0 & weight on the interest rate in central bank loss   \\ \hline  
%    $\rho_k$ & 0.5 & persistence of the gain in the smooth anchoring function   \\ \hline  % <----
%    $\gamma_k$ & 0.001 & weight on squared forecast errors in the smooth anchoring function   \\ \hline   % <----
\end{tabular}     
      \caption{Calibrated parameters}  \label{calibration}
       \floatfoot{*Parameters with an asterisk refer to sections of the paper where a Taylor rule is in effect.}
 \end{table}
\end{center}
% TABLE WAS UPDATED 27 AUGUST 2020.  (Calibration C, Materials 43)

\vspace{-1.4cm}

I initialize the value of the gain at $\bar{g}=0.145$, the value for the gain corresponding to unanchored expectations estimated by \cite{carvalho2019anchored}. If the anchoring function were discrete, as in \cite{carvalho2019anchored}, this parameter would have important implications for model dynamics because in that case the gain is restricted to take on this value if it is nondecreasing.\footnote{See App. \ref{alternative_criteria} for alternative specifications of the anchoring function, such as that of \cite{carvalho2019anchored}.} However, since I rely on an approximation of a smooth anchoring function, $\bar{g}$ does not have a strong bearing on model dynamics. The target criterion (\ref{target}) requires that the derivatives of the anchoring function exist, wherefore a smooth specification makes sense. Moreover, in estimating the functional form of the anchoring rule, I allow the data to speak on my calibration of approximating coefficients $\hat{\alpha}$, thereby avoiding arbitrary numerical choices.\footnote{To avoid confusion, I use the notation $\hat{\alpha}$ for the coefficients of the piecewise linear anchoring function, reserving $\alpha$ for the Calvo probability parameter.}

App. \ref{pea} outlines my preferred solution procedure, the parameterized expectations approach, while App. \ref{vfi} gives the details of the parametric value function iteration approach I implement to back out the optimal policy function. The main output of this procedure is an approximation of the optimal interest rate policy as a function of the vector of state variables. Due to Assumption (\ref{anchoring_simple}), the relevant state variables are expected mean inflation and the exogenous states at time $t$ and $t-1$, rendering the state vector five-dimensional:\footnote{In the general specification without Assumption (\ref{anchoring_simple}), the lagged gain would also be a relevant state variable.}

\begin{equation}
X_t = (\bar{\pi}_{t-1}, r^n_t, u_t, r^n_{t-1}, u_{t-1})
\end{equation}

As a first step, I plot how the approximated policy function depends on $\bar{\pi}_{t-1}$, while keeping all the other states at their mean. One can thus interpret the result, depicted on Fig. \ref{di}, as a partial derivative of the interest rate with respect to $\bar{\pi}_{t-1}$.

\begin{figure}[h!]
%\subfigure[$i(k^{-1})$]{\includegraphics[scale=0.17]{\myFigPath \fignameDiDk}} % taking this out b/c policy doesn't depend on this
\includegraphics[scale=0.24]{\myFigPath \fignameDiDpibar}
\caption{Comparative statics: optimal policy response as a function of changes in expected mean inflation, \%}
\label{di}
\end{figure}

As illustrated by Fig. \ref{di}, optimal interest-rate setting responds linearly and very sensitively to the stance of expectations, $\bar{\pi}_{t-1}$. If expected mean inflation decreases by 0.1 pp, the interest rate drops by about \movei pp!\footnote{This result is qualified in the general specification of the model where the private sector learns about all observables, not just inflation. In particular, when the private sector can learn the evolution of the interest rate, then interest rate expectations play a major role in determining forecasts of future inflation and thus add a stabilizing channel that is absent from the current specification. In the more general case, then, smaller responses in the current interest rate are sufficient. \label{footnote_why_nonstationary}} 

By how much should the central bank then raise or lower the interest rate when expectations become unanchored? Fig. \ref{di} suggests that the interest rate track long-run expectations. In the figure, the answer is that the optimal policy should not tolerate any fluctuation in long-run expectations. Consequently, a downward drift in long-run inflation expectations of even 0.1 pp results in lowering the interest rate by \movei pp. 

This is a large response. Clearly, optimal policy involves subduing unanchored expectations by injecting massive negative feedback to the system. One may then wonder why optimal policy is so aggressive on unanchored expectations when the analysis of the target criterion in Section \ref{ramsey} suggested that learning can alleviate the stabilization tradeoff between output and inflation. 

The reason is that the anchoring expectation formation introduces another intertemporal tradeoff to monetary policy: a volatility tradeoff. If expected mean inflation is anchored at the central bank's target, zero, then even for shocks hitting the economy, the monetary authority need not intervene much because stable expectations lower the pass-through between shocks and observables. However, sufficiently large shocks can unanchor expectations, enabling them to act as a positive feedback loop between shocks and observables. Having unanchored expectations, then, comes at a volatility cost in the central bank's target variables. This volatility cost dictates that, in the long run, the central bank wishes to have expectations anchored. 

From the viewpoint of the central bank, this problem is amplified by the fact that anchoring expectations itself comes at a convex volatility cost. Anchoring expectations requires an aggressive interest response because by this means, the central bank can introduce negative feedback to the system. A strong interest rate response initially increases forecast errors before it lowers them. Taken together with the fact that unanchored expectations involve a large gain, this leads to sizable forecast errors oscillating with a high amplitude.\footnote{As periodically noted in the adaptive learning literature, constant gain learning models have a tendency to produce impulse responses that exhibit damped oscillations. Authors making explicit note of this phenomenon include \cite{evans_honkapohja2001}, \cite{evans2013bayesian} and \cite{anufriev2012evolutionary}. The reason is that under an adaptive learning framework, forecast errors following an impulse are oscillatory.  In fact, the higher the learning gain, the higher the amplitude of forecast error oscillations. App. \ref{app_oscillations} presents a simple illustration for why this is the case.} Thus expectations fluctuate vastly, and observables become volatile.\footnote{App. \ref{app_IRFs} demonstrates the volatility cost of having unanchored expectations as well as of getting expectations anchored by plotting impulse responses of the model following a monetary policy shock. It also discusses the relationship between the model's oscillatory impulse responses and \cite{ball1994credible}'s proposition of expansionary disinflations in NK models.} 

We can see from the policy function how optimal policy resolves this tradeoff: it reacts extremely aggressively to forecast errors. This way, the central bank hopes to avoid even larger interventions that would become necessary were expectations to unanchor further. To avoid having to pay so high a price, the central bank is extra aggressive in the short run to prevent massive unanchoring from ever materializing. Thus the optimal response to the volatility tradeoff is to temporarily increase volatility in order to reduce it in the long-run. Thus the central bank's aggressiveness in the model is driven by the desire to prevent downward (upward) drifting long-run expectations from becoming a self-fulfilling deflationary (inflationary) spiral. In practice, it is likely that there is a small amount of movement in long-run expectations that the central bank is willing to tolerate. 

The presence of the volatility tradeoff also implies that optimal policy aggressiveness is yet again time-varying: the same shock involves a stronger interest rate response if expectations threaten to unanchor than otherwise. To see this, consider another way to investigate optimal policy in the model. Fig. \ref{pea_TCvsTR} compares the evolution of observables conditional on a particular history of exogenous disturbances across two specifications of monetary policy: one that follows the target criterion in (\ref{target}) and one that follows a Taylor-rule with parameter values given in Table \ref{calibration}.\footnote{Given that in my calibration the central bank does not attach any weight to interest rate stabilization ($\lambda_i=0$), this choice of Taylor-rule coefficients would not be optimal in the RE version of the model, since $\lambda_i=0$ implies infinity as the optimal choice for both $\psi_{\pi}$ and $\psi_x$. My choice of $\lambda_i=0$ is motivated by keeping the interpretation as simple as possible.}

\begin{figure}[h!]
\subfigure[Taylor rule in effect and common knowledge]{\includegraphics[scale=0.29]{\myFigPath \fignamePEAobsTR}}
\hfill % this is great to intro space between subfigures
\subfigure[Optimal policy]{\includegraphics[scale=0.29]{\myFigPath \fignamePEAobsAnch}}
\caption{Evolution of observables for policy following a Taylor rule against optimal policy}
\label{pea_TCvsTR}
\end{figure}



Panels (a) and (b) show the outcome achieved by adhering to a publicly announced and internalized Taylor rule and the target criterion (\ref{target}) respectively.\footnote{If the private sector does not know that a Taylor rule is in effect, the model displays explosive dynamics. The reason is that since in this specification only the inflation intercept is learned, absent knowledge of the Taylor rule, the private sector's beliefs would not span the set of model-consistent ones. Therefore the figure displays a specification with a Taylor rule that is assumed to be known by the public.} As opposed to the Taylor-rule specification, optimal policy uses the interest rate tool much more aggressively because it responds to unanchored expectations. This way, it subdues inflation and output gap volatility simultaneously because it brings inflation expectations under control. The central bank is willing to raise the interest rate massively in order to eliminate any potential of large-scale unanchoring. When expectations become anchored, the interest rate can retreat to zero. However, the central bank remains ever alert to change it  again if it sees a threat of unanchoring.\footnote{As prescribed by the optimal policy function, the interest rate tracks long-run expectations of inflation, scaled up. As discussed in Footnote \ref{footnote_why_nonstationary}, this feature is specific to the special case of the model where only inflation is learned.}

% Comparisons to a TR:
Clearly, the main drawback of the Taylor rule is that it does not take the evolution of expectations into account. Thus it misses the opportunity offered by the target criterion in Equation (\ref{target}) to smooth out the inflation-output tradeoff, and it also results in higher overall economic volatility than optimal policy does. The key intuition is that the main driver of volatility in the model is the positive feedback loop between expectations and outcomes. By responding aggressively to long-run expectations, optimal policy can dampen the positive feedback without itself causing excess volatility when there is no threat of unanchoring. Section \ref{welfare} quantifies the welfare loss that results from using Taylor rule instead of the optimal policy rule. 

  
%%%%%%%%%%%%%%%%%%           OPTIMAL TAYLOR RULE           %%%%%%%%%%%%%%%%%% 
\subsection{Optimal Taylor rule under anchoring}\label{opt_TR}
Monetary policy is often formulated using a Taylor rule. Proponents of such a characterization, like \cite{taylor1993discretion} himself, emphasize the benefits of having a simple, time-invariant and easily verifiable rule. Also in the anchoring model, a policymaker may thus be interested in using a Taylor-type approximation to optimal policy in order to combine the benefits of having a simple, yet near-optimal rule.\footnote{Recall from \cite{woodford2011interest} that even under rational expectations, a Taylor rule is not fully optimal. The reason is that the optimal commitment rational expectations version of the targeting rule (\ref{target}) takes the form of (\ref{cgg_discretion}) with an additional $x_{t-1}$ term. This lagged term renders the solution history-dependent, which in turn allows the central bank to reap the benefits of commitment. Therefore, due to its purely forward-looking nature, a Taylor-type interest rate rule is only optimal in the restricted set of fully forward-looking policy rules. Since Result \ref{result_no_commitment} tells us that in the anchoring model there is no distinction between commitment and discretion, this point may be less relevant here than for rational expectations.} Therefore I now consider the restricted set of Taylor-type policy rules and ask what value of the time-invariant Taylor-rule coefficient on inflation is optimal in the case of the anchoring model.\footnote{To simplify the exposition, I continue entertaining a Taylor-coefficient on the output gap of zero.}

I compute the optimal Taylor rule coefficient on inflation numerically by minimizing the central bank's expected loss in a cross-section of $N=100$ simulations of both the rational expectations and learning versions of the model. I continue to use the calibration of Table \ref{calibration} and to parameterize the anchoring function using the estimated $\hat{\alpha}$ from Section \ref{estimation}.

Table \ref{psi_pi_opt} presents an overview of the optimal Taylor rule coefficient $\psi_{\pi}$ for the rational expectations and anchoring models. The table also compares the baseline parameterization with several alternatives. One notices that if the central bank has no concern to stabilize the output gap ($\lambda_x = 0$) or the nominal interest rate ($\lambda_i =0$), $\psi_{\pi}$ is infinity for both the RE and anchoring models. This is because if the central bank suffers no loss upon output variation, then the fact that the divine coincidence is violated does not pose a problem. Similarly, if the monetary authority is willing to allow the nominal interest rate to fluctuate vastly in order to stabilize inflation, this also allows the central bank to be infinitely aggressive on inflation. 

The main observation however is that in terms of optimal Taylor-coefficient on inflation, the distinction between the RE and anchoring models is the lack in the case of the anchoring model to control the stance of expectations. This manifests itself in differences the speed with which optimal inflation aggressiveness drops to 1 as one increases the weights on the output gap or interest rate volatility. Increasing $\lambda_x$ pushes $\psi_{\pi}^*$ to 1 faster in the anchoring model because high aggressiveness leads to high volatility when the central bank cannot bring fluctuations in long-run expectation down.\footnote{See App. \ref{app_oscillations} for impulse responses of the anchoring model illustrating the relationship between $\psi_{\pi}$ and volatility.} Increasing $\lambda_i$, on the other hand, pushes $\psi_{\pi}^*$ to 1 slower in the anchoring model because the cost of interest rate fluctuations is balanced by the benefit of lower fluctuations when expectations are anchored.

The message is that if policy is restricted to be time-invariant in the sense of always responding to inflation by the same factor, then there is nothing that policy can do to take the anchoring expectation formation into account. This is reinforced in Fig. \ref{fig_loss} which plots the central bank's loss in the RE and anchoring models as a function of $\psi_{\pi}$ for various values of $\lambda_x$ and $\lambda_i$. The figure makes clear that while the central bank's loss has the same first derivatives across the two models, the loss in the anchoring model always exceeds that under rational expectations. As discussed in Section \ref{implement}, this is because the central bank adhering to a Taylor rule permits more volatility in the anchoring model than under RE. 


\begin{center}
\begin{table}[h!]
\begin{tabular}{ c | c | c }
 & $\psi^{*,RE}_{\pi}$ & $\psi^{*,Anchoring}_{\pi}$  \\  \hline
  $\lambda_x =0, \lambda_i =0 $     & $\infty$  & $1.5262$ \\  \hline
 Baseline ($\lambda_x =0.05, \lambda_i =0$)  & $2.2098 $  & 1.0894\\  \hline
 $\lambda_x =1, \lambda_i =0 $ & 1 & 1.0088 \\  \hline
 $\lambda_x =0, \lambda_i =1 $ &  1  & 1.0757 \\  \hline
  $\lambda_x =1, \lambda_i =1 $ &  1 & 1.018 \\  \hline
\end{tabular}     
      \caption{Optimal coefficient on inflation, RE against anchoring for alternative parameters}  
      \label{psi_pi_opt}
 \end{table}
\end{center}



%\vspace{-0.8cm}


\begin{figure}[h!]
\subfigure[$\lambda_x = 0, \lambda_i = 0$]{\includegraphics[scale = 0.2]{\myFigPath \fignameCBlossnilnil}}
\hfill
\subfigure[$\lambda_x = 1, \lambda_i = 0$]{\includegraphics[scale = 0.2]{\myFigPath \fignameCBlossonenil}}
\subfigure[$\lambda_x = 0, \lambda_i = 1$]{\includegraphics[scale = 0.2]{\myFigPath \fignameCBlossnilone}}
\hfill
\subfigure[$\lambda_x = 1, \lambda_i = 1$]{\includegraphics[scale = 0.2]{\myFigPath \fignameCBlossoneone}}
\caption{Central bank loss as a function of $\psi_{\pi}$}
\floatfoot{Sample length is $T=400$ with a cross-section of $N=100$.}
\label{fig_loss}
\end{figure}

%\clearpage

%\vspace{-1.2cm}


It is also interesting to note that, unless $\lambda_x =  \lambda_i = 0$, loss is increasing and convex in $\psi_{\pi}$ for the rational expectations model. For the anchoring model, however, the loss becomes concave if $\lambda_x=1$. This is a reflection of the fact that, as we saw in Fig. \ref{pea_TCvsTR}, a Taylor rule in the anchoring model permits much higher output gap volatility than optimal policy does. For this reason, the more the central bank suffers disutility from output gap fluctuations, the more optimal policy would react strongly to long-run expectations. Being restricted not to do so under a Taylor rule, monetary policy incurs higher losses that can be dampened somewhat by raising $\psi_{\pi}$. This renders the loss concave. 

Thus, if the central bank's reaction function is restricted to a Taylor rule in the anchoring model, the monetary authority always engineers higher losses than under RE. Nonetheless, it can still choose an inflation-coefficient that would improve on the RE-optimal choice of $\psi_{\pi}$,. This is, as shown in Sections \ref{ramsey} and \ref{implement}, because optimal policy is time-varying in order to take the stance of anchoring into account. Thus it seems reasonable to expect that if the central bank were allowed to select time-varying Taylor-rule coefficients, it would choose low coefficients when expectations are anchored and high ones when expectations show signs of unanchoring. Such a central bank, as well as one following the optimal policy would appear to the econometrician as time-varying, echoing the findings of \cite{LUBIK201685}.


%%%%%%%%%%%%%%%%%%           WELFARE LOSSES            %%%%%%%%%%%%%%%%%% 
\subsection{Welfare gains from optimal policy}\label{welfare}

As depicted in the previous section, employing a Taylor rule as a monetary policy specification is much costlier in terms of welfare in the anchoring model than under rational expectations. Here I therefore turn to the question by how much welfare can be improved by implementing the optimal policy under anchoring instead of an optimal Taylor rule.\footnote{Here I use the optimal Taylor-rule coefficient for my baseline specification in Table \ref{psi_pi_opt}.} As an alternative, I also consider a Taylor rule with a 1.5 and 0 inflation and output gap coefficient, respectively. I continue to use the calibrated parameters from Table \ref{calibration} and $\hat{\alpha}$ from the estimation in Section \ref{estimation}.


% command_welfare.m: evaluate the loss for optimal policy
\begin{center}
\begin{table}[h!]
\begin{tabular}{ c | c | c }
 & RE & Anchoring \\  \hline
  Taylor rule with $\psi_{\pi} =1.5, \psi_{x} = 0.3$     & 5.3805  & 8.3714 \\  \hline
 RE-optimal* Taylor rule with $\psi_{\pi} =2.2098, \psi_{x} = 0.3$   &5.3148 &  13.6380 \\  \hline
 Anchoring-optimal* Taylor rule with $\psi_{\pi} =1.0894, \psi_{x} = 0.3$   & 5.4868  &  6.1446 \\  \hline
% Optimal policy & na.  & $\approx 19.3928$ \footnote{An estimate based on a single evaluation of the loss function for a PEA-optimal simulation.} \\  \hline  % For Calibration C, the estimate in grid_search_approx.m is 7, so I don't want to speak about it. command_welfare.m gives you 11.8336, so I don't wanna speak about it either. Also that took 19 min, whereas the PEA algorithm takes about 1 min! WTF?
\end{tabular}     
      \caption{Loss, RE against anchoring for alternative specifications of monetary policy}  
      \floatfoot{* ``$x$-optimal'' refers to a Taylor rule optimal under model $x$, rational expectations or anchoring, for baseline parameters.}
      \label{table_welfare}
 \end{table}
\end{center}

On average, using the same Taylor-rule specification incurs 1.75 times higher losses under the anchoring model than under rational expectations. This corroborates the findings of Section \ref{opt_TR} that suggested that using a Taylor rule is in general costlier under anchoring than under RE. If a policymaker thus opts to conduct monetary policy via a Taylor rule, the coefficient on inflation should be chosen in accordance with the true model of expectation formation. In particular, it is interesting to compare the cases where the policymaker chooses the anchoring-optimal coefficient while the true model is RE and vice versa. The welfare loss in these two cases is given by the numbers 5.4868 (the policymaker chooses the coefficient that would be optimal in the anchoring model, but the true model is RE) and 13.6380 (the policymaker chooses the RE-optimal coefficient but the true model is one of anchoring). What is the welfare cost of such a mistake?

Referring to these two cases as Case 1 and Case 2 respectively, we can see that the mistake is not too costly in Case 1, that is, if the policymaker mistakenly picks the anchoring-optimal coefficient. In this case, the welfare loss 1.0324 times higher than it would be had the policymaker chosen the RE-optimal coefficient.  In Case 2, however, the welfare loss is 2.2195 times higher than it would have been for the model-appropriate optimal inflation-aggressiveness. In other words, a policymaker committed to the Taylor rule inflicts greater relative welfare losses if he mistakenly believes the model of expectation formation to be RE than the other way around. 

The interpretation of this is that if considerations of simplicity and transparency lead the central bank to choose the Taylor rule as specification for monetary policy, then it is less costly in terms of welfare to assume that the underlying model is anchoring when choosing the optimal Taylor-rule coefficient on inflation. Moreover, given that my estimates of the anchoring function that provide substantial evidence that short-run expectation formation is characterized by anchoring, this implies that US monetary policy could achieve a decrease in loss of more than 50\% by simply switching to the anchoring-optimal Taylor-rule coefficient. %As the last row of Table \ref{table_welfare} depicts, even more substantial gains could be reached by switching to the fully optimal policy.


%%%%%%%%%%%%%%%%%%           DISCUSSION OF RESULTS           %%%%%%%%%%%%%%%%%% 
\subsection{Discussion}\label{discussion_results}

The analytical and numerical analysis of monetary policy in the anchoring model has led to the following conclusions. As seen in Result \ref{result_no_commitment}, optimal monetary policy is time-consistent and does not exhibit history-dependence. Therefore, optimal responses to shocks feature intertemporal stabilization tradeoffs that allow the central bank to postpone the intratemporal tradeoff between inflation and output gap stabilization to the future. Ironically, although the first-order conditions from the Ramsey problem have the flavor of discretion due to the absence of lagged multipliers, the optimal response to shocks prescribed by the target criterion (\ref{target}) qualitatively resembles commitment as it spreads out the effect of shocks over time. 

The implementation of the target criterion calls for an interest rate policy function that is very responsive to the stance of expected mean inflation. As we saw in Section \ref{implement}, an upward unanchoring of expectations that involves a 0.1 pp increase in long-run inflation expectations induces the central bank to raise the interest rate by \movei pp. The simulation in Section \ref{implement} also shows that, accordingly, the optimal interest rate path is more volatile than a path generated by a standard Taylor rule, especially early in the sample. This comes from optimal policy responding aggressively to any sign of expectations threatening to become unanchored; a phenomenon that is especially likely early in the learning process.

At the same time, as seen in Section \ref{opt_TR}, restricting the central bank to follow a Taylor rule and choosing its response coefficients to minimize the central bank's loss function does not deliver a good approximation to optimal policy. This reflects that the time-varying nature of optimal policy allows the central bank to appropriately deal with the intertemporal volatility tradeoff.  A policymaker believing the economy to be well-described by rational expectations inflicts much larger welfare losses under anchoring than under RE. 

Taken together, these results suggest that optimal policy aggressiveness should be time-varying: the central bank should condition its tolerance of inflation on the stance of anchoring. The policy function of Section \ref{implement} captures this notion as it expresses the interest rate choice as a function of expected mean inflation. If a central bank is reluctant to employ a policy function other than a Taylor rule, then my analysis underscores the conclusion of \cite{LUBIK201685} that the optimal Taylor-rule coefficients should be time-varying. Alternatively, a term capturing the stance of anchoring - the gain or the private sector's expected mean inflation - could be included in the Taylor rule. This allows the central bank's actions to reflect the stance of anchoring in the way the optimal policy rule of Section \ref{implement} prescribes. 



 %%%%%%%%%%%%%%%%%%           CONCLUSION            %%%%%%%%%%%%%%%%%% 
%\newpage
\section{Conclusion}\label{conclusion}
Central bankers frequently voice a concern to anchor expectations. Absent a behavioral theory of anchored expectations, it is difficult to understand how such a concern affects the conduct of monetary policy. In this paper, I have laid out a simple theory of anchoring expectation formation based on adaptive learning models with an endogenous gain. The estimation of the anchoring function and the analysis of the Ramsey policy through analytical and numerical methods yields a number of differences compared to the rational expectations version of the model. 

The main message is that policy is time-varying because it responds to the unanchoring of expectations. The estimation shows that mistakes in the inflation forecast of about \ppFEunanchors pp begin to unanchor expectations. In this case, policy responds to changes in expected mean inflation by raising/lowering the interest rate by \movei pp, depending on the sign of the forecast error.

Optimal policy in the anchoring model is time-consistent, for which reason it can be expressed as a purely forward-looking target criterion for the policymaker to follow. The target criterion prescribes how the policymaker can exploit the learning mechanism to spread out the effect of shocks over time, adding an intertemporal channel to the tradeoff between inflation and the output gap. The sensitivity of the policy function to the stance of anchoring highlights, however, that there is no free lunch: the monetary authority needs to dampen volatility at the same time as guard against the possibility of unanchoring expectations. Thus optimal policy faces an intertemporal volatility tradeoff in which long-run stability can only be achieved at the cost of short-run volatility.

Furthermore, even small changes to the Fed's policy framework yield significant welfare improvements. In particular, adopting an anchoring-optimal Taylor rule reduces volatility costs by more than 50\%. Thus, my model corroborates Jerome Powell's assessment in the Introduction that ensuring that expectations are well anchored should indeed be ``[at] the heart'' of the Fed's policy review.

A number of interesting questions emerge from the analysis of monetary policy and the anchoring expectation formation. One may wonder whether the central bank can use tools other than its leading interest rate to anchor expectations. Especially concerns around a binding zero lower bound would motivate the use of alternative monetary policy tools. Thus the interaction between anchoring and central bank communication, in particular forward guidance, would be worthwhile to examine. An interesting avenue for future research, then, would be to make explicit the communication policy of the central bank to investigate whether anchoring expectation formation could help to resolve the forward guidance puzzle. 

This, however, requires overcoming the implication of Result \ref{result_no_commitment} that adaptive expectations are not able to incorporate any information that is not embedded in the current state vector. One option is to model central bank communication similarly to news shocks in the sense of \cite{beaudry2006stock}. In this case, the anchoring model is likely to deliver differing predictions regarding the effectiveness of Delphic versus Odyssean forward guidance (\cite{campbell2012macroeconomic}) because sharing the central bank's forecasts would not constitute a questioning of the interest rate reaction function, while committing to a future interest rate path would.

In general, extensions to the anchoring expectation formation proposed here would be of interest. The choice of the gain could be endogenized using approaches that allow the private sector to choose its forecasting behavior in an optimizing fashion, perhaps by selecting among competing forecasting models as in \cite{Branch2011} or by choosing the size of the gain to minimize the estimated forecast error variance. 

Lastly, a foray into the empirics of anchoring expectation formation is important to get a clearer idea of the expectation formation process of the public. In practice, it is likely that this process is heterogenous, not just across households and firms, but also within various demographic groups or sectors of the economy. If so, then monetary policy would need to collect and monitor a host of long-run expectations time series to manage the challenge of anchoring expectations. 



 %%%%%%%%%%%%%%%%%%           BIBLIOGRAPHY            %%%%%%%%%%%%%%%%%% 
\clearpage
\newpage
\bibliographystyle{chicago}
\bibliography{\myBibPath ref_next}
%\nocite{*} % uncomment if you wanna include everything that's in the references

 %%%%%%%%%%%%%%%%%%           APPENDIX            %%%%%%%%%%%%%%%%%% 
\newpage
\appendix

% the following command makes equation numbering include the section first, but just for what follows
\numberwithin{equation}{section}

\section{Filtering out liquidity risk from TIPS}\label{TIPS} 
Some caution is needed when inferring inflation expectations from TIPS. The underlying idea is that the difference between real (indexed to inflation) and nominal yields should be a good metric for the market's expectations of inflation on average for the duration of the particular maturity. But since TIPS markets face liquidity issues, especially for seasoned securities, the TIPS yield also incorporates a liquidity premium. The positive bias in the TIPS yield thus leads to a negative bias in expected inflation.

To gauge the presence of liquidity risk in TIPS, I rely on \cite{andreasen2018tips}'s estimation of the liquidity premium. Since their series only covers the period between July 11,1997 - Dec 27, 2013, I make use of the fact that they demonstrate a high correlation between liquidity risk and uncertainty. In particular, a regression of their average TIPS liquidity premium measure on the VIX index and controls yields an estimated coefficient of 0.85 (significant at the 1 percent level), with a constant of -5.21. Thus I can use the VIX to back out a fitted \cite{andreasen2018tips} estimate of the TIPS liquidity premium after 2013. Doing so, I subtract this fitted liquidity premium series from the TIPS yields, allowing me to construct an estimate of breakeven inflation corrected for the liquidity risk bias. Fig. \ref{epi_cleaned} presents the original breakeven inflation series, along with the bias-corrected estimate.

\begin{figure}[h!]
\includegraphics[scale = \myFigScale]{\myFigPath \fignameMarketEPiCleaned} % \fignameMarketEPiCleaned
\caption{Market-based inflation expectations, 10 year, average, \%}
\floatfoot{Breakeven inflation, constructed as the difference between the yields of 10-year Treasuries and 10-year TIPS (blue line), difference between 10-year Treasury and 10-year TIPS, the latter cleaned from liquidity risk (red line).}
\label{epi_cleaned}
\end{figure}

In line with \cite{andreasen2018tips}'s findings, I obtain a negative bias in the inflation expectations series throughout the sample. This bias averages -0.0966 pp, which is sizable, but not as significant as one might have suspected. Also in analogy with \cite{andreasen2018tips}'s results, I find that liquidity issues in the TIPS market pose a bigger problem in recessions, when the market worries more about future TIPS becoming illiquid. In particular once the COVID-shock raises worries at the end of the sample, the estimated liquidity premium hits its highest value of 0.6508 pp. Even this large upward correction in breakeven inflation does not change the overall picture, however. The conclusion that long-run inflation expectations trend downward in the second half of the decade remains solid. 

\section{Compact model notation}\label{app_compact} 
The A-matrices are given by
\begin{align}
A_a & = \begin{pmatrix} g_{\pi a} \\ g_{x a} \\ \psi_{\pi}g_{\pi a} + \psi_xg_{x a}
\end{pmatrix}
\quad A_b = \begin{pmatrix} g_{\pi b} \\ g_{x b} \\ \psi_{\pi}g_{\pi b} + \psi_xg_{x b}
\end{pmatrix}
 \quad A_s = \begin{pmatrix} g_{\pi s} \\ g_{x s} \\ \psi_{\pi}g_{\pi s} + \psi_xg_{x s} + \begin{bmatrix} 0 & 1& 0\end{bmatrix}
\end{pmatrix} \\
g_{\pi a} & =(1-\frac{\kappa\sigma\psi_{\pi}}{w} )  \begin{bmatrix}(1-\alpha)\beta, \kappa\alpha\beta, 0 \end{bmatrix} \\
g_{x a} & =  \frac{-\sigma\psi_{\pi}}{w} \begin{bmatrix}(1-\alpha)\beta, \kappa\alpha\beta, 0 \end{bmatrix}\\
g_{\pi b} & = \frac{\kappa}{w} \begin{bmatrix}\sigma(1-\beta\psi_{\pi}), (1-\beta-\beta\sigma\psi_x, 0 \end{bmatrix}\\
g_{x b} & = \frac{1}{w} \begin{bmatrix}\sigma(1-\beta\psi_{\pi}), (1-\beta-\beta\sigma\psi_x, 0 \end{bmatrix} \\
g_{\pi s} & = (1-\frac{\kappa\sigma\psi_{\pi}}{w} )\begin{bmatrix} 0&0&1 \end{bmatrix} (I_3 - \alpha\beta h)^{-1} -\frac{\kappa\sigma}{w}\begin{bmatrix} -1&1&0 \end{bmatrix} (I_3 -\beta h)^{-1}\\
g_{x s} & =  \frac{-\sigma\psi_{\pi}}{w} \begin{bmatrix} 0&0&1 \end{bmatrix}(I_3 - \alpha\beta h)^{-1}  -\frac{\sigma}{w}\begin{bmatrix} -1&1&0 \end{bmatrix}(I_3 -\beta h)^{-1}\\
w & = 1+\sigma\psi_x +\kappa\sigma\psi_{\pi}
\end{align}
The matrices of the state transition equation (\ref{LOM_s}) are
 \begin{align}
 h  & \equiv \begin{pmatrix} \rho_r & 0 & 0 \\ 0& \rho_i & 0 \\ 0&0& \rho_u 
 \end{pmatrix}  \quad 
 \epsilon_t \equiv \begin{pmatrix}\varepsilon_t^{r} \\ \varepsilon_t^{i}  \\ \varepsilon_t^{u} 
 \end{pmatrix}  \quad  \text{and } \quad \eta  \equiv \begin{pmatrix} \sigma_r & 0 & 0 \\ 0& \sigma_i & 0 \\ 0&0& \sigma_u 
 \end{pmatrix} 
 \end{align}
Note that this is the formulation for the case where a Taylor rule is in effect and is known by the private sector. It is straightforward to remove any of these two assumptions.



\section{The observation matrix for learning}\label{app_FG}
Instead of the matrix $g$ in the rational expectations observation equation
\begin{equation}
z_t = g s_t \label{RE_obs}
\end{equation}
agents in the anchoring model use the estimated matrix $g^l$
\begin{equation}
g_{t-1}^l = \begin{bmatrix} F_{t-1} & G_{t-1} \end{bmatrix}
\end{equation}
with
\begin{align}
F_{t-1} & = \bigg(A_a \frac{1}{1-\alpha\beta} + A_b\frac{1}{1-\beta} \bigg)a_{t-1}\\
G_{t-1} & = A_a b_{t-1}\bigg(I_3 - \alpha\beta h \bigg)^{-1} + A_b b_{t-1}\bigg(I_3 - \beta h \bigg)^{-1} + A_s
\end{align}

\section{Alternative specifications for the anchoring function}\label{alternative_criteria}
The general law of motion for the gain in the main text is given by Equation (\ref{anchoring}), reproduced here for convenience:
\begin{equation}
k_t  = \mathbf{g}(k_{t-1},fe_{t|t-1}) 
\end{equation}

The baseline specification of the anchoring function $\mathbf{g}$ (Equation (\ref{gain}) in the main text) is \begin{equation}
\mathbf{g} = \alpha b(fe_{t|t-1})
\end{equation}
where $ b(fe_{t|t-1})$ is the second order spline basis and $\hat{\alpha}$ are the coefficients estimated in Section \ref{estimation}.

To my knowledge, there are only two other papers that consider an endogenous gain as a model for anchored expectations. The first one, more related to this paper, is \cite{carvalho2019anchored}. In their model, the anchoring function is a discrete choice function as follows. Let $\theta_t$ be a criterion to be defined. Then, for a threshold value $\tilde{\theta}$, the gain evolves according to
\begin{align*}
k_t & = \begin{cases} (k_{t-1}+1)^{-1} \quad \text{if} \quad \theta_t < \tilde{\theta}  \\ \bar{g}  \quad \text{otherwise.}\numberthis \label{anchoring_kinked}
\end{cases} 
\end{align*}
In other words, agents choose a decreasing gain when the criterion $\theta_t$ is lower than the threshold $\tilde{\theta}$; otherwise they choose a constant gain. The criterion employed by \cite{carvalho2019anchored} is computed as the absolute difference between subjective and model-consistent expectations, scaled by the variance of shocks:
\begin{equation}
\theta_t = \max | \Sigma^{-1} ( \phi_{t-1} - \begin{bmatrix} F_{t-1} & G_{t-1} \end{bmatrix}) |
\end{equation}
where $\Sigma$ is the VC matrix of shocks, $\phi_{t-1}$ is the estimated matrix and $[F,G]$ is the ALM (see App. \ref{app_FG}).

As a robustness check, \cite{carvalho2019anchored} also compute an alternative criterion.\footnote{Note that for both criteria, I present the matrix generalizations of the scalar versions considered by \cite{carvalho2019anchored}.} Let $\omega_t$ denote agents' time $t$ estimate of the forecast error variance and $\theta_t$ be a statistic evaluated by agents in every period as
\begin{align}
\omega_t & =  \omega_{t-1} + \tilde{\kappa} k_{t-1}(fe_{t|t-1} fe_{t|t-1}'  -\omega_{t-1})\\
\theta_t & =  \theta_{t-1} + \tilde{\kappa} k_{t-1}(fe_{t|t-1}'\omega_t^{-1}fe_{t|t-1} -\theta_{t-1}) \label{cusum_crit}
\end{align}
where $\tilde{\kappa}$ is a parameter that allows agents to scale the gain compared to the previous estimation and $fe_{t|t-1}$ is the most recent forecast error, realized at time $t$. Indeed, this is a multivariate time series version of the squared CUSUM test.\footnote{See \cite{brown1975techniques} and \cite{lutkepohl2013introduction} for details.}

It is interesting to compare the two discrete anchoring functions with my smooth specification. On the one hand, the \cite{carvalho2019anchored}'s preferred specification requires the private sector to evaluate model-consistent expectations, which runs counter to the maintained informational assumptions. It is more consistent with the present model, then, to assume that firms and households employ a statistical test of structural change, as is the case with the CUSUM-based and smooth functions. These are therefore more appealing on conceptual grounds.

On the other hand, simulation of the model using the different anchoring specifications reveals that \cite{carvalho2019anchored}'s preferred functional form leads to the opposite comparative statics of anchoring with respect to monetary policy aggressiveness as the smooth or the CUSUM-based specifications. In particular, while policy that is more aggressive on inflation (a higher $\psi_{\pi}$ in the Taylor rule) leads to more anchoring in a model with the smooth or the CUSUM-inspired criterion. If one uses \cite{carvalho2019anchored}'s criterion, the same comparative static involves \emph{less} anchoring. This comes from the fact that \cite{carvalho2019anchored}'s criterion endows the private sector with capabilities to disentangle volatility due to the learning mechanism from that owing to exogenous disturbances. Thus agents in the \cite{carvalho2019anchored} model are able to make more advanced inferences about the performance of their forecasting rule and understand that a higher $\psi_{\pi}$ causes more learning-induced volatility. This is however not possible for agents who process data in real time without knowledge of the model. Therefore my smooth and the discrete CUSUM-inspired specifications are preferable both on conceptual and quantitative grounds.

Whether one relies on the discrete CUSUM or my smooth anchoring function, then, is a question of application. The analytical analysis of this paper requires derivatives of the anchoring function to exist. In such a case, the smooth specification is necessary. The discrete choice specification is easier to work with when computing moments conditional whether expectations are anchored or not because it only requires keeping track of two discrete cases.

The second paper with an anchoring function is \cite{gobbi2019monetary}. In their model, which is a three-equation New Keynesian model, firms and households entertain the possibility that the model may switch from a ``normal'' regime to a liquidity trap regime that the authors name the ``new normal.'' Expectations are a probability-weighted average of the regime-specific expectations. The concept of unanchoring in the model is when $p$, the probability of the liquidity trap regime, rises significantly. The function governing the evolution of $p$, which \cite{gobbi2019monetary} refer to as the deanchoring function (DA), is the analogy to my anchoring function. The authors use the following logistic specification for the DA function:

\begin{equation}
 p = h(y_{t-1}) = A + \frac{B C e^{-D y_{t-1}}}{( C e^{-D y_{t-1}}+1)^2}
\end{equation}
where $y_{t-1}$ denotes the output gap and $A,B,C$ and $D$ are parameters.

\section{Estimation procedure}\label{SMM}
The estimation of Section \ref{estimation} is a simulated method of moments (SMM) exercise. As elaborated in the main text, I target the autocovariances of CPI inflation, the output gap, the federal funds rate and the 12-months ahead inflation forecasts coming from the Survey of Professional Forecasters. For the autocovariances, I consider lags $0, \dots, 4$. The target moment vector, $\Omega$, is the vectorized autocovariance matrices for the lags considered, $80\times1$. 

For each proposed coefficient vector $\alpha$, the estimation procedure consists of simulating the model conditional on $\alpha$, the calibrated parameters $\theta$ and $N$ different sequences of disturbances, computing model-implied moments for each simulation, and lastly choosing $\alpha$ such that the squared distance between the data- and model-implied mean moments is minimized. Thus

\begin{equation}
\hat{\alpha} = \bigg(\Omega^{data}-\frac{1}{N}\sum_{n=1}^N\Omega^{model}(\alpha, \theta, \{e^n_t\}_{t=1}^{T})\bigg)' W^{-1} \bigg(\Omega^{data}-\frac{1}{N}\sum_{n=1}^N\Omega^{model}(\alpha, \theta,  \{e^n_t\}_{t=1}^{T})\bigg)
\end{equation}

where the observed data is of length $T=155$ quarters. Here $\{e^n_t\}_{t=1}^{T}$ is a sequence of disturbances of the same length as the data; note that I use a cross-section of $N$ such sequences and take average moments across the cross-section to wash out the effects of particular disturbances. Experimentation with the number $N$ led me to choose $N=1000$, as estimates no longer change upon selecting larger $N$.

Before computing moments, I filter both the observed and model-generated data using the \cite{baxter1999measuring} filter, with thresholds at 6 and 32 quarters and truncation at 12 lags, the recommended values of the authors. As the weighting matrix of the quadratic form in the moments, I use the inverse of the estimated variances of the target moments, $W^{-1}$, computed from 10000 bootstrapped samples.  

To improve identification, I also impose restrictions on the estimates. First, I require that the $\alpha$-coefficients be convex, that is, that larger forecast errors in absolute value be associated with higher gains. Second, since forecast errors close to zero render the size of the gain irrelevant (cf. the learning equation (\ref{RLS})), I impose that the coefficient associated with a zero forecast error should be zero. Both restrictions are implemented with weights penalizing the loss function, and the weights are selected by experimentation. 

Both additional assumptions reflect properties that it is reasonable to expect the anchoring function to have. The convexity assumption captures the very notion that larger forecast errors in absolute value suggest bigger changes to the forecasting procedure are necessary. This is thus a very natural requirement. As for the zero gain for zero forecast error assumption, the idea here is to supply the estimation with information where it is lacking. Since the updating of learning coefficients corresponds to gain times forecast error, as Equation (\ref{RLS}) recalls, a zero forecast error supplies no information for the value of the gain. To impose a zero value here also seems natural, given that since forecast errors switch sign at zero, one would expect the zero forecast error point to be an inflection point in the anchoring function. By the same token, \cite{gobbi2019monetary} also impose a related restriction when they require that their deanchoring function should yield a zero value at the zero input. Lastly, the objective function does not deteriorate upon imposing either assumption, suggesting that they are not at odds with the data.

Fig. \ref{autocovariogram} presents the autocovariances of the observed variables for the estimated coefficients. 


\begin{figure}[h!]
\includegraphics[scale=0.35]{\myFigPath \fignameAutocov}
\caption{Autocovariogram}
\label{autocovariogram}
\floatfoot{Estimates for $N=1000$, imposing convexity with weight 100K}
\end{figure}

\section{The policy problem in the simplified baseline model }\label{app_midsimple_problem}
Denote by $\mathbf{g}_{i,t} \in (0,1), \; i=\pi, \bar{\pi}$, the potentially time-varying derivatives of the anchoring function $\mathbf{g}$. In this simplified setting, $\bar{\pi}_t = e_1 a_t$, the estimated constant for the inflation process. $e_i$ is a selector vector, selecting row $i$ of the subsequent matrix. I also use the notation $b_i \equiv e_i b$.   The planner chooses $\{\pi_t, x_t, f_{a,t},  f_{b,t}, \bar{\pi}_t, k_t\}_{t=t_0}^{\infty}$ to minimize

 \begin{align}
\mathcal{L} &= \E_{t_0}\sum_{t=t_0}^{\infty} \beta^{t-t_0}\bigg\{  (\pi_t^2  + \lambda_x x_t^2 )  \\
 & + \varphi_{1,t} \bigg(\pi_t - \kappa x_t -(1-\alpha)\beta f_{a,t} -\kappa\alpha\beta b_2 (I_3 - \alpha\beta h)^{-1}s_t - e_3(I_3 - \alpha\beta h)^{-1}s_t \bigg) \label{midsimple_first}\\
 & + \varphi_{2,t} \bigg(x_t + \sigma i_t -\sigma f_{b,t}  -  (1-\beta)b_2 (I_3 - \beta h)^{-1}s_t + \sigma\beta b_3 (I_3 - \beta h)^{-1}s_t -\sigma e_1(I_3 - \beta h)^{-1}s_t  \big)\bigg) \\
 & +  \varphi_{3,t}  \bigg(f_{a,t} - \frac{1}{1-\alpha\beta}\bar{\pi}_{t-1}  - b_1(I_3 - \alpha\beta h)^{-1}s_t  \bigg) \\
 & + \varphi_{4,t}  \bigg(f_{b,t} - \frac{1}{1-\beta}\bar{\pi}_{t-1}  - b_1(I_3 - \beta h)^{-1}s_t \bigg)  \\
  & + \varphi_{5,t}  \bigg(  \bar{\pi}_{t} - \bar{\pi}_{t-1} - k_t\big(\pi_{t} -(\bar{\pi}_{t-1}+b_1 s_{t-1}) \big)   \bigg)  \\
  & + \varphi_{6,t}  \bigg(k_t - \mathbf{g}(\pi_t - \bar{\pi}_{t-1}-b_1 s_{t-1})  \bigg)
  \bigg\} \label{midsimple_last}
\end{align}
where $\rho_i$ are Lagrange-multipliers on the constraints.
After a little bit of simplifying, the first-order conditions boil down to the following three equations:
\begin{align}
& 2\pi_t + 2\frac{\lambda_x}{\kappa}x_t -\varphi_{5,t} k_t - \varphi_{6,t} \mathbf{g}_{\pi,t} = 0 \label{gaspar22}\\
& -\frac{2(1-\alpha)\beta}{1-\alpha\beta}\frac{\lambda_x}{\kappa}x_{t+1} + \varphi_{5,t} -(1-k_t)\varphi_{5,t+1} +\mathbf{g}_{\bar{\pi},t}\varphi_{6,t+1} = 0 \label{gaspar21}\\
& \varphi_{6,t} = (\pi_t - \bar{\pi}_{t-1}-b_1 s_{t-1}) \varphi_{5,t} \label{constraints}
\end{align}
Note that Equation (\ref{gaspar22}) is the analogue of \cite{gaspar2010inflation}'s Equation (22) (or, equivalently, of  \cite{molnar2014optimal}'s (16)), except that there is an additional multiplier, $\varphi_6$. This multiplier reflects the fact that in addition to the constraint coming from the expectation process itself, with shadow value $\varphi_5$, learning involves the gain equation as a constraint as well. One can also clearly read off Result \ref{result_no_commitment}: when the learning process has converged such that neither expectations nor the gain process are constraints ($\varphi_5 =\varphi_6 = 0$), the discretionary inflation-output gap tradeoff familiar from \cite{clarida1999science} obtains. Combining the above three equations and solving for $\varphi_{5,t}$, using the notation that $\prod_{j=1}^{0} \equiv 1$, one obtains the target criterion (\ref{target}).

The system of first-order conditions (\ref{simpleFOC1})-(\ref{simpleFOC2}) and model equations for this simplified system also reveal how the endogenous gain introduces nonlinearity to the equation system. In particular, notice how in equations (\ref{simpleFOC1})-(\ref{simpleFOC2}), the gain $k_t$ shows up multiplicatively with the Lagrange multiplier, $\varphi_{2,t}$. In fact, the origin of the problem is the recursive least squares learning equation for the learning coefficient $f_t$
\begin{equation}
f_t = f_{t-1} + k_t(\pi_t - f_{t-1}) \label{simpleRLS}
\end{equation}
where the first interaction terms between the gain and other endogenous variables show up. This results in an equation system of nonlinear difference equations that does not admit an analytical solution. 

Considering equation (\ref{simpleRLS}) is instructive to see how it is indeed the endogeneity of the gain that causes these troubles. Were we to specify a constant gain setup, $k_t$ would merely equal the constant $\bar{g}$ and the anchoring function $\mathbf{g}$ would trivially reduce to zero as well. In such a case, all interaction terms would reduce to multiplication between endogenous variables and parameters; linearity would be restored and a solution for the optimal time paths of endogenous variables would be obtainable. Similarly, a decreasing gain specification would also be manageable since for all $t$, the gain would simply be given by $t^{-1}$, and the anchoring function would also be deterministic and exogenous. 

\section{A target criterion for an anchoring mechanism specified in terms of gain changes}\label{app_generalTC}
Consider the general anchoring mechanism of Equation (\ref{anchoring}):
\begin{equation}
k_t = \rho_kk_{t-1} + \mathbf{g}(fe_{t|t-1})
\end{equation}
With this assumption, the FOCs of the Ramsey problem are
\begin{align}
& 2\pi_t + 2\frac{\lambda_x}{\kappa}x_t -k_t \varphi_{5,t} - \mathbf{g}_{\pi,t}\varphi_{6,t}  = 0 \label{gaspar22_general}\\
& c x_{t+1} + \varphi_{5,t} -(1-k_t)\varphi_{5,t+1} +\mathbf{g}_{\bar{\pi},t}\varphi_{6,t+1} = 0 \label{gaspar21_general}\\
& \varphi_{6,t} \; \textcolor{brownreddark}{+\; \varphi_{6,t+1}} = fe_{t|t-1} \varphi_{5,t} \label{constraints_general}
\end{align}
where the red multiplier is the new element vis-\`a-vis the case where the anchoring function is specified in levels ($k_t = \mathbf{g}(fe_{t|t-1})$), and I'm using the shorthand notation
\begin{align}
c & = -\frac{2(1-\alpha)\beta}{1-\alpha\beta}\frac{\lambda_x}{\kappa} \\ 
fe_{t|t-1} & = \pi_t - \bar{\pi}_{t-1}-b_1 s_{t-1}
\end{align}
(\ref{gaspar22}) says that in anchoring, the discretion tradeoff is complemented with tradeoffs coming from learning ($\varphi_{5,t}$), which are more binding when expectations are unanchored ($k_{t}^{-1}$ high). Moreover, the change in the anchoring of expectations imposes an additional constraint ($\varphi_{6,t}$), which is more strongly binding if the gain responds strongly to inflation ($\mathbf{g}_{\pi,t}$ is high in absolute value).
One can simplify this three-equation-system to:
\begin{align}
\varphi_{6,t} & = -c fe_{t|t-1} x_{t+1} + \bigg(1+ \frac{fe_{t|t-1}}{fe_{t+1|t}}(1-k_{t+1}^{-1}) -fe_{t|t-1} \mathbf{g}_{\bar{\pi},t} \bigg) \varphi_{6,t+1} -\frac{fe_{t|t-1}}{fe_{t+1|t}}(1-k_{t+1}^{-1})\varphi_{6,t+2}\label{6'} \\
0 & = 2\pi_t + 2\frac{\lambda_x}{\kappa}x_t   - \bigg( \frac{k_t}{fe_{t|t-1}} + \mathbf{g}_{\pi,t}\bigg)\varphi_{6,t} + \frac{k_t}{fe_{t|t-1}}\varphi_{6,t+1}\label{1'}
\end{align}
Thus a central bank that follows the target criterion has to compute $\varphi_{6,t}$ as the solution to (\ref{1'}), and then evaluate (\ref{6'}) as a target criterion. The solution to (\ref{1'}) is given by:
\begin{equation}
\varphi_{6,t} = -2\E_t\sum_{i=0}^{\infty}(\pi_{t+i}+\frac{\lambda_x}{\kappa}x_{t+i})\prod_{j=0}^{i-1}\frac{\frac{k_{t+j}}{fe_{t+j|t}}}{\frac{k_{t+j}}{fe_{t+j|t}} + \mathbf{g}_{\pi,t+j}} \label{sol1'}
\end{equation}
The interpretation of (\ref{sol1'}) is that the anchoring constraint is not binding ($\varphi_{6,t}=0$) if the central bank always hits the target ($\pi_{t+i}+\frac{\lambda_x}{\kappa}x_{t+i} = 0, \; \forall i$); or expectations are always anchored ($k_{t+j}=0, \; \forall j$). 

\section{Parameterized expectations algorithm (PEA)} \label{pea}
The objective of the parameterized expectations algorithm is to solve for the sequence of interest rates that solves the model equations including the target criterion, representing the first-order condition of the Ramsey problem. For convenience, I list the model equations:
\begin{align*}
x_t &=  -\sigma i_t + \begin{bmatrix} \sigma & 1-\beta & -\sigma\beta \end{bmatrix} f_{b,t} + \sigma \begin{bmatrix} 1 & 0 & 0 \end{bmatrix} (I_{3} - \beta h)^{-1} s_t \label{A9} \numberthis \\
\pi_t &= \kappa x_t  + \begin{bmatrix} (1-\alpha)\beta & \kappa\alpha\beta & 0 \end{bmatrix}  f_{a,t} + \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}  (I_{3} - \alpha \beta h)^{-1}  s_t \label{A10} \numberthis\\
f_{a,t} & = \frac{1}{1-\alpha\beta}\bar{\pi}_{t-1}  + b(I_{3} - \alpha\beta h)^{-1}s_t \numberthis \\
f_{b,t} & = \frac{1}{1-\beta}\bar{\pi}_{t-1}  + b(I_{3} - \beta h)^{-1}s_t  \label{A8} \numberthis \\
 fe_{t|t-1} &  = \pi_t - (\bar{\pi}_{t-1}+b_1 s_{t-1}) \label{A7} \numberthis \\
 k^{-1}_t & = \gamma_k fe_{t|t-1}^2 \numberthis  \\
 \bar{\pi}_{t} &  = \bar{\pi}_{t-1} +k_tfe_{t|t-1} \numberthis \\
 \pi_t & = -\frac{\lambda_x}{\kappa}\bigg\{x_t - \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t+fe_{t|t-1}\mathbf{g}_{\pi,t} \bigg) \bigg(\E_t\sum_{i=1}^{\infty}x_{t+i}\prod_{j=0}^{i-1}(1-k_{t+1+j}^{-1} - (fe_{t+1+j|t+j})\mathbf{g_{\bar{\pi}, t+j}}) \bigg)
\bigg\} \numberthis \label{B1}
\end{align*}

Denote the expectation on the right hand side of (\ref{B1}) as $E_t$. The idea of the PEA is to approximate this expectation and to solve model equations given the approximation $\hat{E}_t$. The algorithm is as follows:\footnote{For a thorough treatment of the PEA, the interested reader is referred to \cite{christiano2000occasionally}.}
\begin{enumerate}
\item[Objective:] Obtain the sequence $\{i_t\}_{t=1}^T$ that solves Equations (\ref{A9}) - (\ref{B1}) for a history of exogenous shocks $\{s_t\}_{t=1}^T$ of length $T$. 
\item Conjecture an initial expectation $\hat{E}_t=\beta^0 s(X_t)$. \\
The expectation is approximated as a projection on a basis, $s(X_t)$, where $\beta^0$ are initial projection coefficients, and $X_t = (k_t,\bar{\pi}_{t-1}, r^n_t, u_t)$ is the state vector. I use a monomial basis consisting of the first, second and third powers of $X_t$.
\item Solve model equations given conjectured $\hat{E}_t$ for a given sequence of shocks $\{s_t\}_{t=1}^T$.\\
Compute residuals to the model equations (\ref{A9}) - (\ref{B1}) given $\{s_t\}_{t=1}^T$ and $\{\hat{E}_t\}_{t=1}^T$. Obtain a sequence $\{i_t\}_{t=1}^T$ that sets the residuals to zero. The output of this step is $\{v_t\}_{t=1}^T$, the simulated history of endogenous variables (\cite{christiano2000occasionally} refer to this as ``synthetic time series"). 
\item Compute realized analogues of $\{E_t\}_{t=1}^T$ given $\{v_t\}_{t=1}^T$.
\item Update $\beta$ regressing the synthetic $E_t$ on $s(X_t)$.\\
The coefficient update is $\beta^{i+1} = (s(X_t)'s(X_t))^{-1}s(X_t)'E_t$. Then iterate until convergence by evaluating at every step $||\beta^i-\beta^{i+1}||$.
\end{enumerate}

\section{Parametric value function iteration} \label{vfi}
This is an alternative approach I implement as a robustness check to the PEA. The objective is thus the same: to obtain the interest rate sequence that solves the model equations. The general value iteration approach is fairly standard, for which reason I refer to the \cite{judd1998numerical} textbook for details. Specific to my application is that the state vector is six-dimensional, $X_t = (k_t,\bar{\pi}_{t-1}, r^n_t, u_t, r^n_{t-1}, u_{t-1})$, and I approximate the value function using a cubic spline. Thus the output of the algorithm is a cubic spline approximation of the value function and a policy function for each node on the grid of states. Next, I interpolate the policy function using a cubic spline. As a last step I pass the state vector from the PEA simulation, obtaining an interest rate sequence conditional on the history of states. Fig. \ref{compare_pea_vfi} shows the resulting interest rate sequence, obtained through the two approaches, conditional on two different simulated sequences for the states.

\begin{figure}[h!]
\subfigure[$i(X_1)$]{\includegraphics[scale=0.2]{\myFigPath \fignamePEAvsVFIfirstX}} % original was: materials32_policy
%\subfigure[$i(X_2)$]{\includegraphics[scale=0.2]{\myFigPath \fignamePEAvsVFIsecondX}}
\caption{Policy function for a particular history of states, PEA against VFI}
\end{figure}


\section{Oscillatory dynamics in adaptive learning models} \label{app_oscillations}
Here I present an illustration for why adaptive learning models produce oscillatory impulse responses if the gain is high enough. Consider a stylized adaptive learning model in two equations:
\begin{align}
\pi_t & = \beta f_t + u_t \label{simple_NKPC} \\
f_t & = f_{t-1} + k^{-1}(\pi_t - f_{t-1}) \label{simple_expectations}
\end{align}
The reader can recognize in (\ref{simple_NKPC}) a simplified Phillips curve in which I am abstracting from output gaps to keep the presentation as clear as possible. Like in the simple model of Section \ref{ramsey} in the main text, $f_t$ represents the one-period inflation expectation $\hat{\E}_t \pi_{t+1}$. (\ref{simple_expectations}), then, represents the simplest possible recursive updating of the expectations $f_t$. My notation of the gain as $k^{-1}$ indicates a constant gain specification, but the intuition remains unchanged for decreasing (or endogenous) gains. 

Combining the two equations allows one to solve for the time series of expectations
\begin{equation}
f_t = \frac{1-k^{-1}}{1-k^{-1}\beta}f_{t-1} + \frac{k^{-1}}{1-k^{-1}\beta}u_t
\end{equation}
which, for $\beta$ close but smaller than 1, is a near-unit-root process. (In fact, if the gain were to go to zero, this would be a unit root process.) Defining the forecast error as $fe_{t|t-1} \equiv \pi_t - f_{t-1}$, one obtains
\begin{equation}
fe_{t|t-1} = -\frac{1-\beta}{1-k^{-1}\beta}f_{t-1} + \frac{1}{1-k^{-1}\beta}u_t \label{oscillating_fe}
\end{equation}
Equation (\ref{oscillating_fe}) shows that in this simple model, the forecast error loads on a near-unit-root process with a coefficient that is negative and less than one in absolute value. Damped oscillations are the result. 

Note that even if the gain would converge to zero, the coefficient on $f_{t-1}$ would be negative and less than one in absolute value. Thus even for decreasing gain learning, one obtains oscillations, but the lower the gain, the more damped the oscillations become. This corroborates my findings in the impulse responses of Fig. \ref{IRF}. But importantly, the opposite extreme, when $k^{-1}\rightarrow 1$, results in a coefficient of exactly $-1$, giving perpetual oscillations. This clearly illustrates how the oscillatory behavior of impulse responses comes from the oscillations in the forecast error that obtain when the gain is sufficiently large. 

\section{Impulse responses in the anchoring model}\label{app_IRFs}
This section illustrates the dynamics of the model conditional on whether expectations are anchored or not. To cleanly separate these two cases, I here rely on the discrete CUSUM-based anchoring function described in Appendix \ref{alternative_criteria}. The CUSUM-specific parameters $\tilde{\kappa} = 0.8$ and $\tilde{\theta}=2.5$ are set by experimentation to deliver simulated gain sequences that generally decrease over time, but periodically jump up. All impulse responses are following a one-standard deviation contractionary monetary policy shock. As opposed to the main text, I set a persistence parameter for the shock process to $\rho_i = 0.6$ in order that the model dynamics become clearly visible.

Fig. \ref{IRF} portrays the impulse responses of the model after a contractionary monetary policy shock. The red dashed lines show the responses of the observables in the rational expectations version of the model. The blue lines show the responses in the learning model, on panel (a) conditional on expectations being anchored when the shock hits, on panel (b) being unanchored upon the arrival of the shock. 

\begin{figure}[h!]
\subfigure[RE against anchoring, expectations anchored]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath \fignameIRFanchored}}
\subfigure[RE against anchoring, expectations unanchored]{\includegraphics[scale = \mySmallerFigScale]{\myFigPath \fignameIRFunanchored}}
\caption{Impulse responses after a contractionary monetary policy shock}
\floatfoot{Shock imposed at $t=25$ of a sample length of $T=400$ (with 5 initial burn-in periods), cross-sectional average with a cross-section size of $N=1000$. For the anchoring model, the remark refers to whether expectations are anchored at the time the shock hits.}
\label{IRF}
\end{figure}

Not only do the impulse responses show the usual behavior of learning models - dampened responses and increased persistence. More importantly, responses differ strongly depending on whether expectations are anchored or not when the shock hits. In particular, if expectations are anchored, responses are closer to rational expectations than when expectations are unanchored. Moreover, when expectations are unanchored, the endogenous responses of the observables become much more volatile, indeed, oscillatory\footnote{See App. \ref{app_oscillations} for an explanation of how forecast errors are responsible for the oscillatory dynamics.}. This makes intuitive sense: expectations being unanchored reflects the fact that firms and households are confronted with an environment that does not line up with their currently held perceived law of motion. They thus believe that a structural change has occurred and are therefore revising their expectations. Expectations are therefore fluctuating strongly, and as they feed back to the observables, the latter inherit their volatility. 

\begin{figure}[h!]
\subfigure[$\psi_{\pi} = 1.01$]{\includegraphics[scale = 0.14]{\myFigPath \fignameGainPsiSmall}}
\subfigure[$\psi_{\pi} = 1.5$]{\includegraphics[scale = 0.14]{\myFigPath \fignameGainPsiMedium}}
\subfigure[$\psi_{\pi} = 2$]{\includegraphics[scale = 0.14]{\myFigPath \fignameGainPsiBig}}
\caption{Cross-sectional average gains for various values of $\psi_{\pi}$}
\floatfoot{Sample length is $T=400$ (with 5 initial burn-in periods), cross-section size is $N=1000$.}
\label{anchor_psi}
\end{figure}

Fig. \ref{anchor_psi} shows the cross-sectional average of gains that result when $\psi_{\pi}$ takes on different values. Clearly, a higher $\psi_{\pi}$ results in lower and decreasing gains.\footnote{As I remark in App. \ref{alternative_criteria}, if one uses the anchoring function of \cite{carvalho2019anchored}, this conclusion is overturned.} Thus a central bank aiming to anchor expectations needs to employ a high $\psi_{\pi}$. 

Fig. \ref{IRF_unanchored_psi} depicts the same impulse responses to a contractionary monetary policy shock as Fig. \ref{IRF}, focusing however only on responses conditional on expectations being unanchored upon the shock. It shows these responses for three different values of $\psi_{\pi}$.
\begin{figure}[h!]
\subfigure[$\psi_{\pi} = 1.01$]{\includegraphics[scale = 0.20]{\myFigPath \fignameIRFpsipiSmall}}
\subfigure[$\psi_{\pi} = 1.5$]{\includegraphics[scale = 0.20]{\myFigPath \fignameIRFpsipiMedium}}
\subfigure[$\psi_{\pi} = 2$]{\includegraphics[scale = 0.20]{\myFigPath \fignameIRFpsipiBig}}
\caption{Impulse responses for unanchored expectations for various values of $\psi_{\pi}$}
\floatfoot{Shock imposed at $t=25$ of a sample length of $T=400$ (with 5 initial burn-in periods), cross-sectional average with a cross-section size of $N=1000$.}
\label{IRF_unanchored_psi}
\end{figure}
As the figure shows, a high $\psi_{\pi}$ leads to more volatility than a low one does. The intuition is a little subtle. Since expectations are unanchored, they are also volatile because the gain is high. This implies that expected inflation far ahead in the future fluctuates strongly. Since agents know the Taylor rule, this also means that they expect the nominal interest rate far in the future to respond. The more aggressive the central bank, the stronger an interest rate response will the agents expect. This however feeds back into current output gaps and thus inflation. Higher overall volatility is the result.

The model dynamics here echo the predictions of \cite{ball1994credible} of expansionary disinflation. But the underlying channels are quite different. \cite{ball1994credible} observes that, contrary to conventional wisdom, rational expectations New Keynesian models imply expansionary disinflations. To reconcile this model feature with data pointing to the costliness of disinflations, he concludes that central bank announcements must suffer from credibility issues. 

Note that in the present context, when expectations are anchored (Panel (a) of Fig. \ref{IRF}), impulse responses do not exhibit this feature. However, when expectations are unanchored (Panel (b) of Fig. \ref{IRF}), impulse responses look exactly as \cite{ball1994credible} predicts: we obtain an expansionary disinflation. 

The reason this is happening is the above-mentioned fact that when agents know the Taylor rule, long-horizon expectations of the interest rate move in tandem with the same expectations of inflation in the far future. A current disinflation lowers long-horizon inflation expectations, leading the public to expect low interest rates far out in the future. Through the NKIS-curve (Equation \ref{NKIS}), this stimulates current output.\footnote{The extension in which the public has to learn the Taylor rule is interesting in this regard. As expected, the Ball-type disinflationary boom does not initially show up in impulses responses obtained in that extension. However, as the agents are learning the Taylor rule, the expansionary disinflation slowly reemerges in the impulse responses.} But the absence of the ``Ball-effect" from the anchored expectations impulse responses indicates that the channel is only operational when expectations are moving sufficiently. Thus I arrive at a different conclusion than \cite{ball1994credible}; instead of credibility issues, it is anchored expectations that are responsible for the absence of expansionary disinflations of the type seen on Fig \ref{IRF}, Panel (b).


\end{document}





