\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb,lscape, natbib}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{setspace}
\usepackage[final]{pdfpages}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm, bottom=2.5cm]{geometry}
\usepackage{natbib} 
\usepackage{bibentry} 
\newcommand{\bibverse}[1]{\begin{verse} \bibentry{#1} \end{verse}}
\newcommand{\vs}{\vspace{.3in}}
\renewcommand{\ni}{\noindent}
\usepackage{xr-hyper}
\usepackage[]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}
\usepackage[capposition=top]{floatrow}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
\usepackage{tikz}
 
\pagestyle{fancy} % customize header and footer
\fancyhf{} % clear initial header and footer
%\rhead{Overleaf}
\lhead{\centering \rightmark} % this adds subsection number and name
\lfoot{\centering \rightmark} 
\rfoot{\thepage} % put page number (the centering command puts it in the middle, don't matter if you put it in right or left footer)

\def \myFigPath {../../figures/} 
% BE CAREFUL WITH FIGNAMES, IN LATEX THEY'RE NOT CASE SENSITIVE!!
\def \myTablePath {../../tables/} 

%\definecolor{mygreen}{RGB}{0, 100, 0}
\definecolor{mygreen}{RGB}{0, 128, 0}

\definecolor{citec}{rgb}{0,0,.5}
\definecolor{linkc}{rgb}{0,0,.6}
\definecolor{bcolor}{rgb}{1,1,1}
\hypersetup{
%hidelinks = true
  colorlinks = true,
  urlcolor=linkc,
  linkcolor=linkc,
  citecolor = citec,
  filecolor = linkc,
  pdfauthor={Laura G\'ati},
}


\geometry{left=.83in,right=.89in,top=1in,
bottom=1in}
\linespread{1.5}
\renewcommand{\[}{\begin{equation}}
\renewcommand{\]}{\end{equation}}

% New Options
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

%\newtheorem{theorem}{Theorem}[section] % the third argument specifies that their number will be adopted to the section
%\newtheorem{corollary}{Corollary}[theorem]
%\newtheorem{lemma}[theorem]{Lemma}
%\declaretheorem{proposition}
%\linespread{1.3}
%\raggedbottom
%\font\reali=msbm10 at 12pt

% New Commands
\newcommand{\real}{\hbox{\reali R}}
\newcommand{\realp}{\hbox{\reali R}_{\scriptscriptstyle +}}
\newcommand{\realpp}{\hbox{\reali R}_{\scriptscriptstyle ++}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\argmin}{arg\,min}
\newcommand\w{3.0in}
\newcommand\wnum{3.0}
\def\myFigWidth{5.3in}
\def\mySmallerFigWidth{2.1in}
\def\myEvenBiggerFigScale{0.8}
\def\myPointSixFigScale{0.6}
\def\myBiggerFigScale{0.4}
\def\myFigScale{0.3}
\def\myMediumFigScale{0.25}
\def\mySmallFigScale{0.22}
\def\mySmallerFigScale{0.18}
\def\myTinyFigScale{0.16}
\def\myPointFourteenFigScale{0.14}
\def\myTinierFigScale{0.12}
\def\myAdjustableFigScale{0.16}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % this defines a command to make align only number this line
\newcommand{\code}[1]{\texttt{#1}} %code %

\renewcommand*\contentsname{Overview}
\setcounter{tocdepth}{2}

% define a command to make a huge question mark (it works in math mode)
\newcommand{\bigqm}[1][1]{\text{\larger[#1]{\textbf{?}}}}

\begin{document}

\linespread{1.0}

\title{Materials 37 - Cross-section, neighborhood of zero forecast errors}
\author{Laura G\'ati} 
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%             DOCUMENT           %%%%%%%%%%%%%%%%%% 

\tableofcontents

%\listoffigures

\newpage

\section{Simulated ``true'' data}
\begin{itemize}
\item[(a)] In the last materials, we saw that for about $N=1000$, asymptotic behavior happens. So having a cross-section of $N$ seems to work.
\item[(b)] We also saw that in the neighborhood of zero forecast errors the estimates don't converge to 0. These materials aim to solve this issue. The first two figures recapitulate the problem.
\end{itemize}

\subsection{Odd number of knots (5)}

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_758_nfe_5_gridspacing_uniform_Wdiffs2_100000_Wmid_0_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_758_gridspacing_uniform_Wdiffs2_100000_Wmid_0_16_Jul_2020}}
\caption{Mean estimates for $N=100$, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}

\subsection{Even number of knots (6)}

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}) $]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_731_nfe_6_gridspacing_uniform_Wdiffs2_100000_Wmid_0_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_6_loss_731_gridspacing_uniform_Wdiffs2_100000_Wmid_0_16_Jul_2020}}
\caption{Mean estimates for $N=100$, imposing convexity with weight 100K, truth with $nfe=6, fe \in(-3.5,3.5)$ }
\end{figure}

\clearpage
\subsection{Finer grid in the zero neighborhood - uneven spacing and restriction at 0}
\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \colorbox{yellow}{uniformly spaced}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_694_nfe_5_gridspacing_uniform_Wdiffs2_10000_Wmid_0_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_694_gridspacing_uniform_Wdiffs2_10000_Wmid_0_16_Jul_2020}}

\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \colorbox{yellow}{concentrated around 0} $]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_770_nfe_5_gridspacing_uneven_Wdiffs2_10000_Wmid_0_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_770_gridspacing_uneven_Wdiffs2_10000_Wmid_0_16_Jul_2020}}

\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \newline \colorbox{yellow}{concentrated around 0, 0 at 0 imposed with weight 1000} $]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_803_nfe_5_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_15_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_803_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_15_Jul_2020}}
\caption{Mean estimates for $N=100$, 5 breakpoints, imposing convexity w/ weight 10K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}


%\subsection{Can I decrease the weight on the convexity moment?}
\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \newline \colorbox{yellow}{5 breakpoints, convexity weight 1000}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_878_nfe_5_gridspacing_uneven_Wdiffs2_1000_Wmid_1000_15_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_878_gridspacing_uneven_Wdiffs2_1000_Wmid_1000_15_Jul_2020}}
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \newline \colorbox{yellow}{5 breakpoints, convexity weight 10000} $]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_803_nfe_5_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_15_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_803_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_15_Jul_2020}}
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \newline \colorbox{yellow}{7 breakpoints, convexity weight 10000} $]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_781_nfe_7_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_15_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_7_loss_781_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_15_Jul_2020}}
\caption{Can I decrease the weight on the convexity moment? Mean estimates for $N=100$, breakpoints concentrated around 0, 0 at 0 imposed with weight 1000, imposing convexity with variable weight, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}


%% V-shape
\subsection{Is the V-shape forced? A U-shaped truth}
\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \newline \colorbox{yellow}{6 knots, uniform grid, 0 at 0 restriction not imposed} $]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_747_nfe_6_gridspacing_uniform_Wdiffs2_10000_Wmid_0_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_6_loss_747_gridspacing_uniform_Wdiffs2_10000_Wmid_0_16_Jul_2020}}
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \colorbox{yellow}{5 knots} $]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_804_nfe_5_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_804_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \colorbox{yellow}{7 knots}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_801_nfe_7_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_7_loss_801_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\caption{Mean estimates for $N=100$, imposing convexity with weight 10K, knots concentrated around 0, 0 at 0 imposed with weight 1000; truth with $nfe=6, fe \in(-3.5,3.5)$ }
\end{figure}

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \newline \colorbox{yellow}{9 knots} $]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_950_nfe_9_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_9_loss_950_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \colorbox{yellow}{11 knots} $]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_764_nfe_11_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_11_loss_764_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), \colorbox{yellow}{13 knots} $]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_3728_nfe_13_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_13_loss_3728_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\caption{Continued}
\end{figure}
\begin{itemize}
\item I still seem to see the issue that the 0-neighborhood isn't well identified, but it doesn't seem grave.
\item 11 knots or more seems underidentified, 9 may be the max.
\end{itemize}

\subsection{Are there no forecast errors in the 0-neighborhood?}

\begin{figure}[h!]
\subfigure[truth with $nfe=6, fe \in(-3.5,3.5)$\colorbox{yellow}{13 knots} ]{\includegraphics[scale=\myFigScale]{\myFigPath command_GMM_LOMgain_univariate_histogram_FE_sim_nfe_13_loss_3728_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\hfill
\subfigure[truth with $nfe=6, fe \in(-3.5,3.5)$\colorbox{yellow}{5 knots} ]{\includegraphics[scale=\myFigScale]{\myFigPath command_GMM_LOMgain_univariate_histogram_FE_sim_nfe_5_loss_804_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\caption{Distribution of forecast errors in the cross-section, $N=100$}
\end{figure}

$\rightarrow$ Yes there are! In fact, too many knots make forecast errors more concentrated around 0.

%%%%%%%%             REAL DATA             %%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Autocovariogram for real data}
\subsection{\# knots}
\begin{figure}[h!]
\subfigure[$\bar{\hat{\alpha}}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alph_opt_loss_287_nfe_5_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram at an average of the $N$ shock histories]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_nfe_5_loss_287_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\subfigure[Distribution of forecast errors]{\includegraphics[scale=\myFigScale]{\myFigPath command_GMM_LOMgain_univariate_histogram_FE_nfe_5_loss_287_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\caption{Mean estimated parameters, autocovariogram and forecast errors for \colorbox{yellow}{ 5 breakpoints}}
\floatfoot{Constant estimation parameters: cross-section of size $N=100, fe \in(-3.5,3.5)$, convexity imposed with weight 10K, points unevenly spaced (denser at 0), 0 at 0 imposed with weight 1000, mean moment not imposed}
\end{figure}

\clearpage

\begin{figure}[h!]
\subfigure[$\bar{\hat{\alpha}}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alph_opt_loss_279_nfe_7_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram at an average of the $N$ shock histories]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_nfe_7_loss_279_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\subfigure[Distribution of forecast errors]{\includegraphics[scale=\myFigScale]{\myFigPath command_GMM_LOMgain_univariate_histogram_FE_nfe_7_loss_279_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\caption{Mean estimated parameters, autocovariogram and forecast errors for \colorbox{yellow}{ 7 breakpoints}}
\floatfoot{Constant estimation parameters: cross-section of size $N=100, fe \in(-3.5,3.5)$, convexity imposed with weight 10K, points unevenly spaced (denser at 0), 0 at 0 imposed with weight 1000, mean moment not imposed}
\end{figure}

\clearpage

\begin{figure}[h!]
\subfigure[$\bar{\hat{\alpha}}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alph_opt_loss_269_nfe_9_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram at an average of the $N$ shock histories]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_nfe_9_loss_269_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\subfigure[Distribution of forecast errors]{\includegraphics[scale=\myFigScale]{\myFigPath command_GMM_LOMgain_univariate_histogram_FE_nfe_9_loss_269_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\caption{Mean estimated parameters, autocovariogram and forecast errors for \colorbox{yellow}{ 9 breakpoints}}
\floatfoot{Constant estimation parameters: cross-section of size $N=100, fe \in(-3.5,3.5)$, convexity imposed with weight 10K, points unevenly spaced (denser at 0), 0 at 0 imposed with weight 1000, mean moment not imposed}
\end{figure}
Note: only 6/100 converged $\rightarrow$ seems underidentified. Pick 7 knots as default.

\clearpage
\subsection{Support of forecast errors}
\begin{figure}[h!]
\subfigure[$\bar{\hat{\alpha}}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alph_opt_loss_302_nfe_7_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram at an average of the $N$ shock histories]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_nfe_7_loss_302_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\subfigure[Distribution of forecast errors]{\includegraphics[scale=\myFigScale]{\myFigPath command_GMM_LOMgain_univariate_histogram_FE_nfe_7_loss_302_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\caption{Mean estimated parameters, autocovariogram and forecast errors for \colorbox{yellow}{$fe \in (-2,2)$}}
\floatfoot{Constant estimation parameters: cross-section of size $N=100$, 7 knots, convexity imposed with weight 10K, points unevenly spaced (denser at 0), 0 at 0 imposed with weight 1000, mean moment not imposed}
\end{figure}

\clearpage

\begin{figure}[h!]
\subfigure[$\bar{\hat{\alpha}}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alph_opt_loss_274_nfe_7_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram at an average of the $N$ shock histories]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_nfe_7_loss_274_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\subfigure[Distribution of forecast errors]{\includegraphics[scale=\myFigScale]{\myFigPath command_GMM_LOMgain_univariate_histogram_FE_nfe_7_loss_274_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\caption{Mean estimated parameters, autocovariogram and forecast errors for \colorbox{yellow}{$fe \in (-5,5)$}}
\floatfoot{Constant estimation parameters: cross-section of size $N=100$, 7 knots, convexity imposed with weight 10K, points unevenly spaced (denser at 0), 0 at 0 imposed with weight 1000, mean moment not imposed}
\end{figure}

$\rightarrow$ It seems robust to the choice of forecast-error-support!

\clearpage
\subsection{Repeat the last specification with $N=1000$}
\begin{figure}[h!]
\subfigure[$\bar{\hat{\alpha}}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alph_opt_loss_273_nfe_7_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram at an average of the $N$ shock histories]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_nfe_7_loss_273_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\subfigure[Distribution of forecast errors]{\includegraphics[scale=\myFigScale]{\myFigPath command_GMM_LOMgain_univariate_histogram_FE_nfe_7_loss_273_gridspacing_uneven_Wdiffs2_10000_Wmid_1000_16_Jul_2020}}
\caption{Mean estimated parameters, autocovariogram and forecast errors for \colorbox{yellow}{cross-section of size $N=1000$}}
\floatfoot{Constant estimation parameters: 7 knots, $fe \in (-5,5)$,convexity imposed with weight 10K, points unevenly spaced (denser at 0), 0 at 0 imposed with weight 1000, mean moment not imposed}
\end{figure}

%Notes:
\begin{itemize}
\item Took 96.33 min.
\item Seems robust.
\item mean($\hat{\alpha}) = (0.3509;    0.1427;    0.0180;    0.0005;    0.0204;    0.1406;    0.3411)$
\end{itemize}

%%%%%%%%%%     PEA                %%%%%%%%%%%%%%%%
\clearpage
\section{Redo PEA}
using $\alpha=$mean($\hat{\alpha})=(0.3509;    0.1427;    0.0180;    0.0005;    0.0204;    0.1406;    0.3411)$

\begin{figure}[h!]
\subfigure[Policy function: PEA against VFI]{\includegraphics[scale=\myTinyFigScale]{\myFigPath compare_value_pea_results_approx_value_outputs_approx17_Jul_2020_14_27_00_pea_outputs_approx17_Jul_2020_11_40_09_pretty}}
\hfill
\subfigure[Observables]{\includegraphics[scale=\myTinyFigScale]{\myFigPath implement_anchTC_obs_approx17_Jul_2020}}
\subfigure[Gain]{\includegraphics[scale=\myPointFourteenFigScale]{\myFigPath implement_anchTC_invgain17_Jul_2020}}
\caption{Optimal policy solution conditional on one particular evolution of shocks}
\floatfoot{Using the same parameter values as for estimation, using mean($\hat{\alpha})=(0.3509;    0.1427;    0.0180;    0.0005;    0.0204;    0.1406;    0.3411)$ as approximation for the anchoring function, no monpol shocks because Taylor rule is not in effect, thus it is also assumed not to be known. Shock sequence $2\times100$ generated using \texttt{rng(0)}.}
\end{figure}

\subsection{Why is policy nonstationary?}
\begin{figure}[h!]
\includegraphics[scale=\myPointFourteenFigScale]{\myFigPath implement_anchTC_pibar17_Jul_2020}
\hfill
\caption{Tracking long-run expectations? Evolution of $\bar{\pi}$}
\end{figure}

\begin{figure}[h!]
\subfigure[Policy function: PEA against VFI]{\includegraphics[scale=\myTinyFigScale]{\myFigPath compare_value_pea_results_approx_value_outputs_approx17_Jul_2020_14_27_00_pea_outputs_approx17_Jul_2020_15_28_03_pretty_17_Jul_2020_15_34_46}}
\hfill
\subfigure[Observables]{\includegraphics[scale=\myTinyFigScale]{\myFigPath implement_anchTC_obs_approx17_Jul_2020_15_34_46}}
\subfigure[Gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath implement_anchTC_invgain17_Jul_2020_15_34_46}}
\hfill
\subfigure[$\bar{\pi}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath implement_anchTC_pibar17_Jul_2020_15_34_46}}
\caption{Optimal policy solution conditional on a different, \texttt{rng(2)} evolution of shocks}
\end{figure}
\begin{itemize}
\item It sure looks like policy was chasing long-run expectations.
\item Running PEA for different PLMs, in particular, constant-only-inflation-only (default) vs. constant-only, all observables learning, I find that the nonstationary policy only occurs for my default policy. This seems to stem from the fact that if long-run expectations of $x$ and $i$ are also allowed to fluctuate, they, taken together with $\bar{\pi}$ cancel each other out. In particular, the long-run expectation of $i$ I think does the stabilization that $i$ itself needs to do if expectations of $i$ aren't allowed to fluctuate.
\end{itemize}

\subsection{Why is VFI-policy more volatile than that of PEA?}
Is this because the spline, as a spectral approximation method, has a tendency to oscillate around kinks? Or because the approximation is very rough (4 gridpoints (or 6, or 8) for $\bar{\pi}$, 2 each for the two shocks at $t$ and at $t-1$)?

%%%%%%%%%%     IRFS                %%%%%%%%%%%%%%%%
\clearpage
\section{Impulse responses to iid monpol shocks across a wide range of learning models}
$T=400, N=100, n_{drop}=5,$ shock imposed at $t=25$, calibration as above, Taylor rule assumed to be known, PLM = learn constant only, of inflation only.

\begin{figure}[h!]
\subfigure[Decreasing gain learning]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_RIR_LH_monpol_dgain_constant_only_pi_only_2020_07_07}}
\hfill
\subfigure[Mean gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_invgain_dgain_constant_only_pi_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_05_lami_0_2020_07_07}}
\subfigure[Constant gain learning]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_RIR_LH_monpol_cgain_constant_only_pi_only_2020_07_07}}
\hfill
\subfigure[Mean gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_invgain_cgain_constant_only_pi_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_05_lami_0_2020_07_07}}
\subfigure[CEMP criterion (vector)]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_RIR_LH_monpol_again_critCEMP_constant_only_pi_only_2020_07_07}}
\hfill
\subfigure[Mean gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_invgain_again_critCEMP_constant_only_pi_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_05_lami_0_2020_07_07}}
\caption{IRFs and gain history (sample means) }
\end{figure}


\begin{figure}[h!]
\subfigure[CUSUM criterion (vector)]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_RIR_LH_monpol_again_critCUSUM_constant_only_pi_only_2020_07_07}}
\hfill
\subfigure[Mean gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_invgain_again_critCUSUM_constant_only_pi_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_05_lami_0_2020_07_07}}
\subfigure[Smooth criterion, approximated, using $\alpha^{true}= (0.05;0.025;0;0.025;0.05)$, on $fe \in (-2,2)$.]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_RIR_LH_monpol_again_critsmooth_constant_only_pi_only_2020_07_07}}
\hfill
\subfigure[Mean gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_invgain_again_critsmooth_constant_only_pi_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_05_lami_0_2020_07_07}}

\caption{IRFs and gain history (sample means), continued }
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                              APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \clearpage
%    \newpage
\appendix
% the following command makes equation numbering include the section first, but just for what follows
\numberwithin{equation}{section}
\section{Model summary}

\vspace{-0.5cm}

\begin{align}
x_t &=  -\sigma i_t +\hat{\E}_t \sum_{T=t}^{\infty} \beta^{T-t }\big( (1-\beta)x_{T+1} - \sigma(\beta i_{T+1} - \pi_{T+1}) +\sigma r_T^n \big)  \label{A1}  \\
\pi_t &= \kappa x_t +\hat{\E}_t \sum_{T=t}^{\infty} (\alpha\beta)^{T-t }\big( \kappa \alpha \beta x_{T+1} + (1-\alpha)\beta \pi_{T+1} + u_T\big) \label{A2}  \\
i_t &= \psi_{\pi}\pi_t + \psi_{x} x_t  + \bar{i}_t \label{TR} \quad \quad (\text{if imposed})
\end{align}

\vspace{-1.2cm}

\begin{align}
\text{PLM:} \quad \quad & \hat{\E}_t z_{t+h}  =  a_{t-1} + bh_x^{h-1}s_t  \quad \forall h\geq 1 \quad \quad b = g_x\; h_x \quad \quad  \label{PLM} \\
\text{Updating:} \quad \quad & a_{t}  =a_{t-1} +k_t^{-1}\big(z_{t} -(a_{t-1}+b s_{t-1}) \big)  \label{A5} \\
\text{Anchoring function:} \quad \quad & k^{-1}_t  = \rho_k k^{-1}_{t-1} + \gamma_k fe_{t-1}^2 \label{A6}\\
\text{Forecast error:} \quad \quad & fe_{t-1}  = z_t - (a_{t-1}+b s_{t-1}) \label{A7} \\
\text{LH expectations:} \quad \quad & f_a(t) = \frac{1}{1-\alpha\beta}a_{t-1}  + b(\mathbb{I}_{nx} - \alpha\beta h)^{-1}s_t \quad \quad  f_b(t) = \frac{1}{1-\beta}a_{t-1}  + b(\mathbb{I}_{nx} - \beta h)^{-1}s_t  \label{A8}
\end{align}

\vspace{-0.5cm}

This notation captures vector learning ($z$ learned) for intercept only. For scalar learning, $a_t= \begin{pmatrix} \bar{\pi}_t & 0 & 0\end{pmatrix}' $ and $b_1$ designates the first row of $b$. The observables $(\pi, x)$ are determined as:
\begin{align}
x_t &=  -\sigma i_t + \begin{bmatrix} \sigma & 1-\beta & -\sigma\beta \end{bmatrix} f_b + \sigma \begin{bmatrix} 1 & 0 & 0 \end{bmatrix} (\mathbb{I}_{nx} - \beta h_x)^{-1} s_t \label{A9} \\
\pi_t &= \kappa x_t  + \begin{bmatrix} (1-\alpha)\beta & \kappa\alpha\beta & 0 \end{bmatrix}  f_a + \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}  (\mathbb{I}_{nx} - \alpha \beta h_x)^{-1}  s_t \label{A10}
\end{align}

\section{Target criterion}\label{target_crit_levels}
The target criterion in the simplified model (scalar learning of inflation intercept only, $k_t^{-1} = \mathbf{g}(fe_{t-1})$):
\begin{align*}
\pi_t  = -\frac{\lambda_x}{\kappa}\bigg\{x_t - \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+((\pi_t - \bar{\pi}_{t-1}-b_1 s_{t-1}))\mathbf{g}_{\pi}(t) \bigg) \\
\bigg(\E_t\sum_{i=1}^{\infty}x_{t+i}\prod_{j=0}^{i-1}(1-k_{t+1+j}^{-1} - (\pi_{t+1+j} - \bar{\pi}_{t+j}-b_1 s_{t+j})\mathbf{g_{\bar{\pi}}}(t+j)) \bigg)
\bigg\} \numberthis \label{target}
\end{align*}
where I'm using the notation that $\prod_{j=0}^{0} \equiv 1$. For interpretation purposes, let me rewrite this as follows:
\begin{align*}
\pi_t  = & \; \textcolor{red}{-\frac{\lambda_x}{\kappa} x_t} \textcolor{blue}{ \; + \frac{\lambda_x}{\kappa} \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+ fe^{eve}_{t|t-1}\mathbf{g}_{\pi}(t) \bigg)\E_t\sum_{i=1}^{\infty}x_{t+i}}  \\
& \textcolor{mygreen}{- \frac{\lambda_x}{\kappa} \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+ fe^{eve}_{t|t-1}\mathbf{g}_{\pi}(t) \bigg) \bigg(\E_t\sum_{i=1}^{\infty}x_{t+i}\prod_{j=0}^{i-1}(k_{t+1+j}^{-1} + fe^{eve}_{t+1+j|t+j})\mathbf{g_{\bar{\pi}}}(t+j) \bigg)}
\numberthis \label{target_interpretation}
\end{align*}
Interpretation: \textcolor{red}{tradeoffs from discretion in RE} + \textcolor{blue}{effect of current level and change of the gain on future tradeoffs} + \textcolor{mygreen}{effect of future expected levels and changes of the gain on future tradeoffs}

%\section{A target criterion system for an anchoring function specified for gain changes}\label{target_crit_changes}
%\begin{equation}
%k_t = k_{t-1} + \mathbf{g}(fe_{t|t-1})
%\end{equation}
%Turns out the $k_{t-1}$ adds one $\varphi_{6,t+1}$ too many which makes the target criterion unwieldy. The FOCs of the Ramsey problem are
%\begin{align}
%& 2\pi_t + 2\frac{\lambda}{\kappa}x_t -k_t^{-1} \varphi_{5,t} - \mathbf{g}_{\pi}(t)\varphi_{6,t}  = 0 \label{gaspar22}\\
%& c x_{t+1} + \varphi_{5,t} -(1-k_t^{-1})\varphi_{5,t+1} +\mathbf{g}_{\bar{\pi}}(t)\varphi_{6,t+1} = 0 \label{gaspar21}\\
%& \varphi_{6,t} \; \textcolor{red}{+\; \varphi_{6,t+1}} = fe_t \varphi_{5,t} \label{constraints}
%\end{align}
%where the red multiplier is the new element vis-a-vis the case where the anchoring function is specified in levels ($k_t^{-1} = \mathbf{g}(fe_{t-1})$, as in App. \ref{target_crit_levels}), and I'm using the shorthand notation
%\begin{align}
%c & = -\frac{2(1-\alpha)\beta}{1-\alpha\beta}\frac{\lambda}{\kappa} \\ 
%fe_t & = \pi_t - \bar{\pi}_{t-1}-b s_{t-1}
%\end{align}
%(\ref{gaspar22}) says that in anchoring, the discretion tradeoff is complemented with tradeoffs coming from learning ($\varphi_{5,t}$), which are more binding when expectations are unanchored ($k_{t}^{-1}$ high). Moreover, the change in the anchoring of expectations imposes an additional constraint ($\varphi_{6,t}$), which is more strongly binding if the gain responds strongly to inflation ($\mathbf{g}_{\pi}(t)$).
%One can simplify this three-equation-system to:
%\begin{align}
%\varphi_{6,t} & = -c fe_t x_{t+1} + \bigg(1+ \frac{fe_t}{fe_{t+1}}(1-k_{t+1}^{-1}) -fe_t \mathbf{g}_{\bar{\pi}}(t) \bigg) \varphi_{6,t+1} -\frac{fe_t}{fe_{t+1}}(1-k_{t+1}^{-1})\varphi_{6,t+2}\label{6'} \\
%0 & = 2\pi_t + 2\frac{\lambda}{\kappa}x_t   - \bigg( \frac{k_t^{-1}}{fe_t} + \mathbf{g}_{\pi}(t)\bigg)\varphi_{6,t} + \frac{k_t^{-1}}{fe_t}\varphi_{6,t+1}\label{1'}
%\end{align}
%Unfortunately, I haven't been able to solve (\ref{6'}) for $\varphi_{6,t}$ and therefore I can't express the target criterion so nicely as before. The only thing I can say is to direct the targeting rule-following central bank to compute $\varphi_{6,t}$ as the solution to (\ref{1'}), and then evaluate (\ref{6'}) as a target criterion. The solution to (\ref{1'}) is given by:
%\begin{equation}
%\varphi_{6,t} = -2\E_t\sum_{i=0}^{\infty}(\pi_{t+i}+\frac{\lambda_x}{\kappa}x_{t+i})\prod_{j=0}^{i-1}\frac{\frac{k_{t+j}^{-1}}{fe_{t+j}}}{\frac{k_{t+j}^{-1}}{fe_{t+j}} + \mathbf{g}_{\pi}(t+j)} \label{sol1'}
%\end{equation}
%Interpretation: the anchoring constraint is not binding ($\varphi_{6,t}=0$) if the CB always hits the target (
%$\pi_{t+i}+\frac{\lambda_x}{\kappa}x_{t+i} = 0 \quad \forall i$); or expectations are always anchored ($k_{t+j}^{-1}=0 \quad \forall j$). 


\end{document}

%%%%%%%%%%%%%    SUBFIGURE  %%%%%%%%%%%
%\begin{figure}[h!]
%\subfigure[Hodrick-Prescott, $\lambda=1600$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath materials22_gain_dhat_HP}}
%\hfill % this is great to intro dpace between subfigures
%\subfigure[Hamilton, 4 lags, $h=8$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath materials22_gain_dhat_Hamilton}}
%\subfigure[Baxter-King, $(6,32)$ quarters, truncation at 12 lags]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath materials22_gain_dhat_BK}}
%\caption{Inverse gain for $\hat{d}$ for the different filters}
%\end{figure}

%%%%%%%%%%%%%    TABLE  %%%%%%%%%%%
%\begin{center}
%\begin{table}[h!]
%\caption{$\hat{d}$}
%\begin{tabular}{ c |c |c }
%  & $W = I$ & $W = \text{diag}(\hat{\sigma}_{ac(0)}, \dots, \hat{\sigma}_{ac(K)})$ \\ 
%  \hline
% HP & 77.7899 & 10 \\  
% \hline
% Hamilton & 32.1649 & 10 \\  
% \hline
% BK & 90.3929 & 10    
%\end{tabular}
%\end{table}
%\end{center}





