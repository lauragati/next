\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb,lscape, natbib}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{setspace}
\usepackage[final]{pdfpages}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm, bottom=2.5cm]{geometry}
\usepackage{natbib} 
\usepackage{bibentry} 
\newcommand{\bibverse}[1]{\begin{verse} \bibentry{#1} \end{verse}}
\newcommand{\vs}{\vspace{.3in}}
\renewcommand{\ni}{\noindent}
\usepackage{xr-hyper}
\usepackage[]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}
\usepackage[capposition=top]{floatrow}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
\usepackage{tikz}
 
\pagestyle{fancy} % customize header and footer
\fancyhf{} % clear initial header and footer
%\rhead{Overleaf}
\lhead{\centering \rightmark} % this adds subsection number and name
\lfoot{\centering \rightmark} 
\rfoot{\thepage} % put page number (the centering command puts it in the middle, don't matter if you put it in right or left footer)

\def \myFigPath {../../figures/} 
% BE CAREFUL WITH FIGNAMES, IN LATEX THEY'RE NOT CASE SENSITIVE!!
\def \myTablePath {../../tables/} 

%\definecolor{mygreen}{RGB}{0, 100, 0}
\definecolor{mygreen}{RGB}{0, 128, 0}

\definecolor{citec}{rgb}{0,0,.5}
\definecolor{linkc}{rgb}{0,0,.6}
\definecolor{bcolor}{rgb}{1,1,1}
\hypersetup{
%hidelinks = true
  colorlinks = true,
  urlcolor=linkc,
  linkcolor=linkc,
  citecolor = citec,
  filecolor = linkc,
  pdfauthor={Laura G\'ati},
}


\geometry{left=.83in,right=.89in,top=1in,
bottom=1in}
\linespread{1.5}
\renewcommand{\[}{\begin{equation}}
\renewcommand{\]}{\end{equation}}

% New Options
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

%\newtheorem{theorem}{Theorem}[section] % the third argument specifies that their number will be adopted to the section
%\newtheorem{corollary}{Corollary}[theorem]
%\newtheorem{lemma}[theorem]{Lemma}
%\declaretheorem{proposition}
%\linespread{1.3}
%\raggedbottom
%\font\reali=msbm10 at 12pt

% New Commands
\newcommand{\real}{\hbox{\reali R}}
\newcommand{\realp}{\hbox{\reali R}_{\scriptscriptstyle +}}
\newcommand{\realpp}{\hbox{\reali R}_{\scriptscriptstyle ++}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\argmin}{arg\,min}
\newcommand\w{3.0in}
\newcommand\wnum{3.0}
\def\myFigWidth{5.3in}
\def\mySmallerFigWidth{2.1in}
\def\myEvenBiggerFigScale{0.8}
\def\myPointSixFigScale{0.6}
\def\myBiggerFigScale{0.4}
\def\myFigScale{0.3}
\def\myMediumFigScale{0.25}
\def\mySmallFigScale{0.22}
\def\mySmallerFigScale{0.18}
\def\myTinyFigScale{0.16}
\def\myPointFourteenFigScale{0.14}
\def\myTinierFigScale{0.12}
\def\myAdjustableFigScale{0.16}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % this defines a command to make align only number this line
\newcommand{\code}[1]{\texttt{#1}} %code %

\renewcommand*\contentsname{Overview}
\setcounter{tocdepth}{2}

% define a command to make a huge question mark (it works in math mode)
\newcommand{\bigqm}[1][1]{\text{\larger[#1]{\textbf{?}}}}

% use package and define command to add blank page
\usepackage{afterpage}
\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\begin{document}

\linespread{1.0}

\title{Materials 38 - Bias (not only?) in the neighborhood of zero forecast errors}
\author{Laura G\'ati} 
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%             DOCUMENT           %%%%%%%%%%%%%%%%%% 

\tableofcontents

%\listoffigures

\newpage

\section{Ways to try to get identification}
\subsection*{3 potential causes to lack of identification in the zero neighborhood}
\begin{enumerate}
\item The distribution of estimates is skewed $\rightarrow$ take $median(\hat{\alpha})$ instead of the mean. \\
$\Rightarrow$ also unidentified
\item The gain doesn't matter if the forecast error is 0, or very close to it $\rightarrow$ introduce a distinction between the forecast error that's used to choose the gain and the one used to update the coefficients of the learning rule. \\
$\Rightarrow$ tried using a different forecast error (time, output gap), also unidentified
\item  Introduce expectation series (SPF)
\item Taking mean moments across $N$ histories instead of performing the estimation $N$ times.
\item The truth is based on a simulation that doesn't favor the zero neighborhood $\rightarrow$ do 100 simulations from the ``true'' parameters and take the mean moments of those.
\end{enumerate}

\subsection*{Reference for comparison: Fig 1. of Materials 37}

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_758_nfe_5_gridspacing_uniform_Wdiffs2_100000_Wmid_0_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_758_gridspacing_uniform_Wdiffs2_100000_Wmid_0_16_Jul_2020}}
\caption{Reference figure: Mean estimates for $N=100$, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}


\subsection{Point 3: add expectations series}
\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_N_100_loss_0_nfe_5_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_22_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=0.35]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_22_Jul_2020}}
\caption{Estimates for $N=100$, \colorbox{yellow}{incl. 1-step ahead forecasts of inflation}, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}

I've added measurement error to $\pi,x,i$ and the expectation to avoid stochastic singularity from having 4 observables and only 3 shocks. 


\clearpage
\subsection{Point 4: Mean moments instead of $N$ estimations}

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_25_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\myBiggerFigScale]{\myFigPath autocovariogram_sim_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_25_Jul_2020}}
\caption{Estimates for $N=100$,  incl. 1-step ahead forecasts of inflation, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$, \colorbox{yellow}{single estimation of mean moments from $N$ simulations}}
\end{figure}

This does make a difference and I think it improves on the moments vis-a-vis the $N$ estimations case. However, it converges in the wrong direction with $N=1000$. And it really depends on shocks! A seed of \texttt{rng(2)} instead of \texttt{rng(1)} makes a huge difference at $N=100$, b/c $N=100$ doesn't seem sufficient to wash out the shocks. $N=1000$ seems sufficient though. The ``$N$-estimations'' strategy however is robust to changing the seed. 


\clearpage
\subsection{Point 5: 100 simulations from truth}


\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_100_nfe_5_loss_119431835862_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_26_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\myBiggerFigScale]{\myFigPath autocovariogram_sim_constant_only_pi_only_N_100_nfe_5_loss_119431835862_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_26_Jul_2020}}
\caption{Estimates for $N=100$,  incl. 1-step ahead forecasts of inflation, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$, \colorbox{yellow}{single estimation of mean moments from $N$ simulations, 100 truths}}
\end{figure}
Didn't converge!

\newpage
\subsection{Does the estimation strategy work?}
Try in a simplified setting with
\begin{align}
s_t & = 0.7 s_{t-1} + \epsilon_t \\
y_t & = \alpha b(s_t) \quad \quad \text{piecewise linear approx of state}
\end{align}
Estimate the same $\alpha^{true}$ as before, filtering the same way, calculating 5 moments (autocovariances of $y$) the exact same way.
\begin{itemize}
\item The ``estimate mean moments'' strategy is not robust (gets different, incorrect things depending on seed or initialization, even for 2 knots), just like for my application.
\item The ``estimate $N$ times'' procedure is biased but robust just like in my application: biased everywhere but most in the middle (even for 2 knots)). 
\end{itemize}
$\Rightarrow$ Is there something about the way I compute the moments that renders them uninformative?

\subsubsection{How I compute the moments} 
I first fit a VAR to the BK-filtered data (I use $p$ I estimated in the data, usually 4). Then I rely on Hamilton, p. 266 on (p. 280 Mac), Notes 12, p. 25,  to estimate the autocovariances of a VAR as:
\begin{enumerate}
\item Rewrite the estimated $n$-variable VAR(p) as a VAR(1) as:
\begin{equation}
\xi_t = F\xi_{t-1} + v_t \label{var1}
\end{equation}
and define the VC matrix of the $\xi$ as $\Sigma \equiv \E(\xi_t\xi_t')$. The idea is to estimate $\Sigma$ from the VAR(1) representation and then back out the submatrices we need.
\item Take the square of (\ref{var1}) and take expectations:
\begin{equation}
\Sigma = F\E(\xi_{t-1}\xi_{t-1}')F' + \underbrace{\E(v_tv_t')}_{\equiv Q}
\end{equation}
\item The solution to this is (Hamilton's equation [10.2.18]):
\begin{equation}
vec(\Sigma) = [I_{(np)^2} - F\otimes F]^{-1} vec(Q)
\end{equation}
\item The $j$th autocovariance of the process $y_t$ is given by the first $n$ rows and $n$ columns of $\Sigma_j$:
\begin{equation}
\Sigma_j = F \Sigma_{j-1} \quad = F^j\Sigma \quad \quad \text{(Hamilton's eq. [10.2.20]) and [10.2.21])}
\end{equation}

\end{enumerate}

I've also tried to HP-filter instead of BK-filtering, for a truth with only 2 knots, both  mean moments and $N$ estimations strategies, results unchanged.

One thing I am noticing is that the estimated variance of the moments is very tiny, but only for the synthetic data, not for the real data. This means that the weighting matrix of the quadratic form is on the order of between $10^4$ and $10^{10}$ (lower if higher number of knots). Can this ruin the informativeness of the moments? (For my model, using an identity matrix substantially improves the matching of the moments.) And why is this only happening for synthetic data?

Replacing the piecewise linear approx with a linear rule with a slope coefficient works, but an intercept and a slope coefficient doesn't work to estimate the two parameters. 

\newpage
\subsection{Rescale the GMM weighting matrix}

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_command_GMM_LOMgain_univariate_28_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=0.35]{\myFigPath autocovariogram_sim_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_command_GMM_LOMgain_univariate_28_Jul_2020}}
\caption{Estimates for $N=100$, \colorbox{yellow}{incl. 1-step ahead forecasts of inflation, rescale W}, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}

I take the smallest diagonal element of $W$, the variance matrix of moments. I then rescale $W$ by the order of magnitude of the smallest element. That doesn't seem to do really well, although it does hit some moments better, others worse. It definitely stays closer to the initial values.

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_28_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=0.35]{\myFigPath autocovariogram_sim_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_28_Jul_2020}}
\caption{Estimates for $N=100$, \colorbox{yellow}{incl. 1-step ahead forecasts of inflation, rescale W, estimate mean moments once}, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}

Also here we see it sticking to the initialization clearly. It also became significantly faster (about 1/6th of the time!)

\clearpage
\subsection{Loss for holding $\alpha$s at true values, and varying one at a time}

\begin{figure}[h!]
\subfigure[\colorbox{yellow}{Rescale W}]{\includegraphics[scale=0.35]{\myFigPath loss_for_indi_alphas_others_at_true_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_28_Jul_2020}}
\subfigure[\colorbox{yellow}{Don't rescale W} (Rescaling but dropping the convexity moment looks like this too, except the order of magnitude is $10^{-5}$)]{\includegraphics[scale=0.35]{\myFigPath loss_for_indi_alphas_others_at_true_dontrescale_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_28_Jul_2020}}
\caption{Loss for $N=100$, incl. 1-step ahead forecasts of inflation, estimate mean moments once, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}


%%%%%%%%%%%               BLANK   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% adds blank page
\afterpage{\blankpage}

%%%%%%%%%%%%%%%%%%   FOR ME   %%%%%%%%%%%%
\newpage

\clearpage

\section{For me: Simulated ``true'' data}
\subsection*{3 potential causes to lack of identification in the zero neighborhood}
\begin{enumerate}
\item The distribution of estimates is skewed $\rightarrow$ take $median(\hat{\alpha})$ instead of the mean.
\item The truth is based on a simulation that doesn't favor the zero neighborhood $\rightarrow$ do 100 simulations from the ``true'' parameters and take the mean moments of those.
\item The gain doesn't matter if the forecast error is 0, or very close to it $\rightarrow$ introduce a distinction between the forecast error that's used to choose the gain and the one used to update the coefficients of the learning rule.
\item[+1] Taking mean moments across $N$ histories is more natural than performing the estimation $N$ times.
\item[+2] Introduce expectation series (SPF)
\end{enumerate}

\subsection*{Reference for comparison: Fig 1. of Materials 37}

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_758_nfe_5_gridspacing_uniform_Wdiffs2_100000_Wmid_0_16_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_758_gridspacing_uniform_Wdiffs2_100000_Wmid_0_16_Jul_2020}}
\caption{Reference figure: Mean estimates for $N=100$, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}

\newpage 
\subsection*{Point \#1: skewness $\rightarrow$ take median instead of mean }

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha}), median(\hat{\alpha})$, unsorted median]{\includegraphics[scale=\mySmallFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_758_nfe_5_gridspacing_uniform_Wdiffs2_100000_Wmid_0_20_Jul_2020}}
\caption{Mean estimates for $N=100$, imposing convexity with weight 10K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}
I understand what's happening! Half the estimates are L's, the other half are ``inverted L's'', which is why taking a mean or a classical, sorted median has the tendency to produce these nonmonotonic zigzags.

\subsection*{Point \#2: do 100 truths}
\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_63586357933_nfe_5_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_21_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_63586357933_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_21_Jul_2020}}
\caption{Estimates for $N=100$, \colorbox{yellow}{truth is a mean of 100 simulations}, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}
That didn't help, did it now?

\newpage 
\subsection*{Point \#3:  change timing of forecast errors}
\begin{align}
k_t^{-1} &= \mathbf{g}(fe_{t|t-1}) \label{gaineq}\\
\bar{\pi}_t &= \bar{\pi}_{t-1} + k_t^{-1}fe_{t|t-1} \label{pibeq}
\end{align}
The issue 	seems to be: if $fe_{t|t-1} \approx 0$, then the gain is irrelevant for learning because $fe_{t|t-1}$ figures into both equations. So the idea is to decouple the two equations by changing the timing of one of the forecast errors. Note:
\begin{align}
fe_{t|t-1} &= \pi_t - (\bar{\pi}_{t-1}+bs_{t-1})\\
&= \pi_t - \bar{\pi}_{t-1} \quad \quad \text{since shocks iid and $b$ is the RE transition matrix}
\end{align}
So what I can try is to use an older forecast error in equation (2). Try $fe_{t|t-1}\equiv \pi_t - \bar{\pi}_{t-2}$.
\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_745_nfe_5_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_21_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_745_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_21_Jul_2020}}
\caption{Estimates for $N=100$, \colorbox{yellow}{changing the forecast error timing in the updating equation}, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}

A little more symmetric, but no dramatic improvement. 


\newpage
\subsection*{Point \#+1: do $N$ simulations instead of $N$ estimations}
\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_loss_951_nfe_5_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_20_Jul_2020}}
\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\mySmallerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_nfe_5_loss_951_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_20_Jul_2020}}
\caption{Estimates for $N=100$, \colorbox{yellow}{targeting mean moments in a single estimation instead of $N$ estimations of individual moments}, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}
The difference is striking!

\newpage
\subsection*{Point \#+2: introduce expectations series}
\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_N_100_loss_0_nfe_5_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_22_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=0.35]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_22_Jul_2020}}
\caption{Estimates for $N=100$, \colorbox{yellow}{incl. 1-step ahead forecasts of inflation}, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}

I've added measurement error to $\pi,x,i$ and the expectation to avoid stochastic singularity from having 4 observables and only 3 shocks. 

This clearly has added useful info. Otherwise, behavior is like before:
\begin{itemize}
\item Without the convexity restriction, I still get nonconvex estimate.
\item With the 0 at 0 restriction, I can match the 0 region, otherwise I can't.
\item Both restrictions lead to basically identical moments.
%\item $N=1000$ doesn't give drastic improvements either, and in covariograms, not at all.
\end{itemize}

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_N_1000loss_0_nfe_5_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_22_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\myBiggerFigScale]{\myFigPath command_GMM_LOMgain_univariate_autocovariogram_sim_N_1000nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_22_Jul_2020}}
\caption{Estimates for \colorbox{yellow}{$N=1000$,  incl. 1-step ahead forecasts of inflation}, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}

\subsubsection*{What's missing}
To me it seems that the reason we're still underidentified is that the $(-1,1)$-forecast error region (possibly an even bigger, $(-1.5, 1.5)$-region) doesn't produce variation in $\bar{\pi}$ (see Eq. \ref{pibeq}). So let's try to replace the forecast error in the generation of the gain (Eq. \ref{gaineq}) by say the forecast error of the output gap. (Needs the constant-only PLM.) $\rightarrow$ Doesn't work at all, I guess b/c it doesn't correspond to the DGP.

$\rightarrow$ Really wonder if the anchoring function specified in terms of changes, not levels of the gain would help! If instead of equation \ref{gaineq} in system \ref{gaineq}-\ref{pibeq}, we'd have
\begin{align}
k_t^{-1} &= \mathbf{g}(k_{t-1}^{-1},fe_{t|t-1}) \label{gaineqchanges}
\end{align}

No, even that wouldn't work b/c the estimation routine wouldn't be able to discriminate between two points on the line $(k^{-1}_{(i)}, fe \in (-1,1))$, for $\forall i$ in the $k$-space.

$\rightarrow$ I'm increasingly thinking that that region cannot be identified at all because it simply doesn't matter for the evolution of long-run expectations.

2 options:
\begin{enumerate}
\item Select gridpoints strictly outside the $(-1,1)$-region, and use the convexity restriction to interpolate inside the region.
\item Impose the 0 at 0 restriction at the point at 0, and select the rest of the gridpoints outside the  $(-1,1)$-region.
\end{enumerate}
If my conjecture is correct, both of these approaches should be identified.
\clearpage
\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$,  \colorbox{yellow}{4 knots: (-2,-1,1,2)}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_100_nfe_4_loss_0_gridspacing_manual_Wdiffs2_100000_Wmid_0_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
%\hfill
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$, \colorbox{yellow}{5 knots: (-2,-1, 0, 1,2)}, \newline 0 at 0 restriction imposed with weight 1K]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_1000_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$, \colorbox{yellow}{5 knots: (-2,-1, 0, 1,2)}, \newline 0 at 0 restriction imposed with weight 1K,  \colorbox{yellow}{convexity restriction removed}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_0_Wmid_1000_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
\caption{Estimates for $N=100$, incl. 1-step ahead forecasts of inflation, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}

Unfortunately, it seems to me like even outside the trouble region, we're not identified. I say that because the 0 at 0 assumption on its own should have no bearing on the coefficients out in the tails of the forecast error space. But also there, even with a high $N$, I'm not nailing the truth.

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$,  \colorbox{yellow}{4 knots: (-2,-1,1,2)}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_1000_nfe_4_loss_0_gridspacing_manual_Wdiffs2_100000_Wmid_0_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
%\hfill
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$, \colorbox{yellow}{5 knots: (-2,-1, 0, 1,2)}, \newline 0 at 0 restriction imposed with weight 1K]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_1000_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_1000_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$, \colorbox{yellow}{5 knots: (-2,-1, 0, 1,2)}, \newline 0 at 0 restriction imposed with weight 1K,  \colorbox{yellow}{convexity restriction removed} ]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_1000_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_0_Wmid_1000_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
\caption{Estimates for \colorbox{yellow}{$N=1000$}, incl. 1-step ahead forecasts of inflation, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
\end{figure}

\clearpage

\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_25_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\myBiggerFigScale]{\myFigPath autocovariogram_sim_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_25_Jul_2020}}
\caption{Estimates for $N=100$,  incl. 1-step ahead forecasts of inflation, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$, \colorbox{yellow}{single estimation of mean moments from $N$ simulations}}
\end{figure}

This does make a difference and I think it improves on the moments vis-a-vis the $N$ estimations case. However, it converges in the wrong direction with $N=1000$. And it really depends on shocks! A seed of \texttt{rng(2)} instead of \texttt{rng(1)} makes a huge difference at $N=100$, b/c $N=100$ doesn't seem sufficient to wash out the shocks. $N=1000$ seems sufficient though. The ``$N$-estimations'' strategy however is robust to changing the seed. 

\clearpage
\begin{figure}[h!]
\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_pi_only_N_100_nfe_5_loss_119431835862_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_26_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\myBiggerFigScale]{\myFigPath autocovariogram_sim_constant_only_pi_only_N_100_nfe_5_loss_119431835862_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_command_GMM_LOMgain_univariate_26_Jul_2020}}
\caption{Estimates for $N=100$,  incl. 1-step ahead forecasts of inflation, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$, \colorbox{yellow}{single estimation of mean moments from $N$ simulations, 100 truths}}
\end{figure}
Didn't converge!

%%%%%%%%%%%%%%     REAL DATA   %%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Real data with the SPF}

\begin{figure}[h!]
\subfigure[$\hat{\alpha}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\myBiggerFigScale]{\myFigPath autocovariogram_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
\caption{Estimates for $N=1000$, \colorbox{yellow}{ incl. SPF 1-step ahead forecasts of inflation}, imposing convexity with weight 100K, $nfe=5, fe \in(-2,2)$}
\end{figure}

\begin{figure}[h!]
\subfigure[$\hat{\alpha}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_1000_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_0_Wmid_1000_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\myBiggerFigScale]{\myFigPath autocovariogram_constant_only_pi_only_N_1000_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_0_Wmid_1000_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
\caption{Estimates for $N=1000$, $nfe=5, fe \in(-2,2)$, \colorbox{yellow}{ incl. SPF 1-step ahead forecasts of inflation}, \colorbox{yellow}{removing convexity restriction, imposing 0 at 0 with weight 1K}}
\end{figure}

\begin{figure}[h!]
\subfigure[$\hat{\alpha}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_1000_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_command_GMM_LOMgain_univariate_24_Jul_2020}}
%\hfill
\subfigure[Autocovariogram]{\includegraphics[scale=\myBiggerFigScale]{\myFigPath autocovariogram_constant_only_pi_only_N_1000_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_command_GMM_LOMgain_univariate_24_Jul_2020}}
\caption{Estimates for $N=1000$, \colorbox{yellow}{ incl. SPF 1-step ahead forecasts of inflation}, imposing convexity with weight 100K, 5 knots, \colorbox{yellow}{ $fe \in(-5,5)$}}
\end{figure}

\clearpage
\subsection{Choose one real data specification for ZEW application draft}

\begin{figure}[h!]
\subfigure[$\hat{\alpha}$,  \colorbox{yellow}{Don't scale W}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_scaleW_0_command_GMM_LOMgain_univariate_29_Jul_2020}}
\subfigure[$\hat{\alpha}$,  \colorbox{yellow}{Scale W}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nsimulations_scaleW_1_command_GMM_LOMgain_univariate_29_Jul_2020}}
\subfigure[$\hat{\alpha}$,  \colorbox{yellow}{Don't scale W}, \colorbox{yellow}{0 at 0 imposed with weight 1000}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_100_nfe_5femax_2_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_1000_Nsimulations_scaleW_0_command_GMM_LOMgain_univariate_29_Jul_2020}}
\subfigure[$\hat{\alpha}$,  \colorbox{yellow}{Scale W},  \colorbox{yellow}{0 at 0 imposed with weight 1000}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_100_nfe_5femax_2_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_1000_Nsimulations_scaleW_1_command_GMM_LOMgain_univariate_29_Jul_2020}}
%\hfill
\caption{Estimates for $N=100$, incl. SPF 1-step ahead forecasts of inflation, imposing convexity with weight 100K, 5 knots, $fe \in(-2,2)$, $N$ simulations to estimate mean moments once}
\end{figure}

\begin{figure}[h!]
\subfigure[$\hat{\alpha}$,  \colorbox{yellow}{Don't scale W}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_1000_Nsimulations_scaleW_0_command_GMM_LOMgain_univariate_29_Jul_2020}}
\subfigure[$\hat{\alpha}$,  \colorbox{yellow}{Scale W}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_100_nfe_5femax_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_1000_Nsimulations_scaleW_1_command_GMM_LOMgain_univariate_29_Jul_2020}}

\caption{Estimates for $N=100$, incl. SPF 1-step ahead forecasts of inflation, imposing convexity with weight 100K, 0 at 0 with weight 1000, 5 knots, $fe \in(-5,5)$}
\end{figure}

\clearpage
\begin{figure}[h!]
\subfigure[$\hat{\alpha}$,  \colorbox{yellow}{$fe \in(-2,2)$}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_1000_nfe_5femax_2_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_1000_Nsimulations_scaleW_0_command_GMM_LOMgain_univariate_29_Jul_2020}}
\subfigure[$\hat{\alpha}$,  \colorbox{yellow}{$fe \in(-5,5)$}]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alph_opt_constant_only_pi_only_N_1000_nfe_5femax_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_1000_Nsimulations_scaleW_0_command_GMM_LOMgain_univariate_29_Jul_2020}}
\caption{Estimates for $N=1000$ using chosen specifications: incl. SPF 1-step ahead forecasts of inflation, imposing convexity with weight 100K, 0 at 0 with weight 1000, 5 knots, don't scale W}
\end{figure}
Panel a took 70 min, Panel b 340 min!

I think they're plain awful, so I'm just gonna use the Materials 37 results, where I at least already have done the PEA and VFI. 
%\begin{figure}[h!]
%\subfigure[$\alpha^{true}, \alpha_0, mean(\hat{\alpha})$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath alphas_constant_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
%%\hfill
%\subfigure[Autocovariogram]{\includegraphics[scale=\myBiggerFigScale]{\myFigPath autocovariogram_sim_constant_only_N_100_nfe_5_loss_0_gridspacing_uniform_Wdiffs2_100000_Wmid_0_Nestimations_command_GMM_LOMgain_univariate_23_Jul_2020}}
%\caption{Estimates for $N=100$, \colorbox{yellow}{ incl. 1-step ahead forecasts of inflation, constant-only PLM}, imposing convexity with weight 100K, truth with $nfe=5, fe \in(-2,2)$}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                              APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \clearpage
%    \newpage
\appendix
% the following command makes equation numbering include the section first, but just for what follows
\numberwithin{equation}{section}
\section{Model summary}

\vspace{-0.5cm}

\begin{align}
x_t &=  -\sigma i_t +\hat{\E}_t \sum_{T=t}^{\infty} \beta^{T-t }\big( (1-\beta)x_{T+1} - \sigma(\beta i_{T+1} - \pi_{T+1}) +\sigma r_T^n \big)  \label{A1}  \\
\pi_t &= \kappa x_t +\hat{\E}_t \sum_{T=t}^{\infty} (\alpha\beta)^{T-t }\big( \kappa \alpha \beta x_{T+1} + (1-\alpha)\beta \pi_{T+1} + u_T\big) \label{A2}  \\
i_t &= \psi_{\pi}\pi_t + \psi_{x} x_t  + \bar{i}_t \label{TR} \quad \quad (\text{if imposed})
\end{align}

\vspace{-1.2cm}

\begin{align}
\text{PLM:} \quad \quad & \hat{\E}_t z_{t+h}  =  a_{t-1} + bh_x^{h-1}s_t  \quad \forall h\geq 1 \quad \quad b = g_x\; h_x \quad \quad  \label{PLM} \\
\text{Updating:} \quad \quad & a_{t}  =a_{t-1} +k_t^{-1}\big(z_{t} -(a_{t-1}+b s_{t-1}) \big)  \label{A5} \\
\text{Anchoring function:} \quad \quad & k^{-1}_t  = \rho_k k^{-1}_{t-1} + \gamma_k fe_{t-1}^2 \label{A6}\\
\text{Forecast error:} \quad \quad & fe_{t-1}  = z_t - (a_{t-1}+b s_{t-1}) \label{A7} \\
\text{LH expectations:} \quad \quad & f_a(t) = \frac{1}{1-\alpha\beta}a_{t-1}  + b(\mathbb{I}_{nx} - \alpha\beta h)^{-1}s_t \quad \quad  f_b(t) = \frac{1}{1-\beta}a_{t-1}  + b(\mathbb{I}_{nx} - \beta h)^{-1}s_t  \label{A8}
\end{align}

\vspace{-0.5cm}

This notation captures vector learning ($z$ learned) for intercept only. For scalar learning, $a_t= \begin{pmatrix} \bar{\pi}_t & 0 & 0\end{pmatrix}' $ and $b_1$ designates the first row of $b$. The observables $(\pi, x)$ are determined as:
\begin{align}
x_t &=  -\sigma i_t + \begin{bmatrix} \sigma & 1-\beta & -\sigma\beta \end{bmatrix} f_b + \sigma \begin{bmatrix} 1 & 0 & 0 \end{bmatrix} (\mathbb{I}_{nx} - \beta h_x)^{-1} s_t \label{A9} \\
\pi_t &= \kappa x_t  + \begin{bmatrix} (1-\alpha)\beta & \kappa\alpha\beta & 0 \end{bmatrix}  f_a + \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}  (\mathbb{I}_{nx} - \alpha \beta h_x)^{-1}  s_t \label{A10}
\end{align}

\section{Target criterion}\label{target_crit_levels}
The target criterion in the simplified model (scalar learning of inflation intercept only, $k_t^{-1} = \mathbf{g}(fe_{t-1})$):
\begin{align*}
\pi_t  = -\frac{\lambda_x}{\kappa}\bigg\{x_t - \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+((\pi_t - \bar{\pi}_{t-1}-b_1 s_{t-1}))\mathbf{g}_{\pi}(t) \bigg) \\
\bigg(\E_t\sum_{i=1}^{\infty}x_{t+i}\prod_{j=0}^{i-1}(1-k_{t+1+j}^{-1} - (\pi_{t+1+j} - \bar{\pi}_{t+j}-b_1 s_{t+j})\mathbf{g_{\bar{\pi}}}(t+j)) \bigg)
\bigg\} \numberthis \label{target}
\end{align*}
where I'm using the notation that $\prod_{j=0}^{0} \equiv 1$. For interpretation purposes, let me rewrite this as follows:
\begin{align*}
\pi_t  = & \; \textcolor{red}{-\frac{\lambda_x}{\kappa} x_t} \textcolor{blue}{ \; + \frac{\lambda_x}{\kappa} \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+ fe^{eve}_{t|t-1}\mathbf{g}_{\pi}(t) \bigg)\E_t\sum_{i=1}^{\infty}x_{t+i}}  \\
& \textcolor{mygreen}{- \frac{\lambda_x}{\kappa} \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+ fe^{eve}_{t|t-1}\mathbf{g}_{\pi}(t) \bigg) \bigg(\E_t\sum_{i=1}^{\infty}x_{t+i}\prod_{j=0}^{i-1}(k_{t+1+j}^{-1} + fe^{eve}_{t+1+j|t+j})\mathbf{g_{\bar{\pi}}}(t+j) \bigg)}
\numberthis \label{target_interpretation}
\end{align*}
Interpretation: \textcolor{red}{tradeoffs from discretion in RE} + \textcolor{blue}{effect of current level and change of the gain on future tradeoffs} + \textcolor{mygreen}{effect of future expected levels and changes of the gain on future tradeoffs}

%\section{A target criterion system for an anchoring function specified for gain changes}\label{target_crit_changes}
%\begin{equation}
%k_t = k_{t-1} + \mathbf{g}(fe_{t|t-1})
%\end{equation}
%Turns out the $k_{t-1}$ adds one $\varphi_{6,t+1}$ too many which makes the target criterion unwieldy. The FOCs of the Ramsey problem are
%\begin{align}
%& 2\pi_t + 2\frac{\lambda}{\kappa}x_t -k_t^{-1} \varphi_{5,t} - \mathbf{g}_{\pi}(t)\varphi_{6,t}  = 0 \label{gaspar22}\\
%& c x_{t+1} + \varphi_{5,t} -(1-k_t^{-1})\varphi_{5,t+1} +\mathbf{g}_{\bar{\pi}}(t)\varphi_{6,t+1} = 0 \label{gaspar21}\\
%& \varphi_{6,t} \; \textcolor{red}{+\; \varphi_{6,t+1}} = fe_t \varphi_{5,t} \label{constraints}
%\end{align}
%where the red multiplier is the new element vis-a-vis the case where the anchoring function is specified in levels ($k_t^{-1} = \mathbf{g}(fe_{t-1})$, as in App. \ref{target_crit_levels}), and I'm using the shorthand notation
%\begin{align}
%c & = -\frac{2(1-\alpha)\beta}{1-\alpha\beta}\frac{\lambda}{\kappa} \\ 
%fe_t & = \pi_t - \bar{\pi}_{t-1}-b s_{t-1}
%\end{align}
%(\ref{gaspar22}) says that in anchoring, the discretion tradeoff is complemented with tradeoffs coming from learning ($\varphi_{5,t}$), which are more binding when expectations are unanchored ($k_{t}^{-1}$ high). Moreover, the change in the anchoring of expectations imposes an additional constraint ($\varphi_{6,t}$), which is more strongly binding if the gain responds strongly to inflation ($\mathbf{g}_{\pi}(t)$).
%One can simplify this three-equation-system to:
%\begin{align}
%\varphi_{6,t} & = -c fe_t x_{t+1} + \bigg(1+ \frac{fe_t}{fe_{t+1}}(1-k_{t+1}^{-1}) -fe_t \mathbf{g}_{\bar{\pi}}(t) \bigg) \varphi_{6,t+1} -\frac{fe_t}{fe_{t+1}}(1-k_{t+1}^{-1})\varphi_{6,t+2}\label{6'} \\
%0 & = 2\pi_t + 2\frac{\lambda}{\kappa}x_t   - \bigg( \frac{k_t^{-1}}{fe_t} + \mathbf{g}_{\pi}(t)\bigg)\varphi_{6,t} + \frac{k_t^{-1}}{fe_t}\varphi_{6,t+1}\label{1'}
%\end{align}
%Unfortunately, I haven't been able to solve (\ref{6'}) for $\varphi_{6,t}$ and therefore I can't express the target criterion so nicely as before. The only thing I can say is to direct the targeting rule-following central bank to compute $\varphi_{6,t}$ as the solution to (\ref{1'}), and then evaluate (\ref{6'}) as a target criterion. The solution to (\ref{1'}) is given by:
%\begin{equation}
%\varphi_{6,t} = -2\E_t\sum_{i=0}^{\infty}(\pi_{t+i}+\frac{\lambda_x}{\kappa}x_{t+i})\prod_{j=0}^{i-1}\frac{\frac{k_{t+j}^{-1}}{fe_{t+j}}}{\frac{k_{t+j}^{-1}}{fe_{t+j}} + \mathbf{g}_{\pi}(t+j)} \label{sol1'}
%\end{equation}
%Interpretation: the anchoring constraint is not binding ($\varphi_{6,t}=0$) if the CB always hits the target (
%$\pi_{t+i}+\frac{\lambda_x}{\kappa}x_{t+i} = 0 \quad \forall i$); or expectations are always anchored ($k_{t+j}^{-1}=0 \quad \forall j$). 



%%%%%%%%%%     IRFS                %%%%%%%%%%%%%%%%
\clearpage
\section{Impulse responses to iid monpol shocks across a wide range of learning models}
$T=400, N=100, n_{drop}=5,$ shock imposed at $t=25$, calibration as above, Taylor rule assumed to be known, PLM = learn constant only, of inflation only.

\begin{figure}[h!]
\subfigure[Decreasing gain learning]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_RIR_LH_monpol_dgain_constant_only_pi_only_2020_07_07}}
\hfill
\subfigure[Mean gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_invgain_dgain_constant_only_pi_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_05_lami_0_2020_07_07}}
\subfigure[Constant gain learning]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_RIR_LH_monpol_cgain_constant_only_pi_only_2020_07_07}}
\hfill
\subfigure[Mean gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_invgain_cgain_constant_only_pi_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_05_lami_0_2020_07_07}}
\subfigure[CEMP criterion (vector)]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_RIR_LH_monpol_again_critCEMP_constant_only_pi_only_2020_07_07}}
\hfill
\subfigure[Mean gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_invgain_again_critCEMP_constant_only_pi_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_05_lami_0_2020_07_07}}
\caption{IRFs and gain history (sample means) }
\end{figure}


\begin{figure}[h!]
\subfigure[CUSUM criterion (vector)]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_RIR_LH_monpol_again_critCUSUM_constant_only_pi_only_2020_07_07}}
\hfill
\subfigure[Mean gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_invgain_again_critCUSUM_constant_only_pi_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_05_lami_0_2020_07_07}}
\subfigure[Smooth criterion, approximated, using $\alpha^{true}= (0.05;0.025;0;0.025;0.05)$, on $fe \in (-2,2)$.]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_RIR_LH_monpol_again_critsmooth_constant_only_pi_only_2020_07_07}}
\hfill
\subfigure[Mean gain]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_IRFs_approx_pretty_invgain_again_critsmooth_constant_only_pi_only_params_psi_pi_1_5_psi_x_0_gbar_0_145_thetbar_16_thettilde_2_5_kap_0_8_lamx_0_05_lami_0_2020_07_07}}

\caption{IRFs and gain history (sample means), continued }
\end{figure}


\end{document}

%%%%%%%%%%%%%    SUBFIGURE  %%%%%%%%%%%
%\begin{figure}[h!]
%\subfigure[Hodrick-Prescott, $\lambda=1600$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath materials22_gain_dhat_HP}}
%\hfill % this is great to intro dpace between subfigures
%\subfigure[Hamilton, 4 lags, $h=8$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath materials22_gain_dhat_Hamilton}}
%\subfigure[Baxter-King, $(6,32)$ quarters, truncation at 12 lags]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath materials22_gain_dhat_BK}}
%\caption{Inverse gain for $\hat{d}$ for the different filters}
%\end{figure}

%%%%%%%%%%%%%    TABLE  %%%%%%%%%%%
%\begin{center}
%\begin{table}[h!]
%\caption{$\hat{d}$}
%\begin{tabular}{ c |c |c }
%  & $W = I$ & $W = \text{diag}(\hat{\sigma}_{ac(0)}, \dots, \hat{\sigma}_{ac(K)})$ \\ 
%  \hline
% HP & 77.7899 & 10 \\  
% \hline
% Hamilton & 32.1649 & 10 \\  
% \hline
% BK & 90.3929 & 10    
%\end{tabular}
%\end{table}
%\end{center}





