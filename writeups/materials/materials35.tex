\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb,lscape, natbib}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{setspace}
\usepackage[final]{pdfpages}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm, bottom=2.5cm]{geometry}
\usepackage{natbib} 
\usepackage{bibentry} 
\newcommand{\bibverse}[1]{\begin{verse} \bibentry{#1} \end{verse}}
\newcommand{\vs}{\vspace{.3in}}
\renewcommand{\ni}{\noindent}
\usepackage{xr-hyper}
\usepackage[]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}
\usepackage[capposition=top]{floatrow}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
\usepackage{tikz}
 
\pagestyle{fancy} % customize header and footer
\fancyhf{} % clear initial header and footer
%\rhead{Overleaf}
\lhead{\centering \rightmark} % this adds subsection number and name
\lfoot{\centering \rightmark} 
\rfoot{\thepage} % put page number (the centering command puts it in the middle, don't matter if you put it in right or left footer)

\def \myFigPath {../../figures/} 
% BE CAREFUL WITH FIGNAMES, IN LATEX THEY'RE NOT CASE SENSITIVE!!
\def \myTablePath {../../tables/} 

%\definecolor{mygreen}{RGB}{0, 100, 0}
\definecolor{mygreen}{RGB}{0, 128, 0}

\definecolor{citec}{rgb}{0,0,.5}
\definecolor{linkc}{rgb}{0,0,.6}
\definecolor{bcolor}{rgb}{1,1,1}
\hypersetup{
%hidelinks = true
  colorlinks = true,
  urlcolor=linkc,
  linkcolor=linkc,
  citecolor = citec,
  filecolor = linkc,
  pdfauthor={Laura G\'ati},
}


\geometry{left=.83in,right=.89in,top=1in,
bottom=1in}
\linespread{1.5}
\renewcommand{\[}{\begin{equation}}
\renewcommand{\]}{\end{equation}}

% New Options
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

%\newtheorem{theorem}{Theorem}[section] % the third argument specifies that their number will be adopted to the section
%\newtheorem{corollary}{Corollary}[theorem]
%\newtheorem{lemma}[theorem]{Lemma}
%\declaretheorem{proposition}
%\linespread{1.3}
%\raggedbottom
%\font\reali=msbm10 at 12pt

% New Commands
\newcommand{\real}{\hbox{\reali R}}
\newcommand{\realp}{\hbox{\reali R}_{\scriptscriptstyle +}}
\newcommand{\realpp}{\hbox{\reali R}_{\scriptscriptstyle ++}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\argmin}{arg\,min}
\newcommand\w{3.0in}
\newcommand\wnum{3.0}
\def\myFigWidth{5.3in}
\def\mySmallerFigWidth{2.1in}
\def\myEvenBiggerFigScale{0.8}
\def\myPointSixFigScale{0.6}
\def\myBiggerFigScale{0.4}
\def\myFigScale{0.3}
\def\myMediumFigScale{0.25}
\def\mySmallFigScale{0.22}
\def\mySmallerFigScale{0.18}
\def\myTinyFigScale{0.16}
\def\myPointFourteenFigScale{0.14}
\def\myTinierFigScale{0.12}
\def\myAdjustableFigScale{0.16}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % this defines a command to make align only number this line
\newcommand{\code}[1]{\texttt{#1}} %code %

\renewcommand*\contentsname{Overview}
\setcounter{tocdepth}{2}

% define a command to make a huge question mark (it works in math mode)
\newcommand{\bigqm}[1][1]{\text{\larger[#1]{\textbf{?}}}}

\begin{document}

\linespread{1.0}

\title{Materials 35 - ...And still estimating the anchoring function}
\author{Laura G\'ati} 
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%             DOCUMENT           %%%%%%%%%%%%%%%%%% 

\tableofcontents

%\listoffigures

\newpage
\section{Estimation procedure}
Instead of the AR(1) anchoring function used so far (Equation \ref{A6}), I use the following equation
\begin{equation}
k_t^{-1} = \alpha s(X)
\end{equation}
where $X = (k^{-1}_{t-1}, fe_{t|t-1})$ and I use piecewise linear interpolation. I initialize $\alpha_0$ by specifying a grid for $X$, passing the grid through Equation (\ref{A6}) to generate $k^{-1}_t$-values, and approximating by fitting the grid to the $k^{-1}_t$-values.  See Fig. \ref{fig_initial_anchor_fct}.

Then I estimate $\alpha$ using GMM, targeting the autocovariance structure of inflation, the output gap and the nominal interest rate (federal funds rate) in the data. 

\begin{figure}[h!]
\includegraphics[scale=0.17]{\myFigPath command_GMM_LOMgain_initial_approx_17_Jun_2020}
\caption{Initialization via Equation (\ref{A6}) implies this functional relationship}
\label{fig_initial_anchor_fct}
\end{figure}

$T=233$ before BK-filtering, $T=209$ after BK-filtering. Using the ``constant-only, inflation-only'' learning PLM. I drop the $ndrop=5$ initial values. I restrict $\alpha \in (0,1)$, the support of $k^{-1}$ in the grid. I target the lag $0,\dots,4$ autocovariance matrices, dropping repeated entries at lag 0, leaving me with 42 moments.

\clearpage
\section{Simulated data, 1D function}

\subsection{Evaluate loss on a $6^6=46656$ grid}
\begin{figure}[h!]
\subfigure[Full grid and excluding the highest 10]{\includegraphics[scale=\mySmallFigScale]{\myFigPath materials35_6by6_loss_25_Jun_2020}}
\hfill
\subfigure[The lowest 10 losses]{\includegraphics[scale=\myTinyFigScale]{\myFigPath materials35_6by6_loss_10smallest25_Jun_2020}}
\caption{Objective function values on a grid with values $(0;    0.2;    0.4;    0.6;    0.8;   1)$}
\end{figure}

\begin{figure}[h!]
\subfigure[True coefficients]{\includegraphics[scale=\mySmallFigScale]{\myFigPath materials341D_alphas_true22_Jun_2020}}
\hfill
\subfigure[Highest loss]{\includegraphics[scale=\mySmallFigScale]{\myFigPath materials35_largest_alphas25_Jun_2020}}
\hfill
\subfigure[Lowest loss]{\includegraphics[scale=\mySmallFigScale]{\myFigPath materials35_smallest_alphas25_Jun_2020}}
\caption{The $\alpha$s with highest and lowest objective function values}
\end{figure}

\clearpage
\subsection{$\alpha^{true} \in(0,0.1)$ and convex}
\begin{figure}[h!]
\subfigure[$\alpha^{true} = ( 0.1;    0.05;    0.001;    0.00001;    0.02;    0.09)$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_acf_sim_data_univariate_alph_true_24_Jun_2020}}
\hfill
\subfigure[Gain for a simulation using $\alpha^{true}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_acf_sim_data_univariate_true_gain_sim_constant_only_pi_only_24_Jun_2020}}
\subfigure[$\alpha^{true}, \alpha_0, \hat{\alpha}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_24_Jun_2020}}
\hfill
\subfigure[Moments]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_ACFs_24_Jun_2020}}
\caption{Effects of making the truth i) convex ii) between 0 and 0.1}
\end{figure}

Actually, this improves the solver's ability to get close (up to $\alpha(fe=-5)$) dramatically! But this indicates the finite-element issue: there might not be any data in the -5 range for the forecast error. If that's so, then redoing this spiel with the true data involving a smaller forecast error support should do the trick. 

\begin{figure}[h!]
\subfigure[$\alpha^{true} = ( 0.1;    0.05;    0.001;    0.00001;    0.02;    0.09)$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_acf_sim_data_univariate_alph_true_25_Jun_2020}}
\hfill
\subfigure[Gain for a simulation using $\alpha^{true}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_acf_sim_data_univariate_true_gain_sim_constant_only_pi_only_25_Jun_2020}}
\subfigure[$\alpha^{true}, \alpha_0, \hat{\alpha}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_alphas_25_Jun_2020}}
\hfill
\subfigure[Moments]{\includegraphics[scale=\myTinyFigScale]{\myFigPath command_GMM_LOMgain_univariate_ACFs_25_Jun_2020}}
\caption{Effects of making the truth i) convex ii) between 0 and 0.1 iii) shrinking the true forecast error support to $(-3.5,3.5)$}
\end{figure}

BAM! That did it! PTO.

\clearpage
\subsection{100000 starting points, top 10 minima}

\begin{figure}[h!]
\subfigure[Policy function]{\includegraphics[scale=\myTinyFigScale]{\myFigPath placeholder}}
\hfill
\subfigure[Residuals]{\includegraphics[scale=\myTinyFigScale]{\myFigPath placeholder}}
\subfigure[Loss function]{\includegraphics[scale=\myTinyFigScale]{\myFigPath placeholder}}
\hfill
\subfigure[Moments]{\includegraphics[scale=\myTinyFigScale]{\myFigPath placeholder}}
\caption{Top 10 candidates}
\end{figure}

\subsection{Add moments}
\subsubsection{Strict priors: anchoring function should be convex}
\subsubsection{Calibrated moments: e.g. average gain in simulation should be 0.05}

Turning to the real data, I've found that 
\begin{itemize}
\item without (and also with!) additional moments, data is able identify 5 parameters;
\item the convexity moment forces the solution to be convex (otherwise it is often, but not often not convex);
\item the mean moment helps pinning down the solution when the convexity moment is in place.
\end{itemize}
Such a solution seems quite robust to starting points (there are some starting points that lead the solver not to converge). The solution looks like this:
\clearpage
\begin{figure}[h!]
\includegraphics[scale=\mySmallFigScale]{\myFigPath command_GMM_LOMgain_univariate_alph_opt_27_Jun_2020}
\caption{The candidate solution}
\floatfoot{$n_{\alpha}=5, fe\in(-3.5,3.5), \alpha\in(0,1)$, convexity and mean moment imposed, starting point is given by the AR(1) gain function on this grid. $\hat{\alpha}=(0.7696;    0.0026;    0;    0.0058;    0.0107)$}
\end{figure}

\subsection{But is it robust?}

\begin{figure}[h!]
\includegraphics[scale=\mySmallFigScale]{\myFigPath placeholder}
\caption{Converged solutions for 10 runs}
\floatfoot{$n_{\alpha}=5, fe\in(-3.5,3.5), \alpha\in(0,1)$, convexity and mean moment imposed, from different initial points between (0,1).}
\end{figure}

\clearpage

 


%\begin{figure}[h!]
%\subfigure[]{\includegraphics[scale=\myTinyFigScale]{\myFigPath placeholder}}
%\hfill
%\subfigure[$\hat{\alpha}$]{\includegraphics[scale=\myTinyFigScale]{\myFigPath placeholder}}
%\subfigure[Residuals]{\includegraphics[scale=\myTinyFigScale]{\myFigPath placeholder}}
%\hfill
%\subfigure[Moments]{\includegraphics[scale=\myTinyFigScale]{\myFigPath placeholder}}
%\caption{caption}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                              APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \clearpage
%    \newpage
\appendix
% the following command makes equation numbering include the section first, but just for what follows
\numberwithin{equation}{section}
\section{Model summary}

\vspace{-0.5cm}

\begin{align}
x_t &=  -\sigma i_t +\hat{\E}_t \sum_{T=t}^{\infty} \beta^{T-t }\big( (1-\beta)x_{T+1} - \sigma(\beta i_{T+1} - \pi_{T+1}) +\sigma r_T^n \big)  \label{A1}  \\
\pi_t &= \kappa x_t +\hat{\E}_t \sum_{T=t}^{\infty} (\alpha\beta)^{T-t }\big( \kappa \alpha \beta x_{T+1} + (1-\alpha)\beta \pi_{T+1} + u_T\big) \label{A2}  \\
i_t &= \psi_{\pi}\pi_t + \psi_{x} x_t  + \bar{i}_t \label{TR} \quad \quad (\text{if imposed})
\end{align}

\vspace{-1.2cm}

\begin{align}
\text{PLM:} \quad \quad & \hat{\E}_t z_{t+h}  =  a_{t-1} + bh_x^{h-1}s_t  \quad \forall h\geq 1 \quad \quad b = g_x\; h_x \quad \quad  \label{PLM} \\
\text{Updating:} \quad \quad & a_{t}  =a_{t-1} +k_t^{-1}\big(z_{t} -(a_{t-1}+b s_{t-1}) \big)  \label{A5} \\
\text{Anchoring function:} \quad \quad & k^{-1}_t  = \rho_k k^{-1}_{t-1} + \gamma_k fe_{t-1}^2 \label{A6}\\
\text{Forecast error:} \quad \quad & fe_{t-1}  = z_t - (a_{t-1}+b s_{t-1}) \label{A7} \\
\text{LH expectations:} \quad \quad & f_a(t) = \frac{1}{1-\alpha\beta}a_{t-1}  + b(\mathbb{I}_{nx} - \alpha\beta h)^{-1}s_t \quad \quad  f_b(t) = \frac{1}{1-\beta}a_{t-1}  + b(\mathbb{I}_{nx} - \beta h)^{-1}s_t  \label{A8}
\end{align}

\vspace{-0.5cm}

This notation captures vector learning ($z$ learned) for intercept only. For scalar learning, $a_t= \begin{pmatrix} \bar{\pi}_t & 0 & 0\end{pmatrix}' $ and $b_1$ designates the first row of $b$. The observables $(\pi, x)$ are determined as:
\begin{align}
x_t &=  -\sigma i_t + \begin{bmatrix} \sigma & 1-\beta & -\sigma\beta \end{bmatrix} f_b + \sigma \begin{bmatrix} 1 & 0 & 0 \end{bmatrix} (\mathbb{I}_{nx} - \beta h_x)^{-1} s_t \label{A9} \\
\pi_t &= \kappa x_t  + \begin{bmatrix} (1-\alpha)\beta & \kappa\alpha\beta & 0 \end{bmatrix}  f_a + \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}  (\mathbb{I}_{nx} - \alpha \beta h_x)^{-1}  s_t \label{A10}
\end{align}

\section{Target criterion}\label{target_crit_levels}
The target criterion in the simplified model (scalar learning of inflation intercept only, $k_t^{-1} = \mathbf{g}(fe_{t-1})$):
\begin{align*}
\pi_t  = -\frac{\lambda_x}{\kappa}\bigg\{x_t - \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+((\pi_t - \bar{\pi}_{t-1}-b_1 s_{t-1}))\mathbf{g}_{\pi}(t) \bigg) \\
\bigg(\E_t\sum_{i=1}^{\infty}x_{t+i}\prod_{j=0}^{i-1}(1-k_{t+1+j}^{-1} - (\pi_{t+1+j} - \bar{\pi}_{t+j}-b_1 s_{t+j})\mathbf{g_{\bar{\pi}}}(t+j)) \bigg)
\bigg\} \numberthis \label{target}
\end{align*}
where I'm using the notation that $\prod_{j=0}^{0} \equiv 1$. For interpretation purposes, let me rewrite this as follows:
\begin{align*}
\pi_t  = & \; \textcolor{red}{-\frac{\lambda_x}{\kappa} x_t} \textcolor{blue}{ \; + \frac{\lambda_x}{\kappa} \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+ fe^{eve}_{t|t-1}\mathbf{g}_{\pi}(t) \bigg)\E_t\sum_{i=1}^{\infty}x_{t+i}}  \\
& \textcolor{mygreen}{- \frac{\lambda_x}{\kappa} \frac{(1-\alpha)\beta}{1-\alpha\beta} \bigg(k_t^{-1}+ fe^{eve}_{t|t-1}\mathbf{g}_{\pi}(t) \bigg) \bigg(\E_t\sum_{i=1}^{\infty}x_{t+i}\prod_{j=0}^{i-1}(k_{t+1+j}^{-1} + fe^{eve}_{t+1+j|t+j})\mathbf{g_{\bar{\pi}}}(t+j) \bigg)}
\numberthis \label{target_interpretation}
\end{align*}
Interpretation: \textcolor{red}{tradeoffs from discretion in RE} + \textcolor{blue}{effect of current level and change of the gain on future tradeoffs} + \textcolor{mygreen}{effect of future expected levels and changes of the gain on future tradeoffs}

%\section{A target criterion system for an anchoring function specified for gain changes}\label{target_crit_changes}
%\begin{equation}
%k_t = k_{t-1} + \mathbf{g}(fe_{t|t-1})
%\end{equation}
%Turns out the $k_{t-1}$ adds one $\varphi_{6,t+1}$ too many which makes the target criterion unwieldy. The FOCs of the Ramsey problem are
%\begin{align}
%& 2\pi_t + 2\frac{\lambda}{\kappa}x_t -k_t^{-1} \varphi_{5,t} - \mathbf{g}_{\pi}(t)\varphi_{6,t}  = 0 \label{gaspar22}\\
%& c x_{t+1} + \varphi_{5,t} -(1-k_t^{-1})\varphi_{5,t+1} +\mathbf{g}_{\bar{\pi}}(t)\varphi_{6,t+1} = 0 \label{gaspar21}\\
%& \varphi_{6,t} \; \textcolor{red}{+\; \varphi_{6,t+1}} = fe_t \varphi_{5,t} \label{constraints}
%\end{align}
%where the red multiplier is the new element vis-a-vis the case where the anchoring function is specified in levels ($k_t^{-1} = \mathbf{g}(fe_{t-1})$, as in App. \ref{target_crit_levels}), and I'm using the shorthand notation
%\begin{align}
%c & = -\frac{2(1-\alpha)\beta}{1-\alpha\beta}\frac{\lambda}{\kappa} \\ 
%fe_t & = \pi_t - \bar{\pi}_{t-1}-b s_{t-1}
%\end{align}
%(\ref{gaspar22}) says that in anchoring, the discretion tradeoff is complemented with tradeoffs coming from learning ($\varphi_{5,t}$), which are more binding when expectations are unanchored ($k_{t}^{-1}$ high). Moreover, the change in the anchoring of expectations imposes an additional constraint ($\varphi_{6,t}$), which is more strongly binding if the gain responds strongly to inflation ($\mathbf{g}_{\pi}(t)$).
%One can simplify this three-equation-system to:
%\begin{align}
%\varphi_{6,t} & = -c fe_t x_{t+1} + \bigg(1+ \frac{fe_t}{fe_{t+1}}(1-k_{t+1}^{-1}) -fe_t \mathbf{g}_{\bar{\pi}}(t) \bigg) \varphi_{6,t+1} -\frac{fe_t}{fe_{t+1}}(1-k_{t+1}^{-1})\varphi_{6,t+2}\label{6'} \\
%0 & = 2\pi_t + 2\frac{\lambda}{\kappa}x_t   - \bigg( \frac{k_t^{-1}}{fe_t} + \mathbf{g}_{\pi}(t)\bigg)\varphi_{6,t} + \frac{k_t^{-1}}{fe_t}\varphi_{6,t+1}\label{1'}
%\end{align}
%Unfortunately, I haven't been able to solve (\ref{6'}) for $\varphi_{6,t}$ and therefore I can't express the target criterion so nicely as before. The only thing I can say is to direct the targeting rule-following central bank to compute $\varphi_{6,t}$ as the solution to (\ref{1'}), and then evaluate (\ref{6'}) as a target criterion. The solution to (\ref{1'}) is given by:
%\begin{equation}
%\varphi_{6,t} = -2\E_t\sum_{i=0}^{\infty}(\pi_{t+i}+\frac{\lambda_x}{\kappa}x_{t+i})\prod_{j=0}^{i-1}\frac{\frac{k_{t+j}^{-1}}{fe_{t+j}}}{\frac{k_{t+j}^{-1}}{fe_{t+j}} + \mathbf{g}_{\pi}(t+j)} \label{sol1'}
%\end{equation}
%Interpretation: the anchoring constraint is not binding ($\varphi_{6,t}=0$) if the CB always hits the target (
%$\pi_{t+i}+\frac{\lambda_x}{\kappa}x_{t+i} = 0 \quad \forall i$); or expectations are always anchored ($k_{t+j}^{-1}=0 \quad \forall j$). 


\end{document}

%%%%%%%%%%%%%    SUBFIGURE  %%%%%%%%%%%
%\begin{figure}[h!]
%\subfigure[Hodrick-Prescott, $\lambda=1600$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath materials22_gain_dhat_HP}}
%\hfill % this is great to intro dpace between subfigures
%\subfigure[Hamilton, 4 lags, $h=8$]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath materials22_gain_dhat_Hamilton}}
%\subfigure[Baxter-King, $(6,32)$ quarters, truncation at 12 lags]{\includegraphics[scale=\myAdjustableFigScale]{\myFigPath materials22_gain_dhat_BK}}
%\caption{Inverse gain for $\hat{d}$ for the different filters}
%\end{figure}

%%%%%%%%%%%%%    TABLE  %%%%%%%%%%%
%\begin{center}
%\begin{table}[h!]
%\caption{$\hat{d}$}
%\begin{tabular}{ c |c |c }
%  & $W = I$ & $W = \text{diag}(\hat{\sigma}_{ac(0)}, \dots, \hat{\sigma}_{ac(K)})$ \\ 
%  \hline
% HP & 77.7899 & 10 \\  
% \hline
% Hamilton & 32.1649 & 10 \\  
% \hline
% BK & 90.3929 & 10    
%\end{tabular}
%\end{table}
%\end{center}





